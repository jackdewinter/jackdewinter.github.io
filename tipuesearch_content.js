var tipuesearch = {"pages":[{"title":"Markdown Linter - Delving Into the Issues - 18","text":"Summary In my last article , I documented how I was working hard to get to the end of the unprioritized items in my issues list and on to the prioritized parts of the list. This article details the work that I am doing to make that push happen. Introduction Now a week or so removed from the New Year's Holiday break, it was refreshing to know that I was still able to resolve a healthy collection of items from the issues list. Sure, it was not the same volume as during the holiday, but it was still a healthy volume of issues to resolve. And I definitely felt that I was getting closer to the end of the initial phase of the PyMarkdown project. I was pretty sure that I would not be able to resolve every item from the unprioritized section of the issues list this week, but I was confident that it was going to happen in the week after. I just had to maintain a good velicity of resolving issues, and I would get there soon! What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 11 Jan 2021 and 17 Jan 2021 . Doing Some Simple Cleanup As always, I try and ease into the project work for the week with something simple. This week was no different. And while this change might seem to be of no consequence to others, to me it was a question of readability and searchability. The task? To replace newline characters in the source code with ParserHelper.newline_character and to replace the colon character ( : ) separating the extra data field for the Markdown tokens with MarkdownToken.extra_data_separator . While it might not be obvious to people that are not dealing with the source code, the presence of those two characters in an unescaped and unreferenced form had caused me a lot of frustration. I guess if I had to pin down a cause why that happened, it was because a newline character and a single colon character are easy to miss when scanning through source code. I wanted something that I could easily find, and not something that I would easily miss, as had happened numerous times during debugging sessions. For me, the cost of this change was easily outweighed by the benefit for readability. It was not a change that fixed an issue or enabled a scenario tests, but I was able to notice the difference almost instantly. It just helped me see the code better, and that was its goal! Upgrading To Python 3.8 Having started a year ago, Python 3.7 was the stable release of Python at the time the project started. At that time, Python 3.8 had just been released on 14 October 2019 1 , a couple of weeks before I started working on the code. With the first commit of the source code on 22 Nov 2019, it just seemed like a safer bet to stay with version 3.7 until the bugs were worked out of the then brand new Python release. Just over a year later, with Python 3.9 released on 15 Oct 2020 1 , it felt like a good time to upgrade one minor version with the same reasoning in mind. However, there was also another reason: performance. Having started to explore the performance of the project on sample Markdown pages, I found that the project's parser was taking a long time to parse a simple Markdown file. Using cProfile and SnakeViz , I knew that the number one problem that I had with performance was the way I used log statements. Without going too far into my research 2 , in order solve the performance issue while keeping the extra functionality that helped me debug more efficiently, I would soon need to write my own logging wrapper. To do this properly, my research on logging indicated that I would need to use the stacklevel argument to allow the wrapper to function while logging the location where the wrapper's log statement was called from. The catch? It was introduced in Python 3.8. With a good reason to update and a good, stable version of Python 3.8 to update to, I proceeded with the upgrade with relatively few issues. The main issue that I hit was that I needed to ensure that I uninstalled Python 3.7 in the project, install Python 3.8 on my system (including all environment variables), and then install Python 3.8 in the project. Once that was done, the only other issue that I had was with the Black Python formatter. In that case, I needed to examine the graph for that package and make sure that I installed the correct version of the dependent library in the project. After that small headache, which took minutes to solve, everything was working fine and continues to work fine. More Fun With Link Reference Definitions The first thing to mention about the next task is that while the commit was performed earlier than the 3.8 Upgrade commit, chronologically this task came after the upgrade task. The reason that this is relevant is that the project uses the default settings for Black, and either those defaults or the algorithm implementing the line folding changed after the upgrade was completed. Why is this relevant? While the commit itself looks like it has a lot of changes, many of those changes occurred in folding the lines according to upgraded settings. And as I was focused on the Link Reference Definitions, I did not notice those formatting changes until after I had made a number of changes. It was just easier to commit them together at that point than to pull them apart. Other than that noise, there were three new scenario tests introduced, testing Link Reference Definition elements broken up across container block boundaries. The first test added, test function test_link_reference_definitions_extra_01 , was created with a single Unordered List element character, followed by a valid Link Reference Definition spread over two lines, with the second line not being indented: - [ foo ] : / url The second test, function test_link_reference_definitions_extra_02 , used the same format, but used a Block Quote element prefix instead of an Unordered List prefix. Finally, to provide a touchstone, I added function test_link_reference_definitions_extra_02 that has both lines of the Link Reference Definition preceded by the Block Quote element prefix. While it was a duplicate test, I felt it was a good reminder of how a test with both lines worked, and thus it was a good reference test. Now, according to the specification, the List element continues if the next line starts with enough whitespace to maintain the indent or if it is a continuation of a Paragraph within the list. As the Link Reference Definition is not a Paragraph when parsed the first time, the second line terminates the list, and causes the Link Reference Definition to be requeued and parsed as a normal Paragraph on the second parse through. Or so I thought. Github Flavored Markdown vs CommonMark While both specification are usually in sync with each other, sometimes the GFM Specification and the reference CommonMark implementation CommonMark.Js called from Babelmark 2 differ in small implementation details. I had experimented with the Block Quote scenario test for three or so hours before I submitted a question to the CommonMark forums asking if I had misunderstood something in the specification. The answer that came back was a supportive answer, but at the same time, an honest answer. The approach that CommonMark's reference parser had taken was to parse the lines as the start of an Unordered List followed by a Paragraph block. Only after that Paragraph block had been parsed, with the paragraph continuation kicking in, does the parser look for a Link Reference Definition at the start of that Paragraph. Is this 100% according to the GFM specification? No. But does it make sense for the CommonMark team to do this? I would argue yes. Getting Link Reference Definitions correct in the PyMarkdown parser has continued to be a pain to this day. Based on my limited experience, while Link Reference Definitions can be spread over multiple lines, there are very few cases where that is done in \"real life\". From a correctness viewpoint, if I had to guess on the percentages, I believe I would estimate that their approach correctly parses 99.5% of the Link Reference Definition elements, with only some \"weird\" multiline Link Reference Definition scenarios not being parsed. But that left me with a decision. What was the correct thing to do for the PyMarkdown parser? Which To Choose? After thinking about this overnight, I decided that the best approach for the project was to align with the CommonMark reference implementation, while also discussing the differences from the GFM Specification with the CommonMark team in the forums. By making that choice, I had confidence that I would have something to compare against for correctness that was both concrete and measurable. It either would parse properly against commonmark.js 0.29.2 and be considered correct or it would not and be considered a parsing failure. As for any differences, I could clearly note them in documentation somewhere, and talk about them on the forums with the owners of CommonMark and the GFM specification, trying to improve both. It was not a perfect answer, things rarely are perfect. With that decision in hand, I marked all three new tests as skipped before starting to work on the Block Quote functions. Based on what I was seeing in the test failures, everything looked fine in the HTML output, except that the output was missing an entry in the Block Quote for the parsed line. Taking a wild guess, I determined that I needed to introduce something in the handle_block_quote_block function to ensure that the number of lines in the Block Quote element were correct. Surprisingly, that was the fix that was needed. No changes in the Markdown transformer were needed, and no changes in the consistency check was needed. Not sure what I had going on that evening, I decided to mark the List Block version of the scenario test, the function test_link_reference_definitions_extra_01 , as disabled. Cleaning up the code and committing it to the repository. It was a good place to stop while I figured out what was going on in the evening. Getting Back To It With the plans for that evening falling through, I found that I had a couple of hours free that evening. Not wanting to let them go to waste, I decided to see if I could tackle the test_link_reference_definitions_extra_01 function that I elected not to get working in the previous section. To ensure I was moving in the correct direction, I added extra variations of the test that included one and two spaces before the second half of the Link Reference Definition element, as well as one with each half of the Link Reference Definition in its own List Item. As I have had numerous problems with List Blocks in the past, I expected to expend a lot of effort to clean these cases up, but only a little bit of effort was required. Specifically, the only change that was needed with in the ListBlockProcessor class and its __check_for_list_closures function. Like the previous section and the Link Reference Definition element that spanned Block Quote element levels, the CommonMark reference implementation was treating the Link Reference Definition text as a Paragraph before detecting the Link Reference Definition element itself. To replicate this behavior, I needed to modify the __check_for_list_closures function to keep the ‘paragraph' open if it was parsing a Link Reference Definition. Making those modification, I was able to get the main function, test_link_reference_definitions_extra_01 , working, as well as the sibling functions test_link_reference_definitions_extra_01a and test_link_reference_definitions_extra_01b . This meant that a Link Reference Definition split over a List Block and the end of that block with various amounts of indentation was working properly. However, function test_link_reference_definitions_extra_01c , where I split the Link Reference Definition over two List Items was not working at all. With my time used up in the evening, I marked it as skipped, cleaned it up, committed it, and went to sleep for the night. And More Link Reference Definitions As I was looking at the content of the existing scenario tests, I noticed that I did not have a couple of tests that had simple multi-line Raw HTML elements and multi-line Code Span elements in links. Basically, I wanted to take example 644 for Raw HTML elements: foo <! -- this is a comment - with hyphen --> and example 345 for Code Spans: `` foo bar baz `` and place them within both Inline Links and Reference Links. I was hoping that this was a simple test, but I was not sure. Adding all four tests, (two for Inline Links and two for Reference Links), I was pleasantly surprised that all four tests passed without any changes. While I am aware that I am getting closer and closer to the initial release of the project, I still find that I expect things to fail as a default setting. As I am usually an optimistic person, my only explanation for that failure viewpoint is one of writing too many automation tests in my career. When I note something down in the issues list, I believe that I feel that most of those items are going to be things that I forgot to cover, not things that I wish to ensure are covered. Regardless, I need to figure that out and work on it a bit. I do have a lot of confidence in the PyMarkdown project and its accuracy, and I need to project that more. Fixing Disabled Tests After a good night's sleep and a good day's worth of work under my belt, I settled down in the evening to work on the next issue: enabling test functions test_block_quotes_extra_02ax to test_block_quotes_extra_02ad . The good news was that the HTML transformer and the Markdown transformer were both working properly. The bad news was that the consistency checks were failing for all these tests. It took me a bit to get going that evening, but when I did, it was obvious to me that the problem was that the consistency checks were not recognizing the active Block Quote element. Following along in the verify_line_and_column_numbers method, it became obvious that the code: if container_block_stack and container_block_stack [ - 1 ] . is_block_quote_start : was not evaluating to true for the four test functions that I was evaluating. But how to fix them? It took me a while to realize that the reason that the condition was not evaluating to True was that the Block Quote token was not always the last token on that list. When I read the variable named container_block_stack , in my head I was parsing it as \"the stack for container Block Quotes\", not \"the stack for container blocks\". Once I figured that out, the answer became obvious. I created a new function find_last_block_quote_on_stack that went back in the stack until it found to last Block Quote token and returned it. From there, I replaced any occurrence of container_block_stack[-1] with last_block_quote_token . Therefore, the code from above became: last_block_quote_token = find_last_block_quote_on_stack ( container_block_stack ) if last_block_quote_token : I ran the tests, and after clearing up a couple of typing mistakes, the tests all worked properly, and they were now passing! A Quick Fix… I Hope! Looking at the end of the uncategorized section of the issues list, there was one item that I felt confident that I could quickly deal with: - 634 , but forcing an html block To start working on this item, I made six copies of test function test_html_blocks_123 . The first copy was in a List Block element, the next three copies were in various forms of a Block Quote element, the fifth copy was within a SetExt Heading element, and the last copy was within an Atx Heading element. The hard part of each of these tests was that I needed to make sure I was generating an HTML Block token and not a Raw HTML token. That took extra care but did not slow me down that much. Like a handful of other issues like this that I have fixed, the answer to this one leapt out at me as soon as I looked through the log files. When the next line was examined to figure out if the Block Quote element should be continued, the check_for_lazy_handling was allowing it to continue. The only issue here was that it was an HTML block, a leaf block type that does not have any continuation logic in the specification. Having noticed that, it was easy to change the following code: if ( parser_state . token_stack [ - 1 ] . is_code_block or ( is_leaf_block_start ) ): to: if ( parser_state . token_stack [ - 1 ] . is_code_block or parser_state . token_stack [ - 1 ] . is_html_block or ( is_leaf_block_start ) ): thereby fixing the issue. Running the scenario tests again, the tests were indeed fixed without needed any other changes. That Week's Big Thing Wrapping up the work for that week, I wanted to make another dent in the issues list, so I decided to at least get the tests set up for the following item: - links with & and \\ with inner link to mine - see __collect_text_from_blocks The base concept of this item was simple: create a group of tests to verify how inline elements were represented when placed within a Link element. To make sure that I was doing a good scenario test, I made the choice to use a Reference Link element. By doing this, I would be testing the link label normalization code and the representation code at the same time. Starting with test_reference_links_extra_03x , I created a Link element with a link label that contained a backslash in the link label: [ bar \\\\ foo ]: / uri [ bar \\\\ foo ] From there, I then created a copy of that test that encapsulated that link label within another link label: [ xx [ bar \\\\ foo ] yy ]( / uri ) [ bar \\\\ foo ]: / uri1 and finally, I created a copy of that test, changing the Link element to an Image element: ! [ xx [ bar \\\\ foo ] yy ]( / uri ) [ bar \\\\ foo ]: / uri1 Then, after a bit of thinking, I decided there was only one combination I was missing, so I added that: [ xx ! [ bar \\\\ foo ] yy ]( / uri ) [ bar \\\\ foo ]: / uri1 That being done, I then repeated that process with &amp; and &copy; for character entity references, code spans, emphasis, autolinks, raw HTML, and hard line breaks. By the time I was done, I had added 40 scenario tests to cover all these cases. Starting to execute the scenario tests, all the tests that just dealt with Link elements were passing without any changes. The Image elements, they were a different story. The failures seemed to stare back at me, standing in the way of me writing the article for that week. It was just time to start debugging and figuring things out. After a bit of debugging, I seemed to notice that the test failures seemed to be in three separate groups of issues. The first group of issues was that the __collect_text_from_blocks function used to grab the existing tokens and render them as text was not complete. But that was not the entire issue, but I felt that there was too much \"noise\" in the way for me to see the issue clearly. Resolving to reduce the noise in the issue, I started working on the main part of the issue. In the cases where the inline element was on its own inside the parent Link element, the Code Span element, the Raw HTML element, and the Autolink element were not representing their elements properly. A bit of exploration and debugging took care of that. With that noise out of the way, I was able to see the other part of that issue better, and added the condition: if not is_inside_of_link : at the start of each of those handlers. The noise that I had experience was simply that in cases where a possible Link element was within another Link element's link label, each of the changed elements just needed to emit nothing to the collection. Running the tests again, it verified that observations, but I also saw something else. Only after getting rid of those issues was I able to see that the __consume_text_for_image_alt_text function was not handling Emphasis start and end elements properly, failing an assert near the end of that function. That fix was easy, adding four lines to the elif statement: elif inline_blocks [ ind + 1 ] . is_inline_emphasis : pass elif inline_blocks [ ind + 1 ] . is_inline_emphasis_end : pass With both of those issues dealt with, the couple of failures that remained were easy ones. Looking at the HTML output, there were a number of /a character sequences in the HTML output. Being the signature for replacement references, I quickly change of the code for the Text element from: image_alt_text += inline_blocks [ ind + 1 ] . token_text to image_alt_text += ParserHelper . resolve_references_from_text ( inline_blocks [ ind + 1 ] . token_text ) Beware Of Special Cases Running the tests again, all the tests were passing except for those that dealt with the &amp; sequence. All the tests dealing with the &copy; sequence were working fine, so I had to think quickly to figure out what the problem might be. Because I am used to looking at processed HTML code, I initially did not see any problem with a &amp; sequence in the HTML output. It looked right. Then it hit me. The problem was not with the HTML output, it was with the processing of the Markdown input. In the cases that I had problems with, the desired HTML output had &amp;amp; which did not look right until I thought about it. Whereas the &copy; character sequence is interpreted as a named charactery entity and replaced with the © symbol, the sequence &amp; was not being interpreted in the same way. The desired HTML output was correct! The initial & from the sequence was being replaced with the sequence &amp; to ensure it was displayed properly, removing any chance of it being interpreted as a named character entity. Making a quick decision, I looked at the InlineHelper class and noticed the append_text function used to ensure that such strings were properly interpreted. Taking a quick look at the imports for InlineHelper and LinkHelper , I thought there was a chance of a circular reference occurring. Given that observation, I decided to make a copy of the append_text function in the LinkHelper class to get around the possibility of the circular reference. Finally, after a couple of hours of work, all 40 tests were passing. It was a bit of a trek to get there, but it was worth it! Whoops As I was starting to write the article, I also started to look into what it would take to remove the duplicate of the append_text function in the LinkHelper class. I had introduced the clone into the LinkHelper class to avoid any issues with referencing the InlineHelper class from the LinkHelper class. It was as I was starting my research into this task that I discovered something that I had previously missed. The LinkHelper class was already referencing the InlineHelper class. After a bit of \"how did I miss that?\", I replaced the call to LinkHelper.append_text with InlineHelper.append_text and everything worked fine. Removing the instance of the append_text function from the LinkHelper class, I ran the complete suite of scenario tests again, and everything worked fine. What Was My Experience So Far? Looking back at the work I did during the week, it was hard not to get jazzed about the progress. I started off by doing a simple fix that made the source code more readable, enhancing my ability to read the source code. Then I upgraded the base Python version to 3.8, knowing that it would allow me to write the wrapper I wanted around Python's logging functions. Add to that the coverage and testing I was able to add and verify for Link elements, Image elements, and Link Reference Definitions, and it was really good work! At some point I noticed the number of scenario tests that I execute with each change. At approximately 2000 tests, I am confident that I am hitting a very large degree of all scenarios for Markdown transformation, not just the \"Golden\" scenarios, and that was also a boost to my confidence. While I can expect things that I note as an issue to not work, I also need to make sure I appreciate what is working. Having a solid set of tests like that is what allows me to refactor with confidence that I am not negatively impact the code. Refactor? Sigh. One of the things I know I am looking forward to is looking at the refactor tasks in the prioritized sections and getting a couple of them underway. The fact that they are present in those sections is a good reminder to me that I can always learn how to do things better, and how to make the project more maintainable. And that is always something I can look forward to! What is Next? With a solid amount of work done this week, I am hoping to be able to clear out the uncategorized section of the issues list in the next week. Will I make it? Stay tuned! Information courtesy of the Python download page . ↩ ↩ I will be covering this in a separate series of articles in the near future. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2021/01/25/markdown-linter-delving-into-the-issues-18/","loc":"https://jackdewinter.github.io/2021/01/25/markdown-linter-delving-into-the-issues-18/"},{"title":"Markdown Linter - Delving Into the Issues - 17","text":"Summary In my last article , I continued working on some big-ticket items from the issues list, making the most of my New Year Holiday break. Back in \"normal time\", I am tackling Block Quote items, to try to get to the prioritized part of my issues list within the next week or two. Introduction While I knew that I was not going to solve the same volume of items as last week, I was confident that I could use this week to make some good progress in dealing with Block Quote elements and their interaction with other elements. I also knew that my mental space was going to be limited this week due to the end of the holidays. I was not the only one that took the time off from my day job, as most of the company that I work for took the same two weeks off. And with everyone coming back to work at the same time, there were bound to be lots of meetings to make sure everyone was resynced for the New Year. And that week there… were… lots… of… meetings. Factoring that into account, I started my work for that week with a reset of my personal expectations on what I believe I can accomplish in a week. I felt that it was important to my sanity to take the time to seriously understand that I did not need to continue taking care of multiple big-ticket items. Just a handful of normal items would suffice. I knew that if I could manage to make the switch to that mindset, it would be a good week. So, with that in mind, the work started. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 07 Jan 2021 and 11 Jan 2021 . Starting with Something Simple As a matter of pride, I try and keep the PyMarkdown code base clean and following flake8 and pylint guidelines. While I mostly correct any raised issues right away, I often choose to temporarily disable these issues until another time where I can resolve them. My logic in making that decision is that it usually better for me to concentrate on the big picture in the moment, addressing any raised issues when I have some less hectic bursts time. As such, at various points in the code base, there are comments such as: # pylint: disable=too-many-public-methods to disable a warning and: # pylint: enable=too-many-public-methods to enable the warning again. But as I am only human, I sometimes forget to balance these statements out, disabling a warning that is only disabled, not enabling a warning that was disabled, or enabling a warning that was not disabled. Writing up a simple Python script, I quickly figured out where these issues were and corrected them. While it was not a very important thing to do, it was just nice to ensure that I had these nailed down. A good start to the week. Rounding Out Multiline Inline Elements One thing that I was sure that I had not covered properly were newline characters contained with Code Span elements and Raw HTML elements. While I had corrected a handful of issues from this area in the past, I did not feel that I had properly covered all the necessary cases, and I wanted to address that discrepancy. Like I normally do, I started with scenario test creation. This began by taking a good look at the available scenario tests in the test_markdown_raw_html.py module and the test_markdown_code_spans.py module. For the first module, I added variations of test function test_raw_html_634 , focusing on any container block elements or leaf block elements that I had not covered elsewhere. I then repeated this process for the other module by adding variations on the test_code_spans_346 test function. This resulted in eleven new scenario tests being added, four for the Raw Html element and seven for the Code Span element. From a top-level point of view, the scenario tests for Raw HTML elements worked fine, and did not reveal any additional issues. The Code Span element tests were another matter. While I had previously dealt with newline characters in the main body of the Code Span element, I had forgotten to perform the same actions on the leading and trailing whitespace for the element. Feeling embarrassed that I forgot the whitespace parts of the token, I quickly made changes to the handle_inline_backtick function and the __collect_text_from_blocks function to ensure that the correct tokens were being generated. To balance these changes out, I also changed the __verify_next_inline_code_span function in the consistency checks to pay attention to the leading and trailing whitespace. Like the changes detailed in the last paragraph, these changes were not difficult once I knew what the problem was. But looking at the code while I was making these changes, I realized that I should not feel embarrassed. While I was being thorough with my testing, the issues that I was finding were more corner cases than anything else. Put bluntly, unless I was testing corner cases, I was sure that I would not create a Raw Html element like: < a >< b href = \"\" >< c > or a Code Span element like: This is some ` really nasty ` code . Unless some specific formatting called for it in a really weird circumstance, I believe I would always write them on one line, not multiple lines. But it was good to get the corner cases. In my head, I know that if I am focusing on the corner cases, I feel confident about the normal cases. That is a good place for me to be! Adding Glob Support While not a part of the issues list, one of the things that I had been experimenting on in some \"down time\" was adding Glob support to the project. This work came about as the quick script that I threw together for validating PyLint disables and enables needed to be able to specify a targetted set of files with Python glob support. Using the same type of mainline base as the PyMarkdown project, I figured the PyLint scanner script was a low-cost, low-risk place to see how much effort it would take to implement it in the PyMarkdown project. It turned out to be very easy. The __determine_files_to_scan function was the main point of contact for determining the files to process. It took exact file paths, to either a directory or a file, and returned set containing all valid paths. In the case of a file path, it simply added the full path to that file to the collection to be returned. In the case of a directory, the directory was scanned, and all matching files were added to that same collection. Nice, self-contained, and simple. Being self-contained, it was easy to modify this function to add glob support. To handle those simple cases, I moved that functionality out of the main function and into a new helper function __process_next_path . With that extracted, I rewrote the __determine_files_to_scan function as follows. if \"*\" in next_path or \"?\" in next_path : globbed_paths = glob . glob ( next_path ) if not globbed_paths : print ( \"Provided glob path '\" + next_path + \"' did not match any files.\" , file = sys . stderr , ) did_error_scanning_files = True break for next_globbed_path in globbed_paths : next_globbed_path = next_globbed_path . replace ( \" \\\\ \" , \"/\" ) self . __process_next_path ( next_globbed_path , files_to_parse ) else : if not self . __process_next_path ( next_path , files_to_parse ): did_error_scanning_files = True break As the call glob.glob already returns an array of matching elements, I was already most of the way to having this implemented. All I needed to do was to properly add the elements returned from the glob call to the collection. So, instead of rewriting the code to add matching elements to the files_to_parse variable, I just called the already debugged __process_next_path function to do all the heavy lifting. Once that was done, manual testing of the new functionality went fine. Some new scenario tests needed to be added, and a couple of existing scenario tests needed to be changed slightly, but nothing unexpected. After an hour or so, the work was done and tested. While not terribly exciting, I could now do some manual testing of the PyMarkdown project against a set of files that was not a single file, nor every eligible file in that directory. And it just felt good to get a small task like that out of the way! Filling Out Existing Tests Narrowing down the items to work on from the issues list, the one that I settled on was: - test_block_quotes_extra_02a with extra levels of lists ? To start the work on this item, I added three variations of the test_block_quotes_extra_02 test function, altering the number of lists in the document and their locations. Noticing that I could do the same type of variations for Block Quote elements, I also added ten new scenario test functions that were variation on the test_block_quotes_extra_04 function, mixing Block Quote elements with the various types of non-inline elements. Executing the bulk of the new tests, I was pleasantly surprised that everything except for the consistency checks were passing without any changes being needed. Even the changes needed for the consistency checks were relatively minor and in two main groups. The first group of changes were in the inline handling part of the verify_line_and_column_numbers function. These changes were not material in nature but served to ensure that the leading_text_index field from the Block Quote token was properly updated. This required the inspection of each inline token to determine if any newline characters are encountered. If any are encountered, the leading_text_index field is incremented by the number of newline characters, ensuring that any references to that field reference the correct line prefix. Seemingly balancing that change, there were a handful of end Leaf tokens that also needed adjusting to properly support the leading_text_index field. Through trial and error, I quickly isolated each type of token, and was able to properly increment the leading_text_index field to handle the end token. It was not a big task, but it was one that I needed to be very methodical on. I did find that I needed to do adjust each at least once as each test was providing coverage for a specific scenario that had been missed. While it was not that much extra work for each individual test, the amount of work required over all the tests added up quickly. Properly Handling Link Reference Definitions In the case of test function test_block_quotes_extra_04f , the issue was that it was just broken. No niceties or anything else, just broken. Added during the last section's work and disabled, the Markdown was: > [ > abc > ] ( / uri ) > > end What made this test function broken was not the Markdown itself, but the generated tokens for it. For whatever reason, the parsing of the Block Quote was both started and ended on the first line, only to be restarted on the second line. Because of the container nature of the Block Quote element, this then spread the text required for the Inline Link element split over two distinct Block Quotes. It was just wrong! Setting the Stage The debugging took a couple of hours to work through, but it was rewarding when I solved it. The problem with the parsing boiled down to my favorite element (heavy sarcasm is implied), the Link Reference Definition element. Because of the unique nature of this element and how it is parsed, I had to add the ability to rewind or requeue the parser so that failed lines from a Link Reference Definition could be properly processed. And while it had worked properly until this point, test function test_block_quotes_extra_04f provided an interesting twist to normal operation, and therefore, an interesting problem. Because of design decisions for Markdown, the Link element and the Link Reference Definition element both start with the same sequence: [link] . If this sequence is followed by an open square bracket character [ , then it probably specifies a collapsed or full link. If this sequence is followed by an open parenthesis character ( , then it probably specifies an inline link. If this sequence is followed by a colon character : , then it probably specifies a Link Reference Definition. And finally, if not followed by any of the above, it is probably a shortcut link. Most of those combinations do not matter, except for my friend (once again, heavy sarcasm implied), the Link Reference Definition. While the Link element and its types are all processed in the inline phase of processing, the Link Reference Definition is processed much earlier in the block phase of processing. Due to that difference, the Link element processing is done with the entire contents of the processed Text token being available, but the Link Reference Definition processing is done one line at a time. Working Through the Process Why was that information relevant? In the case of the above Markdown, the specified text supports both a Link element and a Link Reference Definition element until line 3. Before that point, the Link Reference Definition processing continues forward. When that point is reached on line 3, the line is processed for suitability as a Link Reference Definition, it fails, and the requeue mechanism needs to be enacted so that the lines can be interpreted properly. Unlike any previous scenario tests, in this case, that requeue mechanism was not sufficient. What was being requeued was only the information after processing. When the requeue mechanism kicked in, it was trying to return to the state that was in place when the Link Reference Definition started. But when it started processing the requeued information, it did so with the processed line of information. That line was missing the Block Quote prefix, causing the Block Quote to be closed. It took a while to get there, but I did figure out why that was happening with the closing of the Block Quote! Fixing the Issue In this case, the line that had been passed to the Link Reference Definition processor did not have the Block Quote prefix attached to it. Having been removed at the container level before being passed on for Leaf Block processing, the lines to requeue were missing information. To fix that issue, I had to figure out a way to ensure that I could retain that information so that it could be requeued if needed. Therefore, I introduced a the unmodified_line_to_parse variable that contains the line as read, unmodified by any processing. This got me a lot of mileage in fixing this issue, but after rerunning some of the tests, a couple of the tests were failing because there was another issue somewhere. Debugging that issue over the course of an hour, I found that there was another requeue issue that I needed to address: the main document and the main token stack. In a couple of the new scenarios, when the processing of the Link Reference Definition was started, another type of block element was ended. The effect of this was that a new Markdown token was placed in the document and a new stack token was placed on the main token stack. While the rewinding took care of the data, it did not take care of that state information. Dealing with that issue was somewhat simple but took a while to get right. Before starting the processing of the Link Reference Definition, I keep track of the lengths of both the main document and the token stack. If I need to requeue elements, I simply remove any entries that are past that mark. It is not very graceful, but it was successful and worked wonderfully. Squeezing One More Task In If things are going normally, I organize and write my articles on Sunday, with the editing the draft article going into Monday evening. During that time, I do take a fair number of breaks to ensure that I am writing and editting with a clear mind. But just because I start working on the article, it does not mean that I stop thinking about whatever it is I was working on. Usually, it is a battle between getting the writing done and my urge to complete what I started. Most of the time, the article wins. In this case, it did not. On Saturday morning, I had started working on figuring out how to get test function test_block_quotes_extra_03b working. And while I had made some progress on it, I was still working on it. For whatever reason, when placed within a Block Quote element, Link Reference Definitions were not being recognized properly. I had started working on this right after fixing test function test_block_quotes_extra_04f and I had spent a decent amount of time trying to get it working. But with a busy weekend in my personal life, I was not able to get a good, solid, contiguous couple of hours to work on this issue as I had hoped to do. As such, I had started to try to figure out this issue about five times and gave up after each short try. It gnawed at me that I could not figure it out. It had not taken me long to resolve the previous set of issues, why was it taking me so long with this one? Regrouping After completing the bulk of the rough draft of the article, I took some time to relax and clear my head, knowing that I needed to look at the problem again. This time, I had a lot better results with my debugging. Starting with the basics, I turned on debug logging for the test and followed along in the source code as I read each line of the debug output. It was then that I noticed the issue: the Block Quote token itself was wrong. As I looked through the logs, everything was fine up until the requeue from the Link Reference Definition happened. From there, everything was just off. Taking some time to think about it, I decided to take our dog Bruce for a walk. During that walk, I tried hard not to think about the issue, and mostly succeeded. When I came back, I was able to examine the log files again, knowing that the Block Quote token was off, and that I had to find the cause. Within five minutes, I had the answer. It was once again a state issue. Before the requeue happened, as each line was being processed within a Block Quote, new information was added to the Block Quote token. This information was about the leading text that was removed from each line in the container processor, ensuring that the leaf processor only had to deal with leaf block related issues. To ensure that the Markdown could be properly rehydrated, this information was stored in the Block Quote token itself. But when the requeue happened, nothing was done to erase the information added to the token between the start of the Link Reference Definition parsing and the start of the requeue. Or at least that is what I thought had happened. Doing some quick testing, I quickly proved my theory to be correct. As I followed along in the logs for the test function, I saw the amount of leading text in the Block Quote token increase, but never decrease. To further prove that I was on the right track, I compared the number of lines that were requeued to the number of extra lines of leading text present in the token, and it was a match! Fixing The Issue With a solid lead on what the cause was, the most concrete manner of proving that I had the right cause to fix it. After mulling around various ideas in my head, the one that won out was to simply store a copy of the Block Quote token in the Link Reference Definition token at the start of processing. With the other requeue logic in place, once I had done all the other requeuing, I simply replaced the changed Block Quote token with the copy of the original token. Running through the tests, this worked right away! After having taken such a long way to get there, I now had it fixed. But since I had ended up solving the issue somewhat late on Sunday evening, I decided to put the changed code aside and to continue edit that week's article. It was enough to knowing that I had solved it and that it just needed cleaning up before committing. It was then after I had completed my final edit of the article on Monday night that I noticed that I had finished early on Monday night with a lot of time to spare. With that extra time in hand, I was able to take the roughly finished solution and polish it up enough to commit it. While technically it should be a part of next week's article, it just felt right to include it with this article, as that is where most of the work occurred. What Was My Experience So Far? After a busy week of getting rid of some big-ticket issues, it was very nice to reduce my scope and focus on the smaller items. Not that I mind working on the big items, it is just that they require me to maintain a larger scope of focus, thereby tiring me out a bit more. The smaller items are not always as satisfying to resolve, but they are also not as draining. During the article, I mentioned that I was becoming more aware that I was dealing more with corner cases than anything else, and that was a good feeling. I am very confident that any of the main scenarios driving the parser have already been addressed. With those out of the way, it stands to reason that any issues that I am finding are the weird cases that do not occur that often. It just makes sense to me. It also means that I am getting more confident that I am nearing the end of this testing phase of the PyMarkdown project. My main drive for the project was to complete the project on my own terms, with the level of quality and testing that I expect from other projects. While I could have started releasing this project a while ago, I wanted to make sure that I have reached that level before I ship the project, and work on improving it from there. And with the knowledge that I am cleaning up corner cases, I know that I now closer to that point with the PyMarkdown project than I have ever been before! And it is a good feeling! What is Next? I do not want to sound like a broken record, but it is back to the same process of finding the next item to work on, and getting it resolved. The only difference was that I was getting close to eliminating all the \"open range\" items in favor of the prioritized issues. Progress!","tags":"Software Quality","url":"https://jackdewinter.github.io/2021/01/18/markdown-linter-delving-into-the-issues-17/","loc":"https://jackdewinter.github.io/2021/01/18/markdown-linter-delving-into-the-issues-17/"},{"title":"Markdown Linter - Delving Into the Issues - 16","text":"Summary In my last article , I continued working on Block Quote issues and some general clean up that I have wanted to do for a couple of months now. With one week left in my holiday, I wanted to make sure I tackled as many of the big-ticket items that I can while I have the time. If I am going to be doing some work during my holiday, I want to make it count! Introduction With a couple of weeks off for the holidays and relatively light \"honey-do\" 1 schedule, I had some free time to devote to getting the project closer to the goal line. While I did not want to lose focus on getting the smaller items completed, I had a number of big-ticket items that I wanted to do. Because of their size or scope, I wanted to ensure that I had a dedicated amount of contiguous time to work on each item. Except for the occasional weekend, I figured that this would be the best time to work on them and hopefully get all of them completed in one big push. With a good week left of New Year's holiday left to go, it seemed like a good idea to try and get as much done as i could in the short amount of time I had. At least, that was my plan. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 28 Dec 2020 and 03 Jan 2021 . Enhancing Code Quality Continuing with the large item cleanup that I had started in the previous week; I was eager to get some refactoring done on the Markdown token classes. The first step towards refactoring those classes was to make each token as read-only as possible, to avoid the possibility of changing the tokens. To accomplish this task, I just went through each token in order, adding the __ prefix to most of the field names, thereby making them private. With each of those fields now private, I added a new getter property for each field named after the original field name. As the property name for the new getter function matched the old name of the field, any read-only access was able to continue without any issues. As with any such change, there are always exceptions that need to be dealt with individually. One such case was the active field of the SpecialTextMarkdownToken class. The first exception was that instead of retaining the name active , I felt that the name is_active was more descriptive. The second exception was that this token type is used to look for starts and ends of various inline token sequences. As such, when those sequences are found, the previous instances of those tokens are marked as inactive, meaning they will not be used any more. To take care of this, I introduced to that token a new function deactivate , specifically used to deactivate the token without exposing the token's member variable in its private form. Once this refactoring was completed, I realized that the markdown_token.py module was way too large for my liking and needed to be broken down. Keeping the base MarkdownToken and EndMarkdownToken classes in their original file, I started to move every Markdown token to one of three new modules: one for container block tokens, one for leaf block tokens, and one for inline tokens. Once the tokens were in their new modules and all tests passed, I added a new base token class for each of the three new modules and switched the base class for each token to the new base token in the same module. By switching over each token to use these three new base classes, I was able to further reduce the amount of code in each token. While it was not too much of a reduction, it was a reduction I felt good about. Enhancing Some More When I started working on this group of tasks, the first thing that came to mind was the words to an old campfire song I learned long ago in Boy Scouts of Canada. While it is a silly song named Nelly In The Barn , the bit between the verses goes: Second verse, same as the first, a little bit louder and a little bit worse! I could think of no phrase better to describe what I needed to do with the StackToken class. Having had good success with changing all the MarkdownToken classes to use private fields and is_x methods (as detailed in my last article), I felt that the StackToken class needed the same treatment. Unlike the almost 20 classes for MarkdownToken descended classes, the transformation on the 9 classes descended from StackToken went by quickly. Like the changes made to the MarkdownToken classes, I was emboldened to make these changes due to the confidence of having a large group of scenario tests that I can use to look for any issues. Without that large group of tests, I would be worried that I would fix one thing, only to break something else in a related piece of code that I forgot about. Closing the Test Loop As I noted in the section of the last article entitled Making the Test Invocation More Efficient , I implemented a change to how I invoked the test infrastructure to reduce the needed code from eight lines (1 blank line, 2 comment lines, and 5 code lines) to two lines (1 comment line and 1 code line). Having done some further examination of those changes, I felt that those changes had settled in nicely and it was time to propagate those changes to all similar test functions. I had a good model for the changes, but even so, the work was very monotonous. With just short of 2000 scenario tests that required changing, it was a test of wills: me versus the code base. To keep myself motivated, I kept a search window open on the side of my editor, selecting a new group of tests to modify whenever I found myself getting bored. Even after taking extra breaks to do household chores, I still found that it was a tough task. But I knew it was a good task to do, so even though I could feel the lobes of my brain numbing with each keystroke, I pressed forward with making the changes. Keeping Things Simple Having ensured that all end Markdown tokens had their start_markdown_token field set, I looked at the code to figure out if there were any redundancies that were introduced with that change. As that field points to the start Markdown token, there was a good chance that I had stored duplicate data in the EndMarkdownToken to avoid having to calculate the start Markdown token for some of the tokens. It was just a matter of identifying any such tokens. While I was sincerely expecting more of an impact, the only change that I was able to perform was around the end Markdown token generated for the EmphasisMarkdownToken token. To get around the constraints at the time that it was written, I had added duplicated data to that end token to denote the length of the emphasis and the emphasis character. With that actual start token now available for reference, I was able to replace the duplicate data stored in the EndMarkdownToken with a reference to the EmphasisMarkdownToken instance. No longer needing that duplicate data, I removed it from the __process_emphasis_pair function. The fallout of that change was approximately 200 instances where I needed to replace the now useless data with the string : . To make things easier while editing, I simply kept the text ::: in the clipboard, searched for [end-emphasis( , and replaced the first three characters after the ) character. It was mind numbing work that I did in three or four shifts, but I got it done. Running the tests, everything was working except for a couple of tests. Investigating each of those failed tests, the failures were all simple typing errors, quickly fixed to make the tests pass. Reorganizing the Series M File This task was a pure cut-and-paste task, but one that I really needed to do. At over 10 thousand lines of code, the test_markdown_paragraph_series_m.py module was just way too big! I was initially okay with the size of the module, seeing that all the scenario tests in the file were related to each other. But as I started to add more and more tests to that one large file, it was becoming too difficult to work on in that form. As such, I simply create one file for each group of tests, such as test_markdown_paragraph_series_m_fb.py for the Fenced Code Block tests, and moved the test functions into their new home. Collapsing Ordered and Unordered List Processing At the start of the project, while I was working through the initial parts of the parser, I was not sure that the processing of Ordered List Blocks and Unordered List Blocks would overlap. With almost a year of processing accomplished, I now had a very solid observation on that subject. Most of the processing overlapped, and overlapped cleanly. Now it was just a manner of taking the time to surgically merge two List Block token concepts into one in different places in the source code. The First Step A big first step on this journey was to move the code for calculating the looseness of a HTML rendered list from the TransformToGfm class into the new TransformToGfmListLooseness class. While it was just a simple cut-and-paste move, I feel that the move left the TransformToGfm class better able to focus on the HTML transformation, instead of also having the responsibility of figuring out the list looseness. It just felt cleaner to have that responsibility in its own class and module. Along with that change, I made equal changes to how the List Block starts were processed in the HTML transformer and the Markdown Transformer. In the HTML transformer, the __handle_start_unordered_list_token function was renamed to __handle_start_list_token and the __handle_start_ordered_list_token function code was merged into that function. In the Markdown transformer, the same process was repeated with the __rehydrate_unordered_list_start function was renamed to __rehydrate_list_start and the __rehydrate_unordered_list_start_end function code was merged into that function. That merge allowed for the handler calls in each module to deal more simply with the List Blocks, something that was worth it to me. Equalizing the Two List Blocks Having done a lot of testing with the Unordered List Blocks, I felt it was time to give some extra focus to the Ordered List Blocks. Before the holiday break started, I had noticed a handful of cases where the Ordered List Blocks had errors in them that I thought should have been caught by existing tests. While fixing this issue was not the primary goal of this round of refactoring, I considered it a simple secondary goal that I should only have to fix list issues once, not twice. Looking at the second half of the is_ulist_start function, I noticed that there were significant differences from its sibling is_olist_start function. Slowly, I started making changes to the is_ulist_start function, bringing it more in line with it sibling. But after making those changes, I still had the task of making sure that those changes were working properly. As any failures were not already caught, but discovered through other tests, I figured that I needed to stop up my test game. To start this off, I picked six scenarios from each of Series M tests and made copies of those tests. Instead of using the Order List Blocks in those tests, I replaced the Ordered List Blocks with Unordered List Blocks. It was only after adding those 108 scenario tests that I was confident that those changes had a good start at getting coverage. And it paid off too. The only issues that were found were in the __perform_container_post_processing_lists function, where the data to be merged with the surrounding list had to be massaged before a call to __merge_with_container_data and restored after that call was completed. Refining List Start Functions The final set of functionality to merge was the is_ulist_start function and the is_olist_start function. Both of these functions had been on my \"refactor\" list for a while, so I was glad to get started on them. On closer examination, there were only a few statements or values that changed between the two functions. Once the setup was accomplished in the first half of the function, the second half was near identical. Creating a new __xxx function, I cut the second half of one of those two functions and pasted it in that new function. After checking to make sure nothing was lost in the cut-and-paste, I compared it line-by-line with the similar code in the other function, adjusting both functions to be represented by the new function. After a couple of rewind moments, the new __xxx function incorporated the process from both original functions. With that accomplished and staged, I removed the second function and used the new function in its place. After fixing a couple of small issues, the new function was up and running and working for both Ordered List Blocks and Unordered List Blocks. At that time, I remember looking at the code and thinking that I had only completed half of the job. Repeating the same process that got me to that point, I soon renamed the __xxx function to __is_start_phase_two , and further extracted code into a new __is_start_phase_one function. With that done, the is_olist_start function was already slim, and I extracted the remaining logic into the __is_start_olist function to keep it slim, replicating that processing with the is_ulist_start function. In the end, I was very satisfied with the amount of refactoring that I was able to accomplish. Both methods were now six statements long, with 95% of the differing functionality in the __is_start_olist function and the __is_start_ulist function. While it was a good feeling getting the refactoring done, it was an even better feeling knowing that I had a healthy set of test cases that I could count on when refactoring! Consolidating New Block Starts Having poured through the code looking for things to simplify, I was keenly aware of one set of function calls that I could simplify: detecting new block starts. Developed in different parts of the code for similar reasons, both the List Block Processor's __check_for_list_closures function and the Block Quote Processor's check_for_lazy_handling function were performing almost the same set of instructions. One by one, the different Leaf Block start functions were being invoked to determine if a newline indicated the start of a new block. It was not a big change but consolidating that code into the is_paragraph_ending_leaf_block_start function just made things cleaner. There was just one function to call with good documentation on what was going on. It just made sense to me. Verifying Paragraph Usage Just before I started writing on that Sunday morning, I decided to add something that was hopefully going to be a slam dunk. While I was reasonably sure that all the various referenced to Paragraph elements and any newlines within them were accounted for, I wanted to be doubly sure. So, to be sure of that, I added the following code to the end of the __rehydrate_paragraph_end function: rehydrate_index = current_token . start_markdown_token . rehydrate_index expected_rehydrate_index = ( current_token . start_markdown_token . extracted_whitespace . split ( \" \\n \" ) ) assert rehydrate_index + 1 == len ( expected_rehydrate_index ), ( \"rehydrate_index+1=\" + str ( rehydrate_index + 1 ) + \";expected_rehydrate_index=\" + str ( len ( expected_rehydrate_index )) ) Basically, by the end of the Paragraph element's processing, the rehydrate_index member variable should have been incremented once for each newline contained within the paragraph. If this did not happen, it means that the use of that rehydrate_index member variable is potentially off and needs to be fixed. Surprisingly, even after adding some extra tests, the only one element had issues: the Raw HTML element. In the main parser's parse_raw_html function, the raw text to use for the tag was not properly accounting for the newline, something that was quickly fixed. This pattern continued in the Markdown generator's __rehydrate_inline_raw_html function, where I specifically created the __handle_extracted_paragraph_whitespace to handle the pulling apart and reintegration of that information. This was a useful function to have as I found some small issues with the __rehydrate_inline_code_span function that required applying that function to the various parts of the Code Span element. Finally, to round out the fixes, the __verify_next_inline_raw_html function needed to have a similar approach taken to address the last issues with the rehydrate_index . Given that it could have been a lot more cumbersome to fix, I was happy that I got off with a relatively light amount of work! As I Was Making These Changes… During the middle of these changes, I rearranged the items in the issues list. My goal was to take the priorities that I had in my mind and reinforce them in the list. The only exceptions to this reorganization were anything that was an immediate focus of what I was doing at the moment. The way I rationalized this was that anything that I could set down for a while was something that I could figure out when to get to it. With an item that remained on the list or was newly added to the list, I deemed that the items were more of an immediate task to get done, and just needed doing. While it may seem like a bit of a wishy-washy rule, it was one that I still felt fine with after a couple of days of having the prioritization in place. Granted, it felt like I was churning through the immediate task section, but that also felt right. As I am working on whatever current task that I need to work on, I observe things in the code and have questions about whether I took the right approach. Writing those things down in the issues list allows me to continue with the task without losing too much of the essence of what I wanted to verify or question. To me, that just seems like a good approach! Preparing for The Future A major part of this week's work was to clean things up a bit with bigger tasks that I do not normally have time for. As such, I decided to spend about four to five hours during the week experimenting with SnakeViz and incorporating primitive SnakeViz support into the project. While it is too early to say what needs the most improvement, I can say that I was able to get some good experience working with the tool and the output HTML that helps visualize the performance. I engaged in the experimentation specifically to gain some experience with code profiling, and I can honestly say that I think I got the experience that I was looking for! The smart thing about using SnakeViz to profile is that it is interactive. To ensure that a user can dig down and get relevant information, SnakeViz takes a performance analysis file and hosts fragments of it through a webserver that it starts. As the webserver is starting, SnakeViz also launches it own page in its users own browser. After that, it is just a matter of clicking around and displaying information on any number of functions that were executed during the code profile run. I do not know if everyone will have the same experience that I did, but I found the interface simple and easy to use. When I start focusing on performance, I know I will spend a decent amount of time looking at the tottime column which displays the total amount of time that was spent in each function during the entire code profile run. I expect I will look at the top few items on that list and try and figure out why they are taking that much time. After making some guesses and changes to the code to match, rerunning the code profile run with the exact same data will be pivotal. While it is still at least a month or so off, I am looking forward to using this tool and making the code perform well! What Was My Experience So Far? In terms of actual issues that I fixed during this week, the count was either low or zero depending on who was doing the accounting. But the big win was getting most of the big-ticket items taken care of. To be honest, it was a lot of pressure off my mind getting those big items done. From my viewpoint, I am not sure that I would have felt okay with an initial release the project without those items being addressed. It was just good housekeeping, and now it was done! It just felt good to make the time to get those big-ticket items resolved. While it can be argued that there were not that many real changes to the code base, each of those changes made a significant impact to my confidence about the project. The refactoring to provide easy and private access to the tokens? It increased my confidence that I have not missed any weird side effects. The reorganizing and simple cleanup? It increased my confidence that I had extra \"stuff\" to maintain across the code base that would make things more complicated. Consolidating the List Block code? It increased my confidence that I have captured a healthy set of scenarios that properly test both Ordered List Blocks and Unordered List Blocks. Each change simply increased my confidence by safely reducing the amount of required code in the project. It also felt good for another reason: it was fun. Sure, there were boring parts, like making 2000 changes for the scenario test refactor. That was definitely not fun. But that was kind of fun because it was something for the project that was different. It was not the same old test-fix-run and repeat process that I had been doing for months. It was just something different. And that was just fun to do sometimes. What is Next? With most of the big-ticket tasks out of the way, I needed to buckle down and try and resolve as many of the Block Quote items in the unprioritized section as possible. While it would not be as fun as the big-ticket items, it knew they were worth doing and it would get me closer to an initial release. To be fair, in our household we have one honey-do list for me and one honey-do list for my spouse. And we both ignore some items until reminded and get some items off the list on our own. You know, typical list. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2021/01/11/markdown-linter-delving-into-the-issues-16/","loc":"https://jackdewinter.github.io/2021/01/11/markdown-linter-delving-into-the-issues-16/"},{"title":"Markdown Linter - Delving Into the Issues - 15","text":"Summary In my last article , I continued working on Block Quote issues and issues with Block Quotes and their interactions with List Blocks. In this article, I document that work that was done to address those issues and resolve them. Introduction With a couple of weeks off for the holidays and relatively light \"honey-do\" schedule around the house, I was hoping to have some good relaxing days along with some good productive project days. In my head, I was mentally starting to evaluate each of the remaining items in the list, assigning them a priority between 1 (highest) and 4 (lowest). I knew completing the Block Quote issues and Block Quote adjacent issues has a priority of 1, but I needed to take some time to think about the others. And while I was thinking about those others, it was a good time to get some solid work in! What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 22 Dec 2020 and 27 Dec 2020 . Creating A New Scenario Series After doing a block of work with Block Quotes, I thought it was a good time to start making a new scenario test group for them, Series N. To start this new series off, I simply copied and pasted example scenarios from other series and began to work through the permutations. Once I had those roughly filled out, I created the test_markdown_paragraph_series_n.py module to hold the scenario tests, and just started working on putting them together. After I had the first group of tests in Series N filled out, it was almost cut-and-paste to get the next group of tests ready, with a simple change to the specific Markdown element I was testing in that group. As nothing is ever easy (it seems…), I ran into some failures that I needed to fix. The first set of failures occurred with unquoted HTML Blocks, in that those HTML Blocks did not end the Block Quote element. This was quickly followed up by the same issue occurring with Atx Heading elements. As I had already solved this problem for List Blocks, it was quickly fixed by adding checks in the check_for_lazy_handling function for both the Atx Heading element starting ( LeafBlockProcessor.is_atx_heading ) or the HTML Block element starting ( HtmlHelper.is_html_block ). At the same time, to handle two extra list-based scenarios, I added the test_block_quotes_213aa function and test_block_quotes_213ab function as variations on the test_block_quotes_213a test function. I tried for a good hour or so to resolve the issues with all these functions, but I did not make a lot of progress. In the end, I got test function test_block_quotes_213ab working, but functions test_block_quotes_213a and test_block_quotes_213aa just did not want to seem to work. My main priority at the time was filling out the Series N set of tests, so I left those two tests disabled while I proceeded to fill out the Series N tests. Nothing exciting to report from that task… it was just a lot of moving, documenting, checking, and rechecking. You know, the normal stuff. Addressing Those Tests In looking good and hard at those disabled tests, I was able to eventually see that a good group of those tests were dealing with nested containers. Specifically, they were dealing with a Block Quote element inside of a List Block element, or the other way around. Putting the disabled tests, their Markdown documents, and their expected tokens under a metaphorical microscope, it was a while before I noticed something that I had previously missed. When I followed the examples closely, it looked like Block Quote elements within a List Block element were not being properly closed in each example. In debugging this issue, I added a handful of extra tests to help me analyze the situation. The Markdown for one of those tests, the test_block_quotes_extra_02a test function, is as follows: > start > - quote > > end and the list of tokens produced for that Markdown were: [ '[block-quote(1,1)::> \\n> \\n>\\n> ]', '[para(1,3):]', '[text(1,3):start:]', '[end-para:::True]', '[ulist(2,3):-::4: ]', '[para(2,5):]', '[text(2,5):quote:]', '[end-para:::True]', '[BLANK(3,2):]', '[para(4,3):]', '[text(4,3):end:]', '[end-para:::True]', '[end-ulist:::True]', '[end-block-quote:::True]' ] As I worked through the example, following the Markdown document and the parser's tokens, I discovered the issue. The Blank Line element on line 3 ended the first Paragraph element, and I was good with that. But more importantly, because that Paragraph element was closed due to the Blank Line being encountered, the new text on line 4 was not eligible to be included as paragraph continuation text. Based on that information, the new Paragraph element for line 4 needed to be created outside of the List Block element, as it did not have the indentation to support being included in the list. Basically, the end Unordered List Block token was in the wrong place. To fix it, I would need to move it above the occurrence of the Blank Line element. Fixing It Right! Being an issue with the tokenization of the Markdown Document, I needed to start with the parser and the BlockQuoteProcessor class. From the above information, I knew that I needed to alter the way in which the parser_state.handle_blank_line_fn function was called, as it was not properly closing the right blocks. After trying a few other things, I reverted to a very basic approach to solve the issue: forgo the complicated calculations and just do the simple calculation where needed. So, in the __handle_block_quote_section function, before the handle_blank_line_fn function is called, a quick calculation is done to see if the List Block should possibly be closed. If so, the actual index number is set in the forced_close_until_index variable. Then, making a small change to the handle_blank_line_fn function, that variable is passed in and all tokens on the stack are closed until they get to that point. At the time, it felt like I was using a large sledgehammer to fix a small problem, but it worked. The tokens were being generated properly, and I was able to move on from there. While I could them go on to describe all the problems I had with the Markdown transformer, I will spare any reader the pain. While the basic handling of Block Quote elements was working fine, the proper handling of indents within Block Quote elements needed a good solid day's overhaul to get it right. It was a good thing I was on holiday, because I had that time to devote to working through all the issues and getting the Markdown rehydration of Block Quotes and their indents just right. Following that, I expected a lot of issues with the consistency checks, but there was only one major issue. In the __validate_new_line function, the leading_text_index member variable of the Block Quote token was not being updated properly, resulting in an off-by-one error. Unlike the problems with the Markdown transformer, this issue took less than half an hour to find and fix. Phew! In the end, I was able to enable four of the test_block_quotes_213 test functions and all five of the new test functions that I had added. That was a good feeling. Still Missing One After a good night's worth of sleep, I got back to tackling the disabled functions, specifically test function test_block_quotes_213aa . With a Markdown document of: > - foo > - boo > - bar it was immediately obvious that this was somewhat related to the work that I completed the day before. It was just figuring out how that stumped me for a while. Adding some debug, I started to suspect that the \"can I start a container block\" logic in the __get_nested_container_starts function was not working properly. Adding more specific debugging in that function, my guess was confirmed. Specifically, there were cases where I felt it was obvious that a new List Block element should start, but the code was skipping over those chances. Stepping through that code, I noticed weird behavior when I got to this part of that function: nested_ulist_start , _ = ListBlockProcessor . is_ulist_start ( parser_state , line_to_parse , 0 , \"\" ) nested_olist_start , _ , _ , _ = ListBlockProcessor . is_olist_start ( parser_state , line_to_parse , 0 , \"\" ) nested_block_start = BlockQuoteProcessor . is_block_quote_start ( line_to_parse , 0 , \"\" ) Looking at the debug output for that code while debugging the test_block_quotes_213aa function, it was obvious that on the second line of the document, the code was not determining that a new block was starting. It just skipped right over it. Thinking through the issue while I was debugging, this started to make sense. It was not triggering on any of the above start checks because the wrong information was being presented to it. In fact, the data presented to it for line 2 of the above Markdown was literally two space characters followed by the Unsigned List Block start character ( - ). While that specific form of data made sense for the __get_nested_container_starts function, trying to invoke any one of the three Container block start functions with that data would never succeed. Each of those functions expected the line to at least be minimally processed, and that line of data clearly had not been. Trying the simple approach first, I tested the following change while debugging function test_block_quotes_213aa : after_ws_index , ex_whitespace = ParserHelper . extract_whitespace ( line_to_parse , 0 ) nested_ulist_start , _ = ListBlockProcessor . is_ulist_start ( parser_state , line_to_parse , after_ws_index , ex_whitespace ) and it worked! Debugging through the test again, that processing allowed the is_ulist_start to work with the processed line data, resulting in a non-None result being returned. After applying that same change to the other two start Container block functions, I ran the tests again and everything was good. The tests were passing without any additional changes being required of the Markdown transformer or the consistency checks. I was stoked! Making the Test Invocation More Efficient As I was making the changes to all these scenario tests, one thing was becoming obvious to me: I was not changing the call pattern. In the beginning of the project, I was concerned that I was going to have a wide array of ways to invoke the scenario tests based on need. While it was a valid concern at the time, it had not played out that way. For each scenario test, I was always adding the following boilerplate code: # Act actual_tokens = tokenizer . transform ( source_markdown ) actual_gfm = transformer . transform ( actual_tokens ) # Assert assert_if_lists_different ( expected_tokens , actual_tokens ) assert_if_strings_different ( expected_gfm , actual_gfm ) assert_token_consistency ( source_markdown , actual_tokens ) While those seven lines are not a lot, over the course of approximately 2,000 scenario tests, those lines added up. Thinking about it a bit, I realized that I was not going to change this format because it was consistently worked well for me. Declared before this code block in each test, the variables source_markdown , expected_tokens and expected_gfm held the relative information for each test. Once set, I could not think of any reason to alter from the pattern of calling the three validation functions, one after another. At the same time, if I was going to make this change on a large scale, I wanted to start out with a smaller scope of work and validate it first. At that point, I created a new act_and_assert function to contain those seven lines of code, and changed that code block from above to the following: # Act & Assert act_and_assert ( source_markdown , expected_gfm , expected_tokens ) After testing it out on a couple of scenario tests, I applied it to all the scenario tests in the test_markdown_paragraph_series_n.py module and the test_markdown_block_quotes.py module. Making sure the scenario tests all worked, including introducing some false failures that I immediately fixed, I decided to leave it alone for a while. My thought process was that I would let it \"set\" in my mind. If I was still interested in making the change after a week or so, I could work on it over the course of a couple of weeks. Link Reference Definitions and Containers Looking at the issues list, the following item gained my attention: - split up link definition within a block quote or list ? While I was not sure it was going to be easy, I at least figured that it would be fun! At the base level, it was an interesting question. Block Quote elements and List Block elements altered the way a line was processed, and Link Reference Definitions could span lines and not appear to follow the right rules. To that end, I added three new scenario tests test_block_quotes_extra_03x to test_block_quotes_extra_03b and I added three new scenario tests test_list_items_extra_01x to test_list_items_extra_01b . The bad news first. Try as I may, I was not able to get the new Block Quote tests to pass within a decent amount of time. As such, I had to commit them as disabled tests. The ContainerBlockProcessor was largely unchanged, with the changes in that class being made to get rid of code or simplify code, rather than fix issues. The Markdown transformer was altered to produce the proper output if there were any leading spaces in the list that were not being merged in by the Link Reference Definition token. Those were the easier two fixes. The hard changes involved the HTML transformer. List Looseness… Again While I thought all my issues regarding list looseness were behind me, I was wrong. Specifically: A list is loose if any of its constituent list items are separated by blank lines, or if any of its constituent list items directly contain two block-level elements with a blank line between them. Breaking that down, a list is lose if, for any item within the list: it is not the first item and preceded by a blank line it is not the last item and followed by a blank line any item contains any two block elements that are separated by a blank line How is this related to Link Reference Definitions? When the PyMarkdown processors parse a Link Reference Definition element, it creates a token that represents that element so it can be analyzed. However, from an HTML point of view, it does not exist. As such, the following Markdown: - [ foo ] : / url \"title\" - [ foo ] is for HTML output purposes equal to the Markdown: - - [ foo ] Basically, from the HTML transformer point of view, it is a Blank Line element. I had some code in the __is_token_loose function to report a Link Reference Definition as a Blank Line element and in the __calculate_list_looseness to trigger off a block, but it was not that simple. Rereading the above section on looseness, it only really mattered inside of the list item if that Blank Line element was between two blocks. Fixing that specific case took a bit of work in the __calculate_list_looseness function to redesign the way I was handling those cases. In cases where the previous token was a Blank Line, I had to change the algorithm to look before that element and see if there was a valid block token there. It so, the logic from the last point above would come into play, and it would discover a valid case to check. Otherwise, the Blank Line token or the Link Reference Definition token would be intentionally overlooked, and the search would continue. Repeat, With Block Quotes It was no surprise that I had to do similar work with Block Quote elements to make sure that they were also working properly. To accommodate that, I added five new scenarios and did a lot of debugging. In the end, the fix was to add a flag in the __handle_blank_line function that triggered when a Link Referenced Definition had been started. Rather than avoid the default handling in this case, that handle was carried out and appended to the already created tokens, resulting the proper output. Once that fix was made, getting the consistency checks to agree with it was easy, with one calculation for the leading_text_index being off by 1. After a simple adjustment to compensate for that case, those tests were now passing. After only an hour and a half of work to get this working, I understood that I was lucky that it did not end up taking as long as the last set of issues to fix. I was grateful! Evaluating How I Was Spending My Holiday I do not think that I am a workaholic, I just like to keep busy. Whether it is figuring out how to do something with home automation, working on a puzzle, cleaning up the garage (again!?), or working on this project, it does not matter. If it is something that keeps me from spending hours doing one thing with no recognizable thing to show for it, I am good. Spending time with my wife helping her out with shopping works wonderfully for that. I get to spend time doing crossword puzzles and other fun things on my phone while waiting for her to finish at various stores she needs to go to. For me, it is all about perspective. So, it was part of the way through Christmas Eve Day when I thought about this with respect to the project. While it was true that I was resolving items from the issues list, it just was not feeling like it was the right thing to do at that time. As I had the weeks before and after Christmas off, I sat back and decided that I wanted to do something for the project that would be hard to do at any other time. I mean, since I have the time, was I using it to my best advantage? Within minutes, I had an answer of \"No, I was not using my time wisely!\". I had a couple of long-term things I wanted to do with the code base, but those things would be hard to fit into a normal week's worth of work. It was time to shift gears. Reducing the Code Required To Check For A Specific Token While this was done very leisurly over two days, this change was something I had wanted to do for a while. As the project's growth was organic, I had started out check for the pressence of a given token by looking at its name. At that time, given the rule module rule_md_019.py , if I wanted to see if a given token was a paragraph, I would use: if token . type_name == MarkdownToken . token_paragraph : pass There is nothing intrinsically wrong with that comparison, but it did have things going against it. The first was that I had to make sure to import the MarkdownToken class into any module where I wanted such a comparison. Next was my observation that I usually had to go to the markdown_token.py module and find out exactly how the token's type name was spelled, hopefully avoiding any naming errors. Finally, I found it bulky. I guess if I had to add more description to bulky, it was that it took an awful lot of typing to figure out that the token was a paragraph. And if it took a lot of typing, it would also take a lot of reading to do the same thing. I needed something simpler. Working through a couple of possibilities, I decided to use a simple approach and replace that entire line with: if token . is_paragraph : pass It required no extra imports, the naming was easy to remember, and it was as simple as I could make it without short forms that would make it to read. Sure is_para might be more compact, but what was I really going to save with that? Over the course of approximately ten commits, I transferred all the Markdown Tokens from the old way of comparing token types to the new way. I started with Paragraphs and the other Leaf Block tokens, moving on to Container Block tokens, before finishing up with Inline tokens. By the time I was done, there were more lines and functions present in the base MarkdownToken class, but the entire code base seemed to read better. And that was the point of these changes. No scenario tests were changed as a part of this code base change, I just wanted it to read better! Do I Make The Code More Object Oriented? Having made those changes, I felt emboldened to make another set of changes that I had been itching to do for a while: making the code more object oriented. While each programming paradigm has its good points and bad points, object-oriented was something I have had a lot of experience with. At its core object-oriented programming had three pivotal concepts: encapsulation, composition, and polymorphism. As the PyMarkdown project generates tokens after parsing the Markdown document, I was most interested in encapsulation and polymorphism and how they could be applied to tokens. From an encapsulation point of view, with a couple of exceptions 1 there is no reason to want to change the values of any of those parsed tokens. In the case of those exceptional tokens, there is a multiline concept present in that token that needs to be properly tracked by any after-parsing process. Other than providing that tracking information during a specific after-parsing process, those fields do not have any meaning to a consumer. Therefore, it is okay that they are exposed. However, I did not feel that it was the case with the other values. And from a polymorphism point of view, it makes sense to me that the tokens should be organized into groups of tokens that have similar properties: Container Blocks, Leaf Blocks, and Inlines. As such, it also follows that some properties and methods are present in one or more of those groups of tokens, but not all those groups. So having a base class for each of those groups is something to work towards. For both reasons, it just made sense to me to proceed! Starting with EndMarkdownToken With the decision made to move forward with this transition, I started at the base by making those changes with the EndMarkdownToken class. To be honest, as I worked through the project's development, I always felt a bit guilty that I had let the use of this token get out of control. From my observations, this class is a \"hidden\" fourth class of tokens, providing a close token to any of the tokens that provide for start functionality. As this functionality ranges across all three groups of tokens, the EndMarkdownToken exists off to the side in its own space. Because of this special grouping, it was the ideal token to start with. To change this token, I started off with the EndMarkdownToken.type_name_prefix field which contains the string that gets prepended to the start of any EndMarkdownToken type name. Moving it into the MarkdownToken class made the use of that field self-contained, allowing it be modified into a protected field. 2 To accomplish that change, I created a new generate_close_markdown_token_from_markdown_token function that allowed me to generate the proper EndMarkdownToken from a given Markdown token. To me, this just made sense. By performing these changes, I had an easy way of generating an EndMarkdownToken that was documented and set all the right fields. I was able to make sure that the start_markdown_token field was set. As it had not been set consistently, there were places in the code base where I had to do work arounds to try and figure out where the start token for a given end token was. This was just cleaner. Simple Refactorings Two of the paradigms that have served me well in my years of software development are D.R.Y. and S.O.L.I.D . The ‘L\" in S.O.L.I.D. stands for the Liskov substitution principle , which (in its abbreviated form) states that an object should be able to be replaced with another object that adheres to the same contract without any parent objects being aware of the change. With respect to the PyMarkdown project, I had been using the isinstance built-in to deal with things instead of following the principle. However, with the recent round of refactorings completed, I was able to fix that. While it may not seem like much, it made me feel better to change: if isinstance ( token , AtxHeadingMarkdownToken ): pass into a more SOLID-like: if token . is_atx_heading : pass While I know that there is only one implementation of the AtxHeadingMarkdownToken class, I was just not comfortable with having calls of isinstance scattered throughout the code base. Over two commits, I was able to eliminate all uses of isinstance except for six legitimate uses of it in pivotal parts of the code base. As with the other changes, this change was a simple change, but an effective one. With all the checks for various tokens using a single manner of access, it was now easier to scan the code base for locations where a given token was referenced. That, and to me, it just looked neater. Cleaning Up The Leaf Block Tokens and EndMarkdownToken All those changes inspired me, and I gave myself the green light to further encapsulate the private fields as Python private fields. It was not even remotely exciting, but it was something I knew would make the code cleaner and more maintainable. Using this stripped-down example of the EndMarkdownToken , this is where all the tokens started: class EndMarkdownToken ( MarkdownToken ): def __init__ ( self , type_name , ): self . type_name = type_name For each of the Leaf Block tokens, I then started by adding the two underscores to the start of the field name, per Python requirements. But as that made the field \"invisible\" to other objects, I then needed to add a new function with the property decorator to be able to retrieve the value. class EndMarkdownToken ( MarkdownToken ): def __init__ ( self , type_name , ): self . __type_name = type_name @property def type_name ( self ): return self . __type_name Lather, rinse, repeat. Nothing special or exciting, which was what I was going for. There were no weird cases with these tokens that required a special setter function, so no workarounds were needed. And because the previously public field name and the newly added property name had the exact same name, so changes were needed in other parts of the code. What Was My Experience So Far? I really cannot lie on this one. While it was good to resolve some items off the issues list, it was really good to get these refactorings done. I have had those ideas on how to clean up the project in my mind for weeks, and just getting them done gave me a bit of an extra pep in my step. It is not that they were any more or less important than the other items, it is just that their large scope meant it was going to be hard to fit them in. As such, they always went into the \"if I ever have some time\" bucket… and never seen again. I honestly think that taking the time to focus on what was important to me with this project was good for me and the project. It helped me refocus on the things that I find important. It helped the project by being able to present a more concise and readable code base to any interested parties. In all, it was a win-win. What is Next? With a week left of holidays, I had hoped to get a similar mix of items dealt with in the next week as I had the week before. Stay tuned! The exceptions as the rehydrate_index field for Paragraph tokens, the leading_spaces_index field for List Block tokens, and the leading_text_index field for Block Quote tokens. ↩ Python does not have a protected field specifically. The common practice of naming fields to be used in a protected-like manner is to preface them with a single _ . ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2021/01/04/markdown-linter-delving-into-the-issues-15/","loc":"https://jackdewinter.github.io/2021/01/04/markdown-linter-delving-into-the-issues-15/"},{"title":"Markdown Linter - Delving Into the Issues - 14","text":"Summary In my last article , I started the transition to working on Block Quote issues. Having made good progress with those issues, this week I continued with those issues, sometimes blurring the line between Block Quote issues and Block Quote/List Block interactions. Introduction Now firmly in the mode of dealing with Block Quote block issues, I was looking forward to making more progress in this area. With each week that passed, I was becoming more aware of how close I was to be able to at least do an initial release of the project. Block Quotes, and possibly Block Quote/List Block interactions are the last big thing that I need to do before taking that leap, so I just needed to buckle down and work through those issues, wherever they would lead. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 16 Dec 2020 and 20 Dec 2020 . Fixing Things Up Before Proceeding Before I started doing a lot of work for the week, I decided I needed to get one of the disabled functions working: test_list_items_282 . While it was not a tremendously important issue, it was one that I had put off for a while, and I just wanted it out of the way. Following the GFM Specification exactly, it can be argued that the following Markdown: 1 . foo 2 . bar 3 ) baz should either be parsed as a list with 2 items or two lists. The crux of the issue is whether the third line is considered a continuation of the second line or if it is an entirely separate list. Basically, it all comes down to how your parser implements paragraphs and paragraph continuations. Luckily, one of the primary authors of the specification (@jgm) chimed in with this following bit of information: The intent was to have language that applies to the starting of list items in the middle of a paragraph that isn't a direct child of a list item at the same level. While some of the other help was not as useful, that one sentence was. I did agree with the other author that Paragraph elements in Markdown are the default element, however I was not as sure that his interpretation of paragraph continuations was correct. But this information helped me out a lot. Using that information, I was able to quickly put together these lines: if parser_state . token_stack [ - 2 ] . is_list : is_sub_list = ( start_index >= parser_state . token_stack [ - 2 ] . indent_level ) Following the suggestion of @jgm, I changed the exclusion code at the end of the is_olist_start function to include a reference to is_sub_list . Following my usual process, I was quick to find that everything just fell into place, and the test was passing. But I was not confident that I had properly fixed the issue, so I created four variants of the test data, each just a little different from each other. It was only when all five tests had passed that I considered the issue resolved and dealt with. An Easy Set of Tests Some of these items take days to complete and some take hours. Until I start working on them, I never know which bucket they will end up in. Therefore, when I started working on this item: - 634 a in bq and in list I had no clue what was going to happen. Starting with the basics, I looked at the Markdown for function test_raw_html_634 which was: < a />< b2 data = \"foo\" >< c > As the start of the Markdown does not have a single HTML tag, or a HTML tag from one of the special groups, an HTML block is ruled out. But when the processing happens for a Paragraph element, the inside of that paragraph is then filled with Raw HTML elements, only slightly altering the Markdown when rendered as HTML: < p >< a />< b2 data = \"foo\" >< c >< p /> As the item suggests, placing that same Markdown inside of a List Block or a Block Quote should result in the same behavior, just inside of that block. Crossing my fingers for good luck, I created two variants of that Markdown: one that was prefixed with - and the other that was prefixed with > . Things flowed quickly through my usual process and I was happy to find that these scenarios both worked without any additional changes being needed. More Fun with Block Quotes With good luck occurring for my last item, I hoped it would carry on to my next item. So, when I looked for more work to round out the Block Quotes tests, I came across the following group of items in the issues list: - 300 with different list following - 300 with extra indent on following item - 301 , but with extra levels of block quotes - 301 , with indented code blocks Rather than tackling them separately, I decided to tackle them together as a group. The first part of that work was making the requested variations on the data for test function test_list_items_300 . That data was somewhat simple, with the Markdown text of: * a > b > * c To address the first item, I added test function test_list_items_300a that included an Ordered List element after the Block Quote element, instead of an Unordered List element. Test function test_list_items_300b addressed the second item by keeping that List Item element as an Unordered List element but adding 2 space characters before it to make it a sublist. Similarly, test function test_list_items_301 has a Markdown text of: - a > b ``` c ``` - d The new variation of this Markdown in test function test_list_items_301a was to change the second line to start two Block Quote elements instead of one. Test function test_list_items_301b modified the test data slightly by indenting each line of the Fenced Code Block element by one space. Test function test_list_items_301c did a more drastic change by replacing the Fenced Code Block element with the single character c indented from the start of the Block Quote element. Finally, the test_list_items_301d function did a more correct version of test function test_list_items_301c by including a blank line and a c character indented by four spaces, but properly enclosing them within the Block Quote started on line 2. With those changes made, it was time to get down to figuring out if there were any problems and dealing with them! Working The Problem Starting with my normal process for working through new test functions, I worked through the tokens and HTML for each of these functions. Apart from the functions test_list_items_301b and test_list_items_301c , the other tests were producing the tokens that I expected them to. After trying to get those two functions working properly for an hour or so, I decided to put them on hold while I got the other functions cleared up. Focusing on those other test functions, the HTML output was mostly there, but required a small amount of fiddling. Specifically, tests in which there was a Blank Line element within a Block Quote element within a List Block element, the Blank Line within the Block Quote was being used to determine whether the list was loose. As that Blank Line was within another Block and not within the List Block itself, it should not have been affecting the calculation of List Block looseness. Luckily, the fix for this was to add nine lines of code to the __calculate_list_looseness function to properly increase and decrease the stack_count variable to account for the Block Quote token. With the tokens and HTML output deal with, it was time to deal with the rehydrated Markdown and the consistency checks. The fix to the Markdown was an easy one to see: the whitespace allocated to the Block Quote tokens was not being added back into the Markdown text that was generated. Some easy changes to incorporate that information was almost as easy to add, leaving the consistency checks. While the consistency checks took a bit, in retrospect they were somewhat easy to understand and fix. At the time though, it took a bit of effort to work through them. Like the changes in the Markdown generator, changes needed to be introduced to properly track which part of the Block Quote's extracted whitespace was applied in the Markdown generator. That tracking is done using the Block Quote token's leading_text_index variable, which was not being properly incremented to track the newlines used within the owning Block Quote token. Once that change was done, things were looking a lot better, but there was a single case where there was an index error getting that whitespace out of the token. Upon debugging, it was obvious that the leading_text_index variable was getting incremented twice. Fixing that took a bit of passing information around but was quickly taken care of. And with that fix in place, each of the tests that I was working on was solved and passing properly. Not Everything Was Solved With the other tests passing cleanly, I refocused my efforts on test functions test_list_items_301b and test_list_items_301c . Doing my usual research into the issues, I started to figure out what the tokens generating by the parser should be, comparing that answer with what was being generated. It was then that I noticed that the tokens for the tests were close to normal, but not correct. In both test functions, the tokens mostly differed in where the one element stopped and the next one started. Now, when I say, \"I fiddled with the code\", I really mean I tried normal paths and interesting paths to try and solve the issue. And it was similar with these issues. After around two hours of fiddling, I was no closer to having a solution than when I first looking at the problem. In the end, after doing a fair amount of research and debugging, I decided that I was going to commit the code with functions test_list_items_301b and test_list_items_301c disabled. I just was not getting the right \"angle\" on solving those issues. I just felt it would be better to commit what I had and work on those two functions in the next couple of days, so that is what I did! Dealing With 301B During that Saturday, I decided to take another shot at test function test_list_items_301b . I knew the the tokens were just wrong. To quantify that wrongness, the Markdown text: - a > b ``` c ``` - d was produced a Block Quote element that tried to include an empty Fenced Code Block element into the Block Quote, leaving the c character by itself on a line outside of the element, followed by another empty Fenced Control Block. From prior work on List Blocks, I knew that the Fenced Code Block should terminate the Block Quote element, as Block Quotes follow similar roads. It was just a matter of figuring out how to get there. Knowing that I have had tried to solve this problem the day before, I decided to take the hard-line approach of debugging line-by-line through some of the code. I don't usually do this as it is very time consuming and requires meticulous notes. While I am comfortable with doing that work, there are just more efficient ways of getting to the same target. But with those ways not working for this problem, it was down to the nitty-gritty. It was a good thing that I used this approach because it told me something interesting. Usually, at the start of processing, the code separates the line into extracted whitespace and parseable line. In this case, the debugger was showing me that the separation that I had expected was not done. As such, when the parser looked at the raw data for the line, the spaces at the start of the line prevented that line from being recognized as the start of a Fenced Code Block. The good news here was that adding these four lines to the __process_lazy_lines function made everything work: after_ws_index , ex_whitespace = ParserHelper . extract_whitespace ( line_to_parse , 0 ) remaining_line = line_to_parse [ after_ws_index :] Basically, I just took the same code that split the original line_to_parse variable in the main-line processor and added it here. With that code in place, the is_fenced_code_block function did the rest, properly noticing the Fenced Code Block start sequence, and properly starting the code block within the Block Quote. Dealing With 301C It was just after 6pm PST when I committed the changes for test_list_items_301b , and I decided to start working on test_list_items_301c . In this case, it was not the tokens that were wrong, but the spacing included from one of the lines within a Block Quote element. Doing research into the night, I still was unable to figure out what the magic sequence was to get this working properly. Rather than press on, I decided to take a break and spend some time with my family, relaxing myself and my brain. This worked well. Starting again in the morning, I was able to quickly see that there were two problems that I needed to address. The first of those problems was that the function __adjust_paragraph_for_block_quotes was always adding an empty string to the Block Quote token, which was the problem I was trying to solve. The second problem was that it was a great solution for most of the time, this specific test being one of the few times where it was not. With that fresh information, I started experimenting and I was able to isolate code in the parse_line_for_container_blocks function that affected the outcome of the test. The fun part was that if I took the indentation used for the list and added it to the token's data, test test_list_items_301c worked, but other tests failed. Doing some extra plumbing and experimentation, I narrowed the active scenario down to specifically only trigger when the parse_line_for_container_blocks function was triggered within a paragraph that was within a block quote. [more] What Was My Experience So Far? Having felt dread at the prospect of Block Quotes taking as long to complete as List blocks did, I am happy to report that the feeling is quickly fading. The sense I am getting from the issues I am looking at are that there are issues to deal with, but nothing I have not dealt with already, just variations on it. On top of that, there are only one type of Block Quotes to worry about with a very simple start sequence. I was very solidly feeling that Block Quotes were going to be a lot easier than List blocks. That did not mean things were going to be easy though! I was starting to come close to the end of the initial set of issues that I had added to the issues list, but I was adding more as I looked through the code. This week, I was able to get rid of a handful of those issues, and it felt good. But with approximately 70 lines of items before hitting those that dealt with Rules, Tabs, and Correctness, it was a sobering reminder that I just needed to get stuff done and done cleanly. And for me, that often poses a problem. There are things that I want to do and feel that I should be doing, and there are things that I need to do. Resolving any issue that deals with Tab characters? Now that is a want, and I can live with a \"stupid\" translation of Tabs until I can get the time to make them right. Resolving any issues that might uncover scenarios that I have not covered yet. To the best of my abilities, that is a must. But there is a grey area between those two. Take the item: - image token handling confusing and non - standard I would like to get it resolved, but I am not sure that I need to do that. I must balance my desire for wanting things done right and delaying any release with my knowledge that I can live with it for a couple of months while I get the project out. And those are the types of decisions that I am going to have to make more and more as this project gets to its closing stages. Do I really need it, or can it wait? At least I know it is going to be fun! What is Next? I was just as happy to get some holiday time off in which I could spend more time with my family as I was to be able to make some solid holiday progress. This next week was going to be a good mixture of resolving solid issues, test cleanup, and code cleanup. Stay tuned!","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/12/28/markdown-linter-delving-into-the-issues-14/","loc":"https://jackdewinter.github.io/2020/12/28/markdown-linter-delving-into-the-issues-14/"},{"title":"Markdown Linter - Delving Into the Issues - 13","text":"Summary In my last article , I worked diligently to resolve all the tests that I had marked as disabled in the previous week. After cleaning up those issues, I finished cleaning up the remaining List Block issues before getting back up to speed on Block Quote issues. Introduction After a week of digging deep to resolve the disabled tests, I checked the issues list. With only a handful of items left in the List Blocks section, I figured it was a good time to make a push to finish with the List Blocks and start with the Block Quotes. Knowing that I was about to do that transition made me happy and filled me with dread at the same time. It made me happy as I was more aware of how close I was getting to the end of the first phase of the project. It also filled me with a sense of dread because I had not done any serious work with Block Quotes in a while. As such, I am not sure if the effort to get the Block Quotes to the same level as List Blocks will be the same. I hope it is significantly less, but we will see. If I think about it, I believe that sense of dread is from not knowing how long it will take to address any issues that arise. I do know, that unless I finish up the List Blocks and start the Block Quotes, that feeling will be stuck at dread. So, time to buckle down and get stuff done! What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 10 Dec 2020 and 13 Dec 2020 . Closing Out List Blocks Having finished cleaning up the disabled scenario tests, there were a few items on the issues list that I needed to get out of the way. While not high-profile items, those items were standing between myself and a list with no list block related items on it. That was motivation enough to put these items at the top of my list. Adding Variations of Existing Patterns During the previous three weeks, I had taken the Series M scenario tests and filled them out a lot. As I filled them out, I did find parsing and transformation issues, all which got fixed. But in creating that series, I needed to take some alternate path to properly exercise the specific patterns that I was trying to make sure were covered by the tests. In doing so, I often veered away from the correct variation to faithfully create the right pattern in the group of scenario tests that I was working on. Confused? Hopefully, a concrete example will explain. I started going through the tests in each group, starting with Fenced Code Blocks and the scenario test function test_paragraph_series_m_ol_nl_fb : 1 . ``` foo ``` The purpose of that specific test was to make sure that a Fenced Code Block following an empty Order List start element was parsed properly. However, from my point of view, there was another variation of that test that was possible. Creating the new scenario test function test_paragraph_series_m_ol_nl_all_i3_fb , I altered the Markdown slightly: 1 . ``` foo ``` Technically that was not a big change, but it was a significant one to me. Instead of testing a variation where the Fenced Code Block terminates the List Block, it instead tests the inclusion of that Fenced Code Block into the List Block. I consider that the most correct form of that test. At its base, that reason for that classification is that if I said I wanted \"an empty list with a fenced code block\", that is what I would expect. Nothing more, nothing less, just a simple example that had that Markdown in its simplest form. I could have repeated that exercise with a lot of the scenario tests, but I stayed with four tests from each group: two tests with a single list and two tests with a sublist. Starting with Fenced Control Blocks, I then proceded to HTML Blocks, Indented Code Blocks, and SetExt Headings. I hoped that the new scenario tests were covering old scenarios, but I was not 100% confident that they were. But as I executed those tests, one by one I was convinced they were old scenarios, just being covered by the Series M tests as a group. Not one of the scenario tests failed! After my usual process of adopting the scenario tests, each test passed on its first try, requiring no changes to the parser, the transformers, or the consistency checks. In the end, I was able to mark all these items from the issues list as resolved: - variations of list + fenced block with proper indent for actual block - variations of list + html block with proper indent for actual block - variations of list + indent block with proper indent for actual block - variations of list + set ext with proper indent for actual block Lazy Continuations and Nested List Blocks Another easy issue to get off the list was this one: - 2 - 3 levels of lists with lazy continuation lines Based on the work I had just completed to round out the Series M tests, I had confidence that these would also not require any changes. It was a bit of an educated guess, but I believe that all I wanted to do here is to provide different levels of List Blocks, ensuring that the principle of lazy continuation lines in lists was being adhered to. Starting with test function test_list_blocks_extra_5a , I added the following Markdown: 1 . abc def ensuring that text def continued the paragraph started in the level 1 List Block. Three tests later, the test function test_list_blocks_extra_5d was testing the level 4 List Block with the Markdown: 1 . abc 1 . abc 1 . abc 1 . abc def This was a simple set of scenarioes that passed right away, but I had expected them to. However, with some of the issues that I have had with List Blocks and lazy continuation lines, it was good to have some tests explicitly covering these cases. Simple Cleanup Having done some rewriting of code lately, I was debugging and found a couple of lines in the __handle_blank_line function that were not being used anymore. With the breadth of scenario tests and summaries of the code coverage of those tests also in place, it was easy to determine and test to make sure this code was no longer being used. With that observation verified, that code was removed. Getting Clarity For a while, I have been convinced that I coded something wrong, but have not been able to prove whether it was correct or not correct. The only thing that I was convinced of was that I needed to deal with this at some point. With this being the last issue in the List Block section, it was time to deal with it. - blank line ending a list is parsed wrong into tokens - >> stack_count >> 0 >> #9 :[ end - ulist ] - should be end and then blank , as the blank is outside of the list - 233 and 235 , should blank and end - list tokens be reversed ? The prototypical example of this was test function test_list_blocks_233 : - one two While the HTML output for that Markdown was correct, I had questions about whether I was emitting the following tokens in the correct order: expected_tokens = [ \"[ulist(1,1):-::2:]\" , \"[para(1,3):]\" , \"[text(1,3):one:]\" , \"[end-para:::True]\" , \"[BLANK(2,1):]\" , \"[end-ulist:::True]\" , \"[para(3,2): ]\" , \"[text(3,2):two:]\" , \"[end-para:::True]\" , ] Specifically, my question was around the Blank token and the end Unordered List token. Should the tokens be in Blank/Unordered order, or Unordered/Blank order? Over the weeks that I have looked at this case, I had never taken the time to sit down and work through it. As it was the last item for List Blocks in the Issue List, it was time. Doing the Dirty Work This may appear to be an easy case to some people, but I had issues with it. Thinking about it at length, it felt that my understanding of this problem was influenced by which part of the GFM Specification I had last dealt with. So, to deal with that influence head-on, I re-read the parts of the specification dealing with List Blocks, Paragraph Blocks, Blank Lines, and lazy continuation lines. With that information in my head, I started to work through the problem logically. Starting at the beginning, line 1 starts the tokens off with the first three tokens of the document, leaving an Unordered List item active and a Paragraph element open. When the Blank Line element in line 2 is encountered, it closes the Paragraph element but leaves the Unordered List and Unordered List item open. Therefore, the fourth and fifth tokens are generated and added to the document. It was at this point in working the problem that the clarity surrounded this problem crystalized and became clear in my mind. I am not sure why, but I had wrongly believed that the Blank Line element on line 2 not only closed the Paragraph Block but closed the List Block and List Block item as well. The Markdown for example 240 clearly shows this: - foo bar as the Markdown is trnslated into a single Unordered List Block with a single item that contains two paragraphs: < ul > < li > < p > foo </ p > < p > bar </ p > </ li > </ ul > This meant that when line 3 is interpreted, the List Block and List Block item are still open, but the previous Paragraph element was closed by the Blank Line. As such, line 3 is not eligible for consideration as a lazy continuation line. With that option removed, the single leading space character is not enough leading space to keep line 3 in the List Block, so that block is closed, and a new Paragraph element is opened with the contents of line 3. It took a bit of work and a straight head to work through, but I had my answer! To make sure I did not forget about it, I added a comment to function test_list_blocks_232 to make sure I can look back at it when I need to. While this was not something that required a code solution, knowing that this issue was finally (and definitively) resolved brought a smile to my face! Starting with Block Quotes With that last issue, all the specifically List Block related items were crossed off the issues list. It was now time to ease myself into work on Block Quotes and getting them up to a comparable level as I had reached with List Blocks. Starting Out Easy I decided to start with an easy item: - \" # TODO add case with > \" for tests In the beginning, each of the scenario functions in the range test_block_quotes_212 to test_block_quotes_216 were simple tests that showed how lazy continuation lines work with Block Quote elements. One of the observations that I made when adding those tests was that, to properly test lazy continuation lines, the removed > character that makes the line \"lazy\" should be able to be inserted without changing the HTML output. Basically, according to the GFM Specification: If a string of lines Ls constitute a block quote with contents Bs, then the result of deleting the initial block quote marker from one or more lines in which the next non-whitespace character after the block quote marker is paragraph continuation text is a block quote with Bs as its content. To properly test this, I created a variant functions of each of those five scenario tests, and in each case I added a variation with the > character at the start of the line. As I worked through the scenarios, all the variant tests were working fine except for function test_block_quotes_213a . Looking at what made that test different, the answer was obvious: it involved List Blocks. Even after adding other variants of this test, I was unable to get any of them working. I was not 100% sure it was the right thing to do, but in the name of progress, I marked the test functions test_block_quotes_213a to test_block_quotes_213d as disabled, knowing I would get back to them when testing Block Quotes and List Blocks and how they interacted. Three Quick Reviews The next three items that I resolved were all easy issues to resolve. The first item was the removal of a piece of code that was no longer being used: if is_in_paragraph and at_end_of_line and is_first_item_in_list : is_start = False The next item was to remove the poorly worded item from the list and replace it with one that specified the problem more clearly: - unify 2 separate calculations in `__pre_list` function And finally, the last item was just to review the existing tests and make sure that agreed with their current state: - 228 and 229 - what is the proper line / col for \" >>> \" ? None of these were tough tasks to undertake, but they were all helping me to get back up to speed on parsing Block Quotes. Mixed Levels of Block Quotes and Serendipity Going through the list looking for other easy items to resolve, this one caught my attention: - block quotes that start and stop i . e . > then >> then > then >>> , etc . To me, this looked like an easy issue to tackle. The test function test_block_quotes_229 was a good base to start with. However, I felt there needed to be a bit more data, so instead of three lines with varying numbers of Block Quote start characters, I created test function test_block_quotes_229a as follows: > 1 >> 2 > 1 >> > 3 > > 2 and test function test_block_quotes_229b with the same content, just blank lines between each of the original lines. Basically, the first test would verify how the different lines worked together, and the blank lines in the second test would verify how each line worked isolated from any other lines. Serendipity Except for some moving code around to make sure it looked correct, only one real change needed to be done to get the tests working. It was an interesting thing to run into, but it was also a lucky break for me. If I had selected any other text for each line, things would have worked fine, and I would be none the wiser. However, with the given content for each line, the parser thought that the number for each line was a possible start for an Ordered List Block. As such, it consumed the digit and then looked for the . or ) character to follow it. When the end of the line was encountered instead of one of those characters, an IndexError: string index out of range error was thrown. While this was quickly fixed by only setting that variable if the start of the List Block had been confirmed, it was a good issue to find! Building Up Test Coverage These issues were ones that I used to start the process of building up to the same level of coverage for various Block Quote scenario groups as I had done with List Blocks. This effort was in response to the following issue list items: - tests like cov2 with blank before , after , and both for html blocks and other blocks - tests like cov2 with multiple lines for block items , like html - all leaf in all container To accomplish this, variations of Paragraph Blocks were added to function test_block_quotes_211 , Thematic Breaks to function test_block_quotes_212 , Indented Code Blocks to function test_block_quotes_214 , and Fenced Code Blocks to function test_block_quotes_215 . After adding 10 new scenario tests to address these issues, I felt that this was a good start to addressing the issue of coverage for Block Quotes. With all those changes completed, I followed my usual process of verifying each scenario test. Except for one issue with the Markdown transformer, the tests all passed without incident. That one issue was that the Markdown rehydration for test function test_block_quotes_214d included an extra \\x03 character in the output. As I have mentioned in previous articles , that character is a NOOP character, and is used to essentially place a \"Blank\" in the tokens that can be removed if not needed. In this case, that NOOP character was added to the content of the Text token to indicate that a Blank Line was part of that content. As that information was not needed for the translation back to Markdown, I eventually added a call to the function ParserHelper.resolve_noops_from_text at the end of the __perform_container_post_processing_lists function to remove that extra character. While I knew that I needed to add a call to that function at some point in the processing chain, it took an hour or so of experimentation to find the right place to insert that call at. Until I found and tested that location, I found lots of locations where test function test_block_quotes_214d was passing, but other test functions started failing. It was frustrating, but I was able to work through all that noise and find the right place, which was satisfying! Sunday Morning Relaxing With a solid amount of work completed during the week, I found myself sitting in front of my computer on another Sunday morning, wanting to get another issue resolved. I do not want anyone thinking that I am workaholic, as I am not. Sunday mornings in our household are mostly for whatever personal projects we are working on. As such, I choose to get up early on Sundays and try and get a couple more issues resolved from one of the projects that I am working on, before the family projects start taking control of my day. Knowing that it was going to take a couple of hours, and having a couple of hours of peace and quiet available, I started looking at the following item: - blank lines as part of bquote - compare test_block_quotes_218 vs test_blank_lines_197a - already fixed test_list_blocks_260 , 257 While it may not seem like much, the positioning and whitespace of those blank lines are just slightly off. Looking at this back at the end of June 2020, I noticed that within containers, the column number for blank lines was off. Specifically, given this Markdown: - foo - \\ a \\ a \\ a - bar (where the \\a character is a visual indicator for a space character), the token for the blank line was being calculated as [BLANK(2,5):] . While that is one possible answer to Blank Line token for that line, it has issues. Specifically, because it is within a List Block, the consistency checking had issues with that line because it did not appear to have the correct indentation. After thinking about it, I eventually settled on the correct form of that token being [BLANK(2,2): ] . As the spaces were all that was on the line, I figured that it was more correct to say that the column number was 2 followed by three space characters than a column number of 3 followed by two space characters. Influenced by example 257 , I have confidence that I made the correct choice, backed up by the commit I made on 28 Jun 2020 with that choice and the fallout from that choice. At that time, I was focusing on List Blocks, and I added the item into the issues list to fix this for Block Quotes, and it was now time to fix that. Unlike the fixes required to resolve this for List Blocks, the fixes required to resolve this for Block Quotes were relatively small. The first part of that change was to set the column number to the length of the text removed by the owning container blocks. This firmly set that column number to the first character after the container processing, removing the determination of the column number from the leaf block processing. To balance that out, the calculation for initial whitespace to allow for a Blank Line token within a Block Quote was set to the amount of whitespace that was extracted. Other than that, no other changes were made. What Was My Experience So Far? As I mentioned earlier, it was a relief to wrap up the verification of the List Blocks and moving on to Block Quotes. But with that transition, there was also a sense of dread that I felt as I started on Block Quotes. Would getting a solid amount of coverage for Block Quotes take a couple of months as it had for List Blocks? Would it be more? Would it be less? I just did not know. I was hoping it would be less, but that not knowing was just driving me nuts. But I also realized that the sense of dread would not disappear until I started doing something about it. Even by working on easy Block Quote items, I was getting a clearer picture of the effort it will take to cover Block Quotes properly. Instead of a sense of dread, I believe I am at a place where I am confident that it will be less than four months, and probably more than two weeks. Not sure where in that range it will land, but pretty confident it will be in there. And that is okay for now. The important thing is that I did not let that dread knock me down. I took it, channeled it, and got some more information that helped me deal with it. Cool! What is Next? Still feeling a small amount of dread, but mostly I am feeling optimism about the progress I am making! As such, I expect to be working with Block Quotes for a while, possibly dipping into dealing with Block Quotes and List Blocks every so often.","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/12/21/markdown-linter-delving-into-the-issues-13/","loc":"https://jackdewinter.github.io/2020/12/21/markdown-linter-delving-into-the-issues-13/"},{"title":"Autism And How It Affects My View of Christmas","text":"Many people in my life have been very understanding, but as always, there are a decent number of people that have left some lasting negative impacts. Some of those people I blame for their actions, and some of those people were just reacting to me and how my Autism appeared to them. But equally important to how I appeared to them was my Autism and how it has affected my view of Christmas throughout my life. When I say that I was diagnosed with Autism in my 40s, that does not mean that I did not have Autism before that. It just means instead of \"the weird guy with Autism\", I was just the \"the weird guy\". And back when I was a kid, I definitely was \"the weird kid\". Contrary to the modern definition of Autism, the definition of Autism when I was a kid was someone with below-average intellect who had severe problems interacting with the world. I did not have the benefit of any helpful labels and programs to help me succeed, such as the help that my son was able to get throughout his childhood. It was just me trying to figure things out as I went, often feeling alone and alien, wanting to fit in with my peers, but not knowing how to even start. That is where my negative view of Christmas started, based on a strong dislike of what I perceived to be the over-commercialization of Christmas. As I have been trying to understand myself better over the years, it has been hard for me to disentangle two aspects of this dislike from each other. The original reason that I disliked Christmas was indeed a selfish one: our family did not have enough growing up. My mom is one of the most important people in my life, and each year I saw her struggle throughout the year to make sure we had our basic necessities and whatever other reasonable things she could get for us. It pissed me off that my father was almost never in the picture. The thought of my mother having to ask our local church or food bank for help and for presents, either made me feel embarrassed and mad at the same time. With the kind heart that I know my mother has, I am pretty confident that she felt like she should have been a better mother, even though that was not in the cards. For me, even though my mom did everything for us out love for us, Christmas was a time when I always felt more negative emotions that positive ones. And there is where the second aspect kicks in: my peers. I remember the feeling of dread that preceded going back to school after Christmas break. Each year, I would go back to school and everyone in the class would be bragging about the wonderful things they got for Christmas. While I tried to pass off the things I got as equally wonderful, my lack of social skills meant that I was unsuccessful, while my classmates would seemingly and convincingly go on and on about the fantastic things that they got. They all seemed to be bragging about how wonderful their lives and families were, and it hurt. As that stuff was happening at school, there were also issues I had with how my extended family dealt with Christmas. I mean, at 11 years old, how could I explain to them that the music of Tchaikovsky appealed to me because of his use of harmony and melody? I just knew that I liked it and could get lost in that music for hours. So, when my mother asked what I wanted for Christmas from my extended family, I asked for a Tchaikovsky cassette tape. It was simple, it would not cost too much, and it was something that I would appreciate. Sounds logical, right? I ended up asking for that single cassette tape of Tchaikovsky for three years straight. I would never dream of asking my mother for it, because she was having a hard enough time making sure we were cared for. But as a request for Christmas from my extended family, I was sure it was an easy request to fulfil. Not so. The first year, one of my older cousins gave me a Bart Simpson poster and a dancing beer can, because \"they were sure that is what I meant to ask for\". The second year I did indeed get a cassette tape, but one with the works of Beethoven. When I asked about the cassette tape, I was asked \"aren't they the same?\". The third year, a year in which I remember one of my younger cousins being responsible for me in the gift exchange, I finally received a cassette tape of some of Tchaikovsky's symphonies. I did not find out who that cousin was, but I wish I did so I could have thanked them… I played that tape until it wore out. But those experiences left a bitter mark on my mind regarding Christmas. Finding out more information about the gifts, I was able to figure out that the dancing beer can was supposed to be a gag gift or a good joke. The Beethoven tape? It was bought two months before my mother gave them my Christmas list. While it may not be the right information to gleam from those events, my Autistic mind analyzed the facts that I had gathered and came to a couple of conclusions. The first conclusion was that my extended family did not care about each other. It was about a good joke with the whole family there, or something that was bought at a sale that they could pass off as good enough. It was not about love or understanding, it was about fulfilling a contract to present a gift. The second conclusion was that even though we were all good Catholics and went to mass in and around the holidays, most of that was all for show. One of the tenets that I learned growing up was: Do unto others as you would have them do unto you. Gospel of Luke, 6:31. When my aunts and uncles asked each other for something for Christmas, I believe they almost always received what they asked for. How would they have felt if it took one of their siblings three years to get it right? I do not think they would have been happy. Furthermore, how would they have felt if they asked for something they really did need, and instead got a joke gift and had everyone laughing at them. How would they feel then? To my recollection, none of my extended family asked me or my mother what was so important about Tchaikovsky that I would ask for it over a toy. I just know that it was not until I was in my 30s that one of my younger cousins asked. The reason? In a life where my Autistic mind was going the speed of thought, listening to those melodies and harmonies helped me find peace and calmness for a brief amount of time. I could lose myself in them and forget about how difficult that the world was for me to process. As I grew older, I was able to start figuring out how these things impacted my life and how it colored my view of Christmas. My loving wife has asked me for the last ten years why I do not share her love of Christmas, and it is only recently that I have been able to sort out parts of it. It has been difficult to try and pull on all these threads to try and figure it out but undertaking that task has helped me find some peace with the subject. From those old threads of my Christmas experiences, I was able to gather some things that I would like to share. As the years have passed, I have heard little bits of information here and there from my trips back home that lead me to believe that a lot of my classmates were not always as lucky as I was. It was with a lot of sadness that I found out what a lot of those classmates had to go through their own versions of hell in their own lives. For quite a few of them, those Christmas presents that they received were the only symbols of love that they received from their parents. For others, they felt a constant need to prove that they were better than others because their families never made them feel like they were good enough. I could go on with other things I have heard about classmates over the years, but the picture is pretty much the same. Every one of those people had different forces impacting them in their lives, and often, enough of them were negative and colored their perceptions. Using the analytical part of my Autistic brain, I came to understand that their bragging about Christmas gifts were mostly cries for help, not joyous refrains. While I was upset with them growing up, regardless of their station in life I now hope that each and every one of them finds peace in their own way. I came to the realization that I was, and continue to be, lucky that I have always had a kind and caring mother in my life. She has always been there for me and never left me feeling that having only one parent in your life was not enough. While it should be more common, it is not, and that is sad. To that extent, I believe that Christmas should not be based on a calendar, but on a state of mind. We should be helping people and making sure our children and each other are comfortable, loved, and respected. It should not be about making a special effort during Christmas to make time for each other and be patient with each other, it should be a normal occurrence in our daily lives. I am not sure if this makes me seem naïve or if it just the way that my Autistic mind works, but it is how I feel. I do not care if it is kind of mushy or idealistic. Can anyone really argue with me that showing each other a little more patience and respect would not be a positive thing in our chaotic world? Another observation is that people are quick to point to the physical gifts that they received at Christmas, but do not always pay attention to the other gifts throughout the year. Even though we did not have enough growing up, I have the utmost confidence that I would not have made it to where I am today without my mother's unconditional love. While she knew I was different from a young age, I never felt like I was a \"weird kid\" to her, just her oldest son, who needed love just as much as her other two kids. Finally, different people experience things differently, and we must make our own efforts to understand people on their turf, not ours. As someone with Autism, I should not be expected to offer to explain myself and my Autism to people. But if someone were to ask me a question about how I perceive things and how they can help, I would definitely be open to that. I would like to think that if my relatives had asked me that question about Tchaikovsky and I answered honestly that they would either respect myself or my mother and help me find that 45 minutes of peace located on a well-worn cassette tape. I know I would gladly offer that gift to someone. Even after finding peace with the subject, I am not sure that a \"love\" of Christmas is in the cards for me. I do know that in my own life, either inspired by my mother's love or her teaching of \"the Christmas spirit\" to me, Christmas is just another day to me as I simply try and apply those \"Christmastime\" principles to my life daily. To be honest, I have no clue on whether it is my Autism or my upbringing that has got me to this place in my life, I just know that it is the path that I chose. And I feel good about that choice. I guess you could say that it is my gift to those around me. My gift to myself? Letting go of some of those negative Christmas experiences each year. Allowing myself to forgive those people and hope that they find peace with their own person daemons. I am not a religious person by nature, but I do know that the many holy books, the Bible included, have some great verses. Here is one I try and live by: A new commandment I give to you, that you love one another: just as I have loved you, you also are to love one another. Gospel of John, 13:34","tags":"Autism","url":"https://jackdewinter.github.io/2020/12/20/autism-and-how-it-affects-my-view-of-christmas/","loc":"https://jackdewinter.github.io/2020/12/20/autism-and-how-it-affects-my-view-of-christmas/"},{"title":"Markdown Linter - Delving Into the Issues - 12","text":"Summary In my last article , I continued in my quest to reduce the size of the issues list. In this article, I worked hard to deal with the 33 scenario tests that I skipped in the last set of commits due to time constraints. Introduction Pure and simple, this week was all about resolving the 33 scenario tests that I marked as disabled from last week. I was mostly confident at the time that the right decision was to mark these tests as disabled and commit the changes I had already made. The question was whether I could fix this issues in five minutes, five hours, or five days. If the answer was anything except five minutes, then disabling the tests was the right thing to do. Otherwise, it would not be the wrong thing to do, I would just feel foolish that it could have been dealt with quickly instead of disabling the tests. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 03 Dec 2020 and 06 Dec 2020 . Cleaning Up from Last Week At the end of the work that I covered in the last article, I marked 33 scenario test functions as disabled. While I felt comfortable in disabling them for a bit, I did not feel comfortable leaving them disabled for too long. It was time to tackle them and get them resolved! Starting with Html Blocks I needed to start somewhere, and HTML Block elements just seemed like the best place to start. There was no real good reason to choose them other than, in my mind, they were a simple block type. The text in HTML Block is translated to HTML with no changes or interpretation, and the start and the stop sequences for HTML Block are very constrained, but also very simple. It just seemed like the simplest place to start for something that I was concerned about. My concerns? That solving the problems with all 33 failures were going to be a very difficult task to accomplish. It was during my usual task of research though debugging that I was able to make a quick breakthrough in diagnosing the problem. As a lucky observation, I noticed that when one of the Series M test functions was executed, the test's Markdown: 1. 1. <script> foo </script> resulted in an HTML Block element that started between the end of the sublist and the end of the list. When the end of that list occurred, it caused the HTML Block element to be closed, leaving the last two lines to be interpreted as a Paragraph containing the text foo and a Raw HTML element of </script> . By looking at the Markdown text above, I believed that both lists should have been closed. The HTML Block start element should have closed both Ordered List blocks as the HTML Block was not indented enough to keep either List block open. But how was the code interpreting that Markdown, that was the real question. Digging into the code a bit, adding extra debug as I went, I was able to soon figure out that the parser was trying to close the list, but the right type of close was not occurring. As I dug into that code, I noticed that I had found a similar problem with the Thematic Break element and list closures at some point in the past. To solve the problem with Thematic Breaks, I added Thematic Break specific code in the __check_for_list_closures function that properly closes the right amount of List elements if a Thematic Break element occurred within a List element. After going through this code and the data from the scenario test's log, I had a theory that I had another instance where I needed to follow this pattern. To test that theory, I quickly changed the code in the HtmlHelper class that checks for the type of HTML block to break out the \"is it a HTML start\" code from the \"what kind of HTML start is it\". By creating this new is_html_block function, it allowed me to change the __check_for_list_closures function in the ListBlockProcessor class to calculate whether the current line was indeed the start of a HTML Block, storing that value in the is_html_block variable. With that completed, I changed code that specified is_theme_break as part of two if conditions to is_theme_break or is_html_block and I was ready to go. Following my usual pattern of validation scenario tests, I was able to get the new and changed tokens validated quickly, producing the HTML output that I expected. With no other changes, I was able to get all four disabled scenario tests that dealt with HTML Blocks working again. With those four tests no longer being disabled, the total count of scenario tests to fix was down 4 scenario tests from 33 to 29. It was a good start, but I still had a long way to go. But with momentum on my side, I carried on! Maintaining the Momentum with Fenced Code Blocks As I had a pattern that worked for one type of block, I decided to try that same pattern out on the next type of blocks: Fenced Code Blocks. When I did that, I was pleasantly surprised that following the same pattern led to the same results, with one exception. In this case, the \"is this a Fenced Code Block start\" function already existed as LeafBlockProcessor.is_fenced_code_block , so it was easy to wire it into the __check_for_list_closures function after assigning it to the is_fenced_block variable. The one exception was that there was a small failure in the rendering of the Fenced Code Block tokens to HTML. In cases where an empty Fenced Code Block element was present, any extra whitespace in the Blank Line was being added to the HTML output after the newline character, instead of just adding the newline character. As this was the only failure, and (at least to me) it was obvious that this was the issue, I was able to fix it by adding the exclusion_condition variable and keying off of that variable. The testing for that change happened almost as quickly as the coding did, and I was able to resolve that group of four failing scenario tests. With another 4 tests that were no longer disabled, the disabled test count was now down to 25. Atx Headings for The Three-peat? Deciding to work on Atx Headings next, I applied the same pattern and got a very good result. Other than that changes required to isolate the \"is this an Atx Heading start\" code into its own function (and its inclusion into the __check_for_list_closures function), no other Atx Heading code required changes. After validating and modifying the tokens for the scenarios, all the tests just passed. It was so weird to have that happen, that I ran the scenario tests again just to be sure, and they were indeed passing. To be clear, it was not that I was doubting myself, I just expected a lot more difficulty in resolving these issues. But, with a little bit of work, another 3 tests were removed from the disabled list, bringing the disabled test count down to 22 tests. Cleaning Up The HTML: Indented Code Blocks After looking at the disabled tests dealing with Indented Code Block elements, it was almost immediately obvious the same pattern would not work with these tests. As far as I could tell from my initial look, the problem was not with the tokens, but in the determination of the looseness of the Lists. In each of the failures, the only difference was whether the items in the List blocks were surrounded by HTML's Paragraph tags or not. The problem definitely had something to do with looseness. The GFM Specification's definition of looseness states: A list is loose if any of its constituent list items are separated by blank lines, or if any of its constituent list items directly contain two block-level elements with a blank line between them. Otherwise a list is tight. (The difference in HTML output is that paragraphs in a loose list are wrapped in <p> tags, while paragraphs in a tight list are not.) And whether I liked it or not, the HTML output was just a little bit off. Not much, but enough. Specifically, it was cases in which the Blank Line token was appearing right after an end List Block token, a case that I felt should not be marked as loose. Adding a lot of debug and scanning through the log files, I decided to take an approach of removing the one set of cases that were producing bad results, rather than redesign the __calculate_list_looseness function from the ground up. As far as I could tell, everything else was working, just this one case was failing. Therefore, I created the new __correct_for_me function where I looked for Blank Line tokens, looked for the end List Block tokens directly before it. At that point, being within the proper list, the code was then able to make the right determination on the looseness of the list. Testing that theory took a while to code up, but the testing went by quickly. It was during that testing when I noticed that the whitespace in the Paragraph Block tokens was off by one. As the HTML output is checked before the Markdown rehydration is checked, the issues with the HTML output prevented this issue from being seen. However, due to the extensive logging I have in place, I was able to quickly deduce that I had mixed up the passing of the leading_space_length variable to the __adjust_line_for_list_in_process function with the before_ws_length variable. After a quick fix and some retesting, everything was now working, and the count of failing tests had fallen by another 2 tests down to 20 failing tests. The Long Slog: SetExt Headings Working through the previous scenario tests, I was confident looking at the two tests that deal with SetExt Heading elements that I would be able to deal with them quickly. If someone is right with their confidence on something like this, they are told they are right. If they are not right, they are usually told that they had too much hubris. That day, hubris was having a fun time laughing at me. As it was a Saturday, I was doing things around the house and getting them off our house's to-do list. In between each of these tasks, I would sit down and work on trying to figure out what the issue was with these tests and how to properly identify them. I was fairly convinced that I needed to be able to detect the start of a SetExt Heading element. I was failing miserably on trying to figure out how to perform that detection. I tried to be smart about the detection, and it failed. I tried to be simple about the detection, and that too failed. And with each attempt, I was just getting more frustrated. It was mainly as a joke that I decided to add or True to the end of one of the if statements. At the very most, I thought I might possibly find some information related to the \"detecting the start of a SetExt Heading\" issue that I was working on. I was flabbergasted when the scenario tests I was debugging just started working. In retrospect, it somewhat makes sense, though I want to dig into that function some more in the future to verify it. I believe that it worked because SetExt Heading elements and Paragraph elements are related. To get a SetExt Heading, you start with a Paragraph element, and you transform it into the SetExt Heading element once the SetExt Heading sequence is seen after that paragraph on its own line. As such, I didn't need to add any extra processing above that of the Paragraph processing to the first if statement in the __check_for_list_closures function, as the Paragraph element processing was already sufficient. In the case of the second if statement, all the other types of Leaf Block element had already been added, so adding or True just included the one or two remaining types of leaf blocks that had been missed. As far as I can tell, it was nothing more than a shortcut. Regardless of whether my guess is correct or not, the count was now down from 20 disabled scenario tests to 18 scenario tests… and these ones were going to take some time to figure out. The Last 18 I was then down to the last eighteen scenario tests that I needed to get working, and I knew some serious work was going to be involved. I had already been thinking about this for a while, even going as far to contact the CommonMark Discussion List to help understand some of the thorny issues. Understanding the Scope Of The Problem When all was said and done, these last scenario tests were failing due to my lack of understanding and implementation of the following section of the GFM specification: Exceptions: 1. When the first list item in a list interrupts a paragraph—that is, when it starts on a line that would otherwise count as paragraph continuation text—then (a) the lines Ls must not begin with a blank line, and (b) if the list item is ordered, the start number must be 1. It took me a bit to work through it, but here is how I think about it. Take the Markdown: 1 . abc 1 . The first line clearly starts a list, but it also opens a paragraph block with the text abc in it. Because of that open paragraph, the text in line 2 needs to be handled carefully, as the previously noted exceptions come into play. In this case, because of section (a) of the exceptions, this Markdown will be interpreted as: < ol > < li > abc 1. </ li > </ ol > If there is even 1 non-whitespace character on that second line after the list start identifier, say the Markdown: 1 . abc 1 . a then that section of the exceptions is no longer in play, allowing the second line to be a valid list start: < ol > < li > abc < ol > < li > a </ li > </ ol > </ li > </ ol > However, the harder one for me to understand was section (b) of the exceptions. The impact of that exception is that while the above example works as I would expect, the following Markdown did not: 1 . abc 2 . a producing the following HTML < ol > < li > abc 2. a </ li > </ ol > But why? Everything Is Not Perfect I could give many different examples of what is and is not a proper translation, but the example from the discussion forum that sticks in my head a lot is: 1 . The project risked missing its deadline of March 13 . A meeting was called . 2 . The architect would take flight 457 . It was the red eye . 3 . She would stay at the Hilton , room 22 . It had an ocean view . First off, this example is plainly an example based in the English language, and I assumed that equally valid examples could be constructed for any language. Given that assumption, when I read that Markdown for the first item in the list, it is obvious to me that the end of the first item in the list refers to March 13. , not a sublist starting at 13 with the contents of A meeting was called. . The context of the rest of the sentence leaves me with little doubt that the sentence was written that way. And each of the other two list items left me with the same confidence. Those numbers were part of the sentences, and thus the paragraphs that the sentences were in, not the starts of new lists. But how should the specification handle the codification of this? The concepts of \"sentence context\" and \"looks right\" do not apply to the specification. For something to apply to the specification, there needs to be a solid rule that can be followed without fail. In this case, the second exception comes into play: (b) if the list item is ordered, the start number must be 1. While it is not perfect, this exception allows the specification to handle the above cases in a way that it has a solid rule to follow, and hence predictable results. No guesswork or \"sentence context\" involved. I believe that @jgm from the discussion board put it best: If we were inventing a light markup language from scratch, I'd want to require a blank line before a list or sublist (see Beyond Markdown), for this and many other reasons. But we're not, so we need to find a compromise that works. And that indeed is a viable solution for starting sublists with any number as the list's start number. Therefore, if you the previous example with a non-one sublist start of 2. to be rendered properly, you need to add a newline: 1 . abc 2 . a producing the following HTML: < ol > < li > < p > abc </ p > < ol start = \"2\" > < li > a </ li > </ ol > </ li > </ ol > As @jgm said, and as I agree with, it is a solid compromise. Attacking the Problem The first thing that I did was to validate that this issue applied to both Ordered Lists and Unordered Lists, which was quickly accomplished. To me, this indicated that I was going to be making near identical changes to the is_olist_start function and the is_ulist_start function. As the Order List changes were the most complicated, I decided to start with those. The first part of detecting the condition described above was deciding that the current line being parsed was already marked for a list start. This was already being performed in the function, so it was an easy change to do some extra processing if the is_start variable was set. In that case, to narrow down the things that need to be checked, the first two changes were to set the is_in_paragraph variable to indicate whether a paragraph block was open, and the at_end_of_line variable to indicate that there was no more data to process on the line (hence, a blank line). With those easy changes out of the way, the variable is_first_item_in_list needed to be set to indicate whether or not the proposed Ordered List start sequence actually indicated a new List item or a brand-new List block. While lengthy in its description, the next part of the algorithm checked, in order, to see if a parent List block contained the Paragraph element, if it was the same type of List element, if it had the same List character, and if the start index for the proposed List element was greater than that of a matching List element already in progress. If any one of those checks failed, the proposed List start was stored as True in the is_first_item_in_list variable. From there, the check was relatively easy. After an additional change to set the is_not_one variable to indicate whether the olist_index_number variable was not the string 1 , the calculation was easy: if ( is_in_paragraph and ( at_end_of_line or is_not_one ) and is_first_item_in_list ): is_start = False Following the exceptions detailed earlier, when a new list start occurs within an ongoing paragraph ( is_first_item_in_list and is_in_paragraph ), a further check is done to to see if the List element would begin with a blank line ( at_end_of_line ) or is an Order List start sequence where the start number is not 1 ( is_not_one ). How Well Did This Work? The changes documented in the last section were the bulk of the work, and after that the remaining changes were easy to figure out and work on. With the line: if is_theme_break or is_html_block or is_fenced_block or is_atx_heading or True : resolving to True in all cases, I removed that line to make things clearer. While it makes a mess of any displayed differences, it really is only removing that line and shifting all text text that was under that if statement to the left by four spaces. After running through some tests, there were some failures with the translation to HTML. Those failures were all fixed with two lines of code: elif output_html and output_html [ - 1 ] != ParserHelper . newline_character : output_html += ParserHelper . newline_character With that completed, all the scenarios were running except scenario test function test_list_items_282 . After working on that for a while, I marked that scenario test as disabled, to research it and work on it later. Along the way, I also added two variations of test function test_list_blocks_263 to test specific cases that I thought would be a problem, both working without any problems. What Was My Experience So Far? The first thing that came to mind is that I did have an answer to my question from the Introduction section. It took me five days to resolve those disabled tests. More than anything else, that really cemented my feeling that I made the right decision in committing that block of work I had from the previous week, with tests disabled. While I was still a bit tired after my sinus cold from the previous two weeks, it felt good to get some real solid debugging work done and out of the way. The contrast between this one week's work and the previous two week's work was just staggering. It was a real good feeling to get back up to a speed that I know I can easily achieve. And given that non-cold increase in momentum, along with the recent reduction of items in the issues list, it was nice to see that the finish line is getting visibly closer. While I don't want to jinx myself by setting a date, and then missing it, I am guessing that I am going to be ready sometime in early 2021, and that is good with me. Believe it or not, I am very much looking forward to it! What is Next? With some of the hard issues out of the way, I wanted to finish up all the List Block issues and get started on the Block Quotes issues. Here is hoping that I would be able to do that!","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/12/14/markdown-linter-delving-into-the-issues-12/","loc":"https://jackdewinter.github.io/2020/12/14/markdown-linter-delving-into-the-issues-12/"},{"title":"Markdown Linter - Delving Into the Issues - 11","text":"Summary In my last article , I continued in my quest to reduce the size of the issues list. In this article, I split my time between adding to the scenario cases tables and dealing with items from the issues list. Introduction I jokingly referred to this week as the week from hell. It was hellish in nature because of a bad cold that got into my sinuses and would not leave, no matter what I tried to get rid of it. As such, I felt that I got approximately half the work done that I had wanted to. Therefore, I felt that it was appropriate to talk about the work done during the two-week period that I had the sinus cold instead of just my usual one week period. Even though my brain was fogging with the sinus cold for a good solid two weeks, I was able to get some good work done, even if it was not at the pace that I am used to having. The big focus at this point in the project was on reducing the number of items on the issues list that dealt with List elements. Having taken a significant amount of time working on the leaf block elements and getting those items resolved, I was hoping to get a good chunk of the list issues dealt with. But I also knew that the impending American Thanksgiving holiday and a nasty sinus cold were going to slow me down. It was just a matter of being honest with myself about what I could accomplish during this period. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 17 Nov 2020 and 29 Nov 2020 . Indented Code Blocks and List Blocks Up to that point, I had a good variety of cases for each leaf block type in Markdown save for one: Indented Code Blocks. Hence, I had logged this issue: - 256 i tests , not computing indent properly if empty list and indented I gave a good solid first try at getting them working, but in my mind, Indented Code Blocks were different enough that they posed additional difficulty. As such, I left them for last. The example that gave me the trouble was an additional test that I added, function test_list_blocks_256i or function test_list_blocks_256ix as I renamed it. The Markdown for the example was simple: 1 . foo With the list start indented to the maximum with three leading spaces, the indentation of the text foo should have been enough to make it eligible for an Indented Code Block with four leading spaces. Instead, it was just getting captured as part of the text for the List Item element. Working Through It Granted, my cold was raging in my head, and I was not thinking clearly, but eventually I looked at the tokens long enough and something stuck out at me. The start List Block token for line 1 was [olist(1,3):.:1:3: : ] which did not look weird to me at first. Looking at the end of the token, where I usually look, everything was fine. There were three leading spaces before the start List element, and there were three spaces registered for that token. Exactly as it should be! And it was an start Ordered List token with a single digit start number, so the indent_level for that list should be 3: 1 for the single digit, 1 for the list character . , and one for the whitespace after it. Check! Then it dawned on me. While the three leading spaces were appearing in the token itself, they were not being accounted for in the indent_level . As such, when the parser got to the second line, the indent_level was set to 3 , and it looked like the that line was only indented by one character, not enough to start an Indented Code Block. After making some changes to pass through the extracted_whitespace variable from the first line, the indent_level was adjusted by the length of the extracted_whitespace variable, resulting in an indent_level of 6. As the four leading spaces on the second line was less than that value, it was properly interpreted as an Indented Code Block element. After adding some additional variations to test and make sure that the change was the right change, I was happy to resolve this issue, and get some rest. The Birth of Series M Having documented this process before for the other series, I will not take the time to go through all the steps performed to move these scenario tests over. I will point out that for me, with a sinus cold that was not letting up, it was the perfect thing for me to work on. It was a lot of moving tests over two days, but it was slow, and it was methodical. More importantly, it had built in error checking. A good thing to have when you are not 100% sure of how clearly you are thinking. As this series was genuinely moving scenario tests over from their origin module test_markdown_list_blocks.py , I did not expect any issues and there were none. Due to some clarity in thinking when setting up this work, any errors that I did make during that process were caught and recovered from right away. Other than that, the entire process was a blur. \"Weird\" List Contents Mostly due to the sinus cold, which was finally starting to ease up, it took me another couple of days to get the next issue resolved. Mentally, I realized that I could either push myself hard and perhaps prolong the cold, or I could take more breaks and have that energy go towards resolving the cold. Whether it was the positive thinking or the natural course of the cold, I will never be sure which one helped more. But by noon on Saturday, I was starting to feel better, and I started to tackle these issues: - code span inside of a list - multi - line link inside of a list The first issue was easy. I started with something simple, adding the function test_list_blocks_extra_2a to test split paragraphs with the following Markdown: 1 . abc def 1 . ghi jkl 1 . three From there, I made a small modification to test for Code Spans by using the following Markdown: 1 . `one` 1 . `` one - A `` 1 . `two` 1 . `` two - A `` With the Code Spans dealt with, I moved on to links, using Inline Link elements and splitting them between two lines are various points in the link itself. While not that interesting, it was a good solid scenario that I wanted to make sure was working: 1. [ test ]( / me \"out\" ) 1. [ really test ]( / me \"out\" ) 1. three Tracking Down the Issues After coding those new tests, I started executing the tests and everything within the changing parts of the lists looked fine. However, on the third line of each example, when the next item of the base list was defined, some of the tests emitted their text surrounded by a Paragraph tag. As this relates to whether a List is considered loose , I took some time to poke around and debug through it. Looking at the debug, I realized that I had some issues with the function __reset_list_looseness in the transform_to_gfm.py module. In trying to be smart about locating the end of the relevant tokens belonging to a given list, I was going forward from the start List token looking for the matching end List token. The problem was that I was not being selective about which end List token I found, just that I found a token. A short while later, I had some changes coded up that kept track of the stack_count associated with the start List tokens and end List tokens that were seen. The start List tokens bumped the count by one and the end List tokens reduced the count by one. If the stack_count variable was ever zero, it meant that the algorithm had found the matching end List token, and it broke out of the loop. After I finished executing the tests and verifying the results, it was clear to me that I had found and remedied the issue. While it was not a big issue to fix, it was a sneaky one to find, and I was happy to resolve it. Sometimes It Is Not Obvious Feeling good from my success and solving the last issue, and with the sinus cold allowing, I started to work on another issue: - 242 with variations on where the blank lines are While I remembered adding this item to the issues list, I could not remember anything around the reason that made me add this to the list. As I was not aware of the reasoning behind the inclusion of this item into the list, and I could not figure out from the item, I decided to make copies of function test_list_blocks_242 and experiment with the positioning and number of blank lines within the document. What I found was interesting. It was a time where I was very happy that I had taken the time to add consistency checks, as they caught this problem right away, where the output HTML comparison tests did not. The problem? In cases where the __remove_top_element_from_stack function in the blank line handling of the tokenized_markdown.py module were removing blank lines to be added to the document, it was doing so in reverse order. That reverse order meant that in cases with multiple blank lines, the latest blank line would be added first, messing up the ordering in the document. Once again, a quick fix, and with a couple of iterations of testing to make sure other functions were not impacted by that side effect (and mitigating those), things were taken care of. Another issue solved and resolved. Variation on Example 297 I had some energy left from fighting my cold, and some time left before the American Thanksgiving holiday started, so I figured I could work on something light. Hopefully picking something easy, I picked this task off the list: - 296 and 297 - added in case for LRD , but need to test : - other types of blocks - block , blank , then multiple blocks After a quick look at the Markdown for example 297 : - a - b [ ref ] : / url - d I had a good feeling that I would be able to deal with this issue in a couple of hours or less. To deal with this issue properly, I quickly created variations on function test_list_items_297 to test those different scenarios. Instead of a Link Reference Definition in each variation, I used an Atx Heading element, a SetExt Heading element, a HTML Block element, an Indented Code Block element, and a Fenced Code Block element. Just for good measure, I added an extra scenario test that had a Fenced Code Block element followed by a HTML Block element. After adding those scenario test and executing them, I was greeted by the good news which was that the tokens and the output HTML matched what was expected of each test. The only issue was in the Markdown generator where the original Markdown was being reconstructed from the tokens. After a quick bit of debugging was done around the processing of the Html Block token, a small change was needed in the function __merge_with_container_data to allow the remove_trailing_newline variable to be set if the block ends with a newline character. With those small changes in place, the newly added scenarios worked fine, generating the correct Markdown to match the original Markdown. Fun with List Elements I do not have any notes on why I picked this task: - test_link_reference_definitions_185f & test_link_reference_definitions_183f but it was a fairly interesting task to pick. Previously, I had disabled both tests as I was not able to get them working previously. And it was not much, but I somewhat remembered working on both these items for at least a couple of hours each, and not making much progress. As I said, this was going to be interesting. The good news was that, after a small amount of debugging, I was convinced that I was looking at two separate issues. While I did not have any concrete information, I had a strong feeling that the test_link_reference_definitions_183f function failures were due to the Block Quote element in the Markdown, while the test_link_reference_definitions_185f function was simply an issue of getting the Markdown generator adjusted properly. Debugging the Issues Picking what I thought was the easy issue to solve, I decided to start working on the problem with the handling of the Block Quote element. This happened to be a good choice as some simple debugging showed me that the issue was a simple one of not closing off an active List before starting off the Block Quote element. I quickly fixed that by adding a simple loop in the __ensure_stack_at_level function of the BlockQuoteProcessor class to ensure that occurs before the Block Quote itself is started. With that part of the issue fixed, my focus shifted to dealing with ensuring that the Markdown was being properly generated. After a couple of hours of debugging, I finally figured out that the failures were being caused when the already transformed data ends with a newline character, and the next token to transform is either a normal text token, or one of the SpecialTextToken related tokens: Links, Images, and Emphasis tokens. In each case, these tokens somehow interrupted the accumulated text, leaving it ending with a newline character. To properly add any more text to that accumulated text, the new data to be added needs to be processed differently to accommodate that break. Like one of the previous sections, the first issue was relatively quick to fix, while the second issue took hours. Working through the debugging with a sinus cold was a bit of a slog, but it was a good issue to take off the list. Bulking Up the Series M Tests It was Saturday afternoon and I had finished doing some work around my house. While I was a bit fatigued, I felt that the sinus cold was letting up enough that I could spend some weekend time making some progress on getting more depth to the Series M scenarios. To do that, I basically started by placing each of the tests in Series M of the scenario tests into their own tables. Having over 45 tests at that point, that separation was equal parts necessity for my sanity to keep each table separate and readability for anyone looking at them. Adding 60 scenario tests to the series, I added 10 scenario tests in each of the six groups within the series. While there were small variations to each group of tests, the underlying tests were essentially the same 10 tests added each time. And just as I have mentioned before, the process was a long one: adding the rough form of the specific test to the table, adding a scenario test to match that rough form, and then dialing in the example, the token list, and cleaning up the final form of the entry in the table. And as usually, it was a long, grueling process. Powering Through the Scenarios The bad news was that I did not get everything done. After working hard to get all tests passing, there were 35 tests that for one reason or another were not passing. Between the scope of the changes and the last vestiges of my sinus cold, I did not think twice of marking those failed tests with @pytest.mark.skip , to be handled in the following week. I had a feeling that this task was more than I could handle in the time allotted with the energy I had, and I was right. Regardless, I had 25 new scenario tests passing where I did not have them before. The good news was that in those 25 new scenario tests, I only found two issues that I needed to fix and was able to fix. The most obvious one was in the case of two empty start List elements, nested together on the same line. Following through the code and the log files for that scenario test, it was immediately obvious to me that assigning the first element of the result from the function parse_line_for_container_blocks to _ (in essence, throwing it away), was the wrong thing to do. Assigning that first element to the produced_inner_tokens variable and adding the line: parser_state . token_document . extend ( produced_inner_tokens ) Fixed that problem. One down, one to go. Digging Deep into The Issue The other issue that I found was in dealing with empty list starts in the __pre_list function. In one of the first iterations of this function, I added code in the True evaluation of if after_marker_ws_index == len(line_to_parse): to handle those empty list items. After a lot of work to come up with the correct formula, I had settled on the code in that function, part of it for empty list items, and the other part of it for the non-empty list items. And that worked well. That is until I started looking at it considering the new examples added during these tasks. Looking at why scenario tests with empty list items were failing, I kept on looking at this __pre_list function. And with each debugging session that I came back to that function, the surer I was that I missed something pivotal. And that feeling was getting stronger each time. Given that feeling, I spent a couple of hours taking that if statement apart and putting it back together. Ultimately, I left the True case of the if statement as it was, but I changed the condition to after_marker_ws_index == len(line_to_parse) and ws_after_marker . As for the cases where ws_after_marker is 0 , I added the following code to the False case to handle that: if after_marker_ws_index == len ( line_to_parse ) and ws_after_marker == 0 : ws_after_marker += 1 After my experimentation, it just seemed like the right thing to do. I did find other solutions that were way more complicated than this one, but those solutions were a lot more convoluted. This one was simple. Instead of doing a complicated calculation and having lots of if statements, this just added a slight adjustment to the variable ws_after_marker , after which the rest of the False part of the if statement was executed without change. While the first solution with the tokens took less than a half an hour to code and test, when all was said and done, more than five hours had been spent on the task. But even though it took a while, I was pleased with the result, and I am confident that the time was well spent in upgrading those solutions. What Was My Experience So Far? In the beginning of this project, having those 35 scenario tests marked as skipped would have hung heavily over me. But at this stage of the project, I recognized that it was a necessary tool at my disposal. Instead of waiting until all 60 new scenario tests were working 100%, it was better to chip away at those tests, committing those changes to the repository as I went. Having worked on this project for almost a year at this point, I knew there were going to be things that ended up running away from me. I also knew that while I try and break bigger issues into smaller issues, there are times that is not possible, for one reason or another. In this case, I was concerned that if I did not add all 60 scenarios at once, I would miss one and it would be hard to detect. It just meant I would have to adjust. And for me, both in my professional life and with this project, is the big takeaway that I have learned in the last couple of years. It is extremely important to set expectations at a healthy level that can be sustained. Too little, and you can be viewed as taking it easy. Too much, and you may be expected to sustain that level of output for months or years. I have found great success in clearly stating my goals and how I plan to achieve them, and resetting expectations on a weekly or daily basis. It just makes sense to me. Well, it does now. That was not always the case. From my point of view, I could not see a clear way to break up that big issue without sacrificing the quality in the Series M group changes. So, I reset my own expectations for resolving that issue, promising myself that I would address each of those skipped tests in the next week. And I was at peace with my decision to do that. What is Next? Leaving 35 scenario tests marked as skipped because I could not figure them out did not sit well with me, so I made them the priority for the following week.","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/12/07/markdown-linter-delving-into-the-issues-11/","loc":"https://jackdewinter.github.io/2020/12/07/markdown-linter-delving-into-the-issues-11/"},{"title":"Markdown Linter - Delving Into the Issues - 10","text":"Summary In my last article , I continued in my quest to reduce the size of the issues list. In this article, I split my time between adding to the scenario cases tables and dealing with items from the issues list. Introduction With a specific focus on getting list issues resolved this week, I was hoping to make some decent headway with the issues list. From my reading of the list at the start of the week, there were a fair number of List element related issues, Block Quote element related issues, and cross-over issues between the two. It just made sense to me to pick one and focus on it. And I picked List elements related issues. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 11 Nov 2020 and 15 Nov 2020 . Starting Off with An Easy Task While not a big task, the task: - 292 x with ordered lists ? was a nice easy one to start with. This task was simply to take the functions test_list_items_292a to test_list_items_292c , copy them to test_list_items_292d to test_list_items_292e , changing them from an Unordered List elements to Ordered List elements. Nothing more, nothing less. And this was very simple, and similarly, the tests passed without any issues. A simple, but a good, solid start. Double Checking For these tasks: - 269 and 305 , and variations - whitespace is not 100 % correct - weird cases in list_in_process nothing was changed, but some research was done. To start, I went to those two scenario tests and their variations and looked at the whitespace in the tests. While it took me a second to remember why the whitespace looks that way in the list tokens, there was nothing wrong with the way it was being stored there. However, it did take me a second or two to remember that a paragraph within a list stores part of its leading whitespace in the List token and the remaining leading whitespace in the paragraph token. Maybe that is what caused me to add the item to the issues list? I then looked at the code for the list_in_process function with a similar exploratory effort, but nothing seemed out of place. I even enabled debug for a few of the test_list_items_292 functions, tracing through the code to look for any issues. While I did not find anything wrong, I was happy to take a second look at these areas to put any perceived issues that I might have had to rest. Ordered List Blocks and Start Numbers Once again, I found a small task to get out of the way: - CommonMark and how handles non - initial cases for list starts To examine this issue, I created two new functions, test_list_blocks_extra_1 and test_list_blocks_extra_2 . In the first of these functions, I added a couple of lists with sub-lists, all starting with the the integer 1 . In the second of these functions, I changed one of the sub-lists to start with the integer 2 instead. Checking the PyMarkdown parser HTML output against the BabelMark, everything was fine for the first function, but there was a slight difference for the second function. Instead of acknowledging the 2. signifying the start of a sub-list, that 2. text was combined with the paragraph element from the previous line. That was curious. After combing through the specification for about an hour, I posted a question to the CommonMark Discussion Boards , and waited for a response. More on that in a later article. Variations on Existing Lists Graduating from the simpler tasks, I decided to tackle a task that had a bit more substance to it: - 276 with extra level of depth , with olist / olist , ulist / olist , and olist / ulist Starting with the scenario test for example 276 : - - foo I started generating variations based along different themes. For each of the main variations, I simply ran through a simple list of combinations of the Ordered List element and the Unordered List element: Unordered/Ordered, Ordered/Unordered, Ordered/Ordered, and Unordered/Unordered. After those combinations were taken care of, the a and b variations of those variations were created by adding an Unordered List element (for a ) or an Ordered List element (for b ). While I expected something to happen, it was nice to be proven wrong. The tests all passed without any issues. The reason for my initial doubt on this issue? I have had problems with \"empty\" lists before. While I do not use them myself, the Markdown specification allows for a List element that only contains a start List Element text sequence, say 1. , with no other text on that line. And from my knowledge of the GFM Specification, empty list items are covered, but not as completely as the list item starts followed by text. Based on that background, it was good to see that so far, those empty list items were not going to be an issue. Variations on A Theme On a bit of a roll with some easy wins in the completed task column, I decided to do a couple of tasks together as a single task: - 256 with extra spaces on blanks - 256 with other list types for last instead of just li Based on previous tasks, it seemed like a good idea to come up with variations to deal with these tasks. To do this, I started with the example for example 256 : - foo - ``` bar ``` - baz For the first two variations on test_list_blocks_256 , I modified the example Markdown to include extra trailing whitespace as part of the empty list items. From there, I added a variation which replaced the unordered list item elements with ordered list item elements. In addition, I further modified that variation by adding extra blank lines and by reducing the indent on the Fenced Code Block element from 3 to 2, making it ineligible for inclusion into the list. Basically, I looked at Example 256 and experimented with what I thought would be good variations to test. While a lot of those variations did not result in the discovery of any expected issues, there was one interesting new issue. In cases where there is a blank line inside of a list, there was a weird ordering where the processing of the blank line and the processing of the blank line to close the list were in the wrong order. As such, a small fix was required to make sure that the information is available to make a proper decision on how to handle that blank line with the correct data. That one took a bit of effort to figure out, but it was a good warm up for what I knew was going to be a bear of a task to follow. This Week's Big Issue While there was not an explicit entry in the issues list for this issue, it was an issue that I had long been concerned about. As the commit message stated, that issue was to \"add better support for lists that are aborted due to missing leading space in front of a new block\". I knew I was opening a can of worms by exploring this issue, but for me to have confidence in the project, I felt that I needed to explore this, and explore it now. Aborted Due to Missing Leading Space? Basically, in its simplest form, this issue breaks down into the following scenario: 1 . --- In this scenario, the Markdown specifies a simple List element that is created without any content on that line, an empty list item. In starting to parse the second line, the parser must first determine if that newly parsed text will be part of that list or not. To ensure that text is added to that List element, the text on the next line must be indented 3 spaces, matching the indent level of that List element. Therefore, when the Thematic Break element on that next line fails to maintain that level of indentation, the original list is then aborted, and the Thematic Break element is processed after the list has been closed. This reading of the specification is backed by the HTML output generated by BabelMark for this scenario: < ol > < li ></ li > </ ol > < hr /> What Is the Issue Then? While I had a specific answer to a specific question regarding how the List element and the Thematic Break element interacted, I wanted a more generic answer that I could work with. As usual, I opened up the GFM Specification in my browser and started looking for that generic answer. Unfortunately, I did not get an answer that I was satisfied with. The first thing that I looked for was some variation on the previous example, of which I kind of found a related test with example 27 : - foo *** - bar While it was not exactly what I was looking for, it was something. When I went to look for a similar example including an Atx Heading element, I did not find any comparable example. To be clear, I do not believe this is the fault of the GFM Specification. To be honest, I think the specification has done a great job and specifying the main cases that people are going to encounter. But there is always room for improvement, and I am hoping to contribute to that improvement with the PyMarkdown project's testing suite. That is part of the process, and how the specification gets better. With that newfound information in mind, I was left with a slightly modified issue. As I did not have a good set of examples detailing how lists and other leaf blocks interacted, I therefore did not have a good comprehensive scenario test suite that I had confidence in. I was confident that the GFM Specification was getting me a good 90% of the way there, but I wanted more than that 90%. Therefore, the newly modified issue that I needed to solve was that I needed to specifically add more specific tests in this area. Getting A Good View on The Issue The first thing I did to address this issue was to stop and think clearly about what needed to be done. From the last bit of work, I knew that the scenario functions test_list_blocks_256* were a good start, so I decided to add functions with similar names more from that point. Starting with the example I outlined the function test_list_blocks_256f with the following markdown: 1 . --- Then I added a variation on that, making test function test_list_blocks_256fa be the same thing, just with text after the List Item element. Once that was done, I basically copied those scenario tests, replacing the Thematic Break element with an Atx Heading element, a SetExt Heading element, an Ordered Code Block element, a Fenced Code Block element, and an HTML Block element. With scenario test titles going from 256f to 256k , I went back and used BabelMark to replace the output HTML in each of the scenario tests. Running the newly created scenario tests, I discovered a solid number of issues that needed to be looked at. I knew at this point that there was not going to be an easy solution here. I rolled up my sleeves and got to work. HTML blocks Picking one of the elements to start with, that night I decided to start working with the HTML Block element and regretted it within hours. Looking at the failures from those tests, it was obvious that there was more than one problem with these tests. The most immediate problem was that the tokens produced by the parser just looked wrong. Taking the time to look at the problem in depth, I quickly discovered that the HTML Block element was not causing the list to close like it should. As that was a major issue to find, everything else after that token was affected in some way. To address this issue, I created the correct_for_leaf_block_start_in_list function to allow the parser to clean up in situations like this. More specifically, it was for cases where a paragraph had just been closed while in an active list. In those cases, the parser thought the lists had been closed, but the proper tokens were not being emitted. But even with those observations in place, there still was something about the tokens that looked \"off\". However, I know that there was currently too much noise in the way for me to see that other issue clearly, so I just decided to get it out of the way first. Cleaning Up HTML Blocks in Lists To take care of those concerns, I experimented with eight other variations on that list scenario, including some variations with sub-lists. While I was mostly pleased with the results, there were three tests that were failing due to issues in generating the correct HTML and rehydrated Markdown text. After some quick investigation, it became obvious that there was whitespace missing at the start of the HTML blocks. Double checking with the other failing tests, the pattern that emerged was that the spacing between the end of the previous paragraph and the new HTML block needed to be altered a bit. To make sure that information could get from the parser to the HTML generator, I added a new member variable fill_count to the HTML Block token, using the add_fill function to adjust its value. Once that was added, I was then able to make small alterations to the correct_for_leaf_block_start_in_list function to adjust that fill_count member variable with the difference between the number of whitespace characters removed and the indentation level of the currently active list. With that information now present in the token, the HTML generator was easily changed to add those extra characters between the start of the HTML Block and the processing of the text within that block. With those changes generating HTML properly, the focus shifted to applying similar changes to the the Markdown generator and the consistency checks. While the first part of this issue had taken days to complete and fix, this part took only half an hour. At that point, the HTML blocks and all their variations were working properly. Moving on To The Other Blocks After fixing the HTML Block elements, I took a closer look at the other failing tests and started to notice similar problems. While the fill_count solution was only useful for the HTML blocks, the correct_for_leaf_block_start_in_list function was useful in fixing issues with the handling of Fenced Code Block elements and Atx Heading elements. In both of those instances, the new element was supposed to abort the previous list but was not doing so. The correct_for_leaf_block_start_in_list function needed a couple of tweaks to make sure it was handling things consistently across all three scenarios, but they were all easy fixes. With the parser generating the tokens properly, the tests were making more progress but still failed on the output HTML. While not a big difference, the output HTML was missing a newline between the end of the previous list and new Fenced Code Block elements, Atx Heading elements, and SetExt Heading elements. That difference was quickly eliminated by adding two small lines of code to the handle of those elements: if output_html . endswith ( \"</ol>\" ) or output_html . endswith ( \"</ul>\" ): output_html += \" \\n \" After crossing my fingers for good luck, and fixing a couple of typing mistakes, the new tests passed without much fanfare. It was just a relief. I knew it would take a while, but I did not think it would take three days to complete the implementation and verification of these new scenario tests. It Was A Slog… Over that three days, I was able to get 18 new scenario tests coded and passing properly. To be honest, I was doubtful at certain points that I would get done, as progress was hard to come by. While I cannot remember how \"on my game\" I felt during that time, I do remember that I felt burdened by the knowledge that this work was only the starting point, and I would have to repeat it multiple times in the future. And to that end, I had my next task already lined up. Lather, Rinse, Repeat While it was a tough couple of days getting through that block of work, I felt that it was a good task to complete, but that it was not yet 100% complete. With some extra time left in the evening, I decided to take a shot at replicating the width of tests that I added for HTML Block elements to the other Leaf Block elements as well. Taking a bit of time to setup (copy, paste, alter, repeat… many times), I was pleasantly surprised with the low number of failures. For those tests that did fail, patterns that I was familiar with from the previous issues with HTML Block elements began to resurface. As these issues arose in the Atx Heading elements and the Fenced Code Block elements, the first set of changes I made were to add a fill_count member variable to those token class, similar to how I had added them to the Html Block token class. As some of those classes were in a bit more of a \"raw\" state than the HTML Block class was, I needed to do a bit of extra work to make sure that I could adjust the fill_count variable and have it be persisted. But it was nothing I had not done multiple times before, so it was quickly accommodated. Other than a couple of small changes, the only big change was to the function __check_for_list_closures . Taking a while to get right, I needed to alter a few of the functions that feed that function to pass the right parameters around. While it was not too difficult, I was hoping that I would find time in the near future to revisit parts of this code and refactor it to make it cleaner. What Was My Experience So Far? Getting a fair number of items related to lists taken off the issues list was a good feeling. While there were a couple of draining issues in the middle, it was still good to make that progress forward. While it was a bit disheartening finding issues in the parser after a spell without any major issues with the parser, it was par for the course. Better for me to discover them now, than to have a consumer discover them later. But as long as I was finding these issues, I would have to make sure to examine the existing tests and identify any potential areas where I can add extra tests to validate that I had found most of the issues with lists. At this point, I thought it would be useful for me to reiterate a point from previous articles. I am not trying to make this project perfect. After years of software development and years of software automation, I know that eliminating all issues is not possible. It always boils down to when the next issue is going to happen, not if. But for me, it is very important to make my best effort in getting the quality to a point that I feel comfortable with. Following that, while I know there is a bit left to go before releasing this project, I know that it is getting nearer with each item I resolve from the issues lists. And that is the point. It is getting closer, and I just need to keep my focus on the prize: that project release. What is Next? Continuing with my efforts to get better scenario tests around lists, I knew that I had to be more structured about that testing. As that meant creating a new test series, it forbade that the next week's work would be moving this week's work into a new series and cleaning it up.","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/11/30/markdown-linter-delving-into-the-issues-10/","loc":"https://jackdewinter.github.io/2020/11/30/markdown-linter-delving-into-the-issues-10/"},{"title":"Markdown Linter - Delving Into the Issues - 9","text":"Summary In my last article , I continued in my quest to reduce the size of the issues list. In this article, I split my time between adding to the scenario cases tables and dealing with items from the issues list. Introduction Not much of an introduction here, just my usual plodding forward. Having spent time in the last couple of weeks working on either the scenario cases tables or resolving items from the issues list, I tried this week to split my time evenly between those two tasks. Without further ado, on to the work! What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 03 Nov 2020 and 08 Nov 2020 . Dismissing an Easy Issue Initially, looking at the following item: - 553 with other in - lines ? I thought I would have some work to do. However, when I started looking at this item, it did not take longer than a couple of minutes before I was able to resolve this issue. Along the way, there are times where I have good ideas on things to check, and then other times where I just have ideas. While I think I meant well with this item, it ended up falling into neither of those two buckets. Taking a look at the Markdown for example 553 : [bar][foo\\\\!] [foo!]: /url I believe I wanted to make sure that I tested other concepts to make sure the lookup worked properly. The most obvious of those concepts would usually be inline elements, so I think it might have made sense from that point of view. However, I had missed one little thing. As function test_reference_links_553 centers around subtle variations with the link reference, any inline element would be treated as plain text, without any interpretation. Based on that quick research and the fact that I already had tests for inline elements in the link label, I just resolved it without any changes. I think while I might have had something else on my mind when I added that issue to the list, I was unsure of a good way to honor it in any reasonable form. It was good to check out though, just nothing to do to enhance the project with. Empty Link Labels Added a long time ago, I spotted two issues that I knew that I could resolve quickly: - link ref def with empty link label , like 560 ? - full reference link with empty link label , like 560 ? Obviously added at a time when I was not as complete in my knowledge of the GFM Specification as I am now, both items were indicating confusion as to why an empty link label wasn't valid. With the experience gained since those items were added, it was easy for me to reference the GFM Specification on Link Reference Definitions , select the link label reference in the first line of the first paragraph, and extract the following bit of text from the definition of a link label: Between these brackets there must be at least one non-whitespace character. While that one line escaped me early in the development of the project, I was now familiar enough with it to be able to locate it in 30 seconds or less. As I acknowledge it is a boundary case, I can see why the specification writers added that text in there to deal with that case. From my point of view, an empty link label is just an empty string that needs to be parsed. But I also understand that there is plenty of precedence to also look on an empty string as having no value. I am not sure if that is the way I would have gone with this, but I was happy to follow along with the specification with this one. Bolstering Up the Scenario Cases The more I used the new scenario-cases.md document, the more I was enjoying it and the confidence it brings to the project. While it is still early in the document's life, I am starting to rely on that document at the same level that I rely on the GFM Specification examples. Basically, if a parser can properly handle either one of those groups of tests, it is a good thing. If it can properly handle both groups of tests, it is a wonderful thing. As such, a certain amount of this week was spent beefing up that important document. Moving Simple Inline Links At the start of this block of work, one of the things that I wanted to do was to move the non-base test_inline_links_518 functions into the Series F group by moving them into the test_paragraph_extra_ group. While this was not a big move, it filled a hole that I had perceived in the Series F group tests. And since it was just moving the tests from one module to the other, the tests were already passing. That made the duration task seem to fly by. Adding Links as The Last Element in the Document Having just moved that small group of tests into the Series F group, I noticed that all the test cases in that group ended with a Text token, and not a Link element. As that affects what is checked at the end of a leaf block, I thought it was prudent to go through the Series F group and add a variation for each case that tested the base document without any elements after the document. That was not a difficult task but was a task that was both tedious and lengthy. I went through each of the 16 base tests registered in the Series F group and created a new variant of that base test. Once created, I removed any trailing non-link characters from each test document, double checking that I had not disturbed the Link element itself. As usual, I verified the HTML document against Babelmark , then running the tests to see if there were any issues. When I ran the tests, I was greeted with the good news that the parser itself was working properly and the consistency checks only required minor changes. Those changes were in the __handle_last_token_end_link function, each of them small adjustments to handle the various parts of the Link token in its various forms, but nothing that wasn't immediately resolvable. Following Up with Image Elements It should be no surprise that after completing the work documented in the previous two sections that I decided to follow that work up with ensuring parity for the Image elements in the Series F group. In total, 29 new scenarios were added to the group, mirroring the existing Link element tests. Due to previous hard work and a bit of luck, there was only one change required in the __handle_last_token_image function. In the case where the last token is a full Image token, I just needed to add a single line to properly increase the inline_height variable by one for each newline in the text_from_blocks field of the Image token. While the verification phase of each test took a while, the testing phase of these additions went by very successfully and very quickly. Moving Scenario Tests into Their Own Modules Over the course of the next three commits, I took on the immense chore of moving and renaming tests belonging to seven of the identified scenario case groups. Those seven groups were Series A to Series E, Series H and Series J. For each group, I created a new file, such as test_markdown_paragraph_series_a.py and moved tests in from their original modules, renaming them as I went. As I renamed those functions, I started to come up with a solution for how to identify each test uniquely. What I quickly settled on was to start the test name with the series that it belonged to, followed by a descriptive name based on the contents of the test document. In this way, I could easily tell if I repeated a test within a given group by looking at the name of the function. While this work was primarily copying and renaming scenario tests, it was exhausting. For each test, I needed to make sure that the name of the function matched the Markdown document contained within the test. Then I needed to take that Markdown document and run in through Babelmark to make sure the HTML output was correct. Repeated on over 100 scenario tests, it took a lot of time and a lot of patience to get correct. But in the end, it was satisfying to be able to see the groups come together, painting a cohesive picture of a group of passing tests along a given theme. Better Tests for Link Reference Definitions Switching back to resolving items from the issues list, the first thing that caught my eye was an issue dealing with Link Reference Definitions. Of all the leaf blocks elements that I have had to design and code for this project, the Link Reference Definition element was by far the most difficult to get right. It was no surprise to me to find the following item in the issues list: - what if bad link definition discovered multiple lines down , how to back track ? Starting back in April 2020 when I added support for Link Reference Definitions , I felt that while the feature was implemented, I knew that there was always going to be a possibility of a gap in the feature implementation. Because of the unique multiline nature of this feature, it is impossible to determine if the element itself is valid without reading the next line. As such, I had to implement a \"requeue\" functionality to allow the parsing of a possible Link Reference Definition element to be rewound and attempted again as a different element. While that has worked well, the bulk of my concerns over this feature centered around whether that rewinding functionality dealt with all possible side effects, not just the most common set of them. Given that history, I decided to add functions test_link_reference_definitions_166a and test_link_reference_definitions_166b to test for two more cases where an element was only discovered to be invalid. In the case of function test_link_reference_definitions_166a , I made sure that the title portion starts on the same line but was not properly terminated. This was to make sure that the entire element would be discarded as there was no solution where the Link Reference Definition could be considered complete under any circumstances. When I added function test_link_reference_definitions_166b , I took the opposite approach, starting the title on the next line. As I started it on the next line, the Link Reference Definition could be completed, just without the title. When I ran the tests for these two tests, it was no surprise to me that there was a failure. In looking at the tests, the failure was with function test_link_reference_definitions_166b which fails due to an extra Blank Line token being generated before the rewind is reprocessed. It took me a bit of time to realize that I needed to add the line: force_ignore_first_as_lrd = len ( lines_to_requeue ) > 1 at the end part of the __stop_lrd_continuation function that dealt with continuations that were partially successful. I just had to try different combinations before figuring out what the correct one was before proceeding. Don't Judge A Book… Indeed, when I came across this item: - 603 - href ? doesn ' t look right I agreed that it did not look right. The Markdown for example 603 is: < http : // foo . bar . baz / test ? q = hello & id = 22 & boolean > producing the HTML: < p >< a href = \"http://foo.bar.baz/test?q=hello&amp;id=22&amp;boolean\" > http://foo.bar.baz/test?q=hello &amp; id=22 &amp; boolean </ a ></ p > I was able to verify it quickly against Babelmark, but it took me a bit to figure out what the parser did to get to that result. The big thing that I had to remember for this case was that it was interpreted as an Autolink, which is meant as a quick way to provide references. As such, it makes sense that instead of a literal interpretation of the specified link, the processing leans more towards what the user probably intended. To that end, it makes sense that the ampersand ( & ) character in the link is translated into the named character entity &amp; for use in both the reference and the text. So, after thinking it through and checking it out, the function test_autolinks_603 is 100% correct. For extra points though, to produce the correct link, I determined that the following HTML block would be needed: <a href= \"http://foo.bar.baz/test?q=hello&id=22&boolean\" > http://foo.bar.baz/test?q=hello & id=22 & boolean </a> Yeah, I like puzzles, and this was a good one. I Really Need to Be More Specific While I am usually good at adding items to the issues list, this one was cryptic: - 620 - more bad cases , like < Huh? That really was not a lot to go on, but I gave it a shot. Without more information in the item, I just got a bit creative. Taking a look at function test_autolinks_604 , I took the initial URI autolink of <irc://foo.bar:2233/baz> , stripping it down to <irc:foo.bar> for function test_autolinks_604a and expanding the theme to <my+weird-custom.scheme1:foo.bar> for function test_autolinks_604b . Similarly, I took the email Autolink of <foo+special@Bar.baz-bar0.com> from function test_autolinks_613 and reduced it down to <l@f> for function test_autolinks_613a . Having added some good positive tests, I then decided to add negative tests. For function test_autolinks_620a I specified a theme with too few characters, while function test_autolinks_620b specified a theme with too many characters. Test function test_autolinks_613c specified a scheme with an invalid character in the theme, while function test_autolinks_613d had no domain part and function test_autolinks_613e had no name part. These tests all passed without incident, but it felt good to increase the scenarios and increase my confidence in the project. While I was pretty sure that these would all pass, as they are all based on regular expressions with specific character counts, it just felt right to explicitly test those limits and make sure they were consistent. Verifying Link Reference Definitions with Other Blocks In the same manner as other tests, this one started from the issues list item: - test_link_reference_definitions_183 is a partial lrd followed by bq , add cont + leaf blocks In test function test_link_reference_definitions_183 , the Link Reference Definition (or its acronym LRD as used in the item) follows an Atx Heading element. In the GFM Specification for this example, it explicitly states: However, it can directly follow other block elements, such as headings and thematic breaks, and it need not be followed by a blank line. While it states that it can follow other block elements, it only gave three examples: one after an Atx Heading element, one before a Thematic Break element, and one before a SetExt Heading element. Those three tests cases, spread out in the three functions between test_link_reference_definitions_183 and test_link_reference_definitions_185 were a good start, I felt that better coverage was warranted. Therefore, I created functions a to g for function test_link_reference_definitions_183 and functions a to f for function test_link_reference_definitions_185 to cover the before and after cases. Except for two of the tests, they all passed without incident. The two that did not pass were tests that involved a Link Reference Definition occurring both before and after a list. As I knew I was going to be finishing up with leaf blocks and heading to container blocks in the next week or two, I marked those test as disabled, added an item to the issues list, and kept on going. Cleaning Up Character Entity Tests At first, when I saw the issues list item: - test_markdown_entity * various extra tests I thought that I had missed a couple of cases and looked for some missing cases. It was during that search that I came across the following text at the end of the test_markdown_entity_and_numeric_character_references.py module: # TODO # # & and various forms at end of line # # 327 special parsing for html blocks? # <a href=\"&ouml;&ouml;.html\" x=\"&ouml;\"> # <x-me foo=\"&ouml;\"> # <script> # &ouml; bar=\"&ouml;\" bbb Comparing the items in the Python list, I determined that all those cases had already been covered by other tests, but there were some tests that I thought it was worth adding. While the example for function test_character_references_321 specified that the text must match that of an entity in the named entities table, I added function test_character_references_321a to make it explicit that it was a case-sensitive lookup. Similarly, functions test_character_references_322 and test_character_references_323 mention turning numeric entities into characters, but only included the special NUL character 0 as a byproduct of the text in an example. As such, I created the test_character_references_323a function to call attention to this special character, also showing that any number of leading zeroes does not matter for numeric entities. In a similar pattern, but at a higher level, I added the test_character_references_336 series of functions, named a to e . While I was okay with the examples showing the usage of entities in paragraphs, I felt that having explicit cases of entities in each of the other leaf blocks was useful. In order, the tests added named entities in each of an Atx Heading element, a SetExt Heading element, and Indented Code Block element, a Fenced Code Block element, and a Html Block element. I also verified that the entity was interpreted in the first two elements, and not interpreted in the last three elements, as per the GFM Specification. Finally, as a simple set of comprehensive tests, I wanted to have a good example of specifying a named entity using all three forms: named, decimal, and hexadecimal. As such, I created the test_character_references_extra_ functions with 01 using &quot; , 02 using &#34; , and 03 using &#x22; . I know that these functions were going to pass ahead of time, but it gave me confidence knowing that I had a concrete set of three tests showing that the form of the entity didn't matter, as they all produced the same HTML results. Closing Things Up Even though I was getting close to writing time on Sunday morning, I wanted to try and clear one more easy issue from the list: - is HTML transformer using text_from_chars , instead of other field ? - see https : // github . com / jackdewinter / pymarkdown / commit / a506ddd3bda08a8ca1d97a7b0d68c114325b545e `extra_74` This was more of a bookkeeping issue than anything else, or at least I hoped it was. During a previous change on 02 Oct 2020, I thought I had noticed that the HTML transformer was using the text_from_blocks field to create the text for the links. Thankfully, resolving this took a quick look at the __handle_image_token function in the transform__to_gfm.py module to verify it was not using that field. When I took a second, this should have been more obvious to me. While it is possible to derive the image_alt_text field from the text_from_blocks field, it is the last thing I would have thought about when generating HTML. But I still felt good that I verified this and dispelled any doubts about the HTML output being based on the wrong part of the token out of my mind. What Was My Experience So Far? The work went on like it always does, but an interesting milestone was met with the completion of this work: any outstanding issues clearly identified as being attributable to a leaf block has been solved. Short version? I finished any issue that was clearly a leaf block issue. While the realization of that goal was not a big thing to me, it wasn't a small one either. It still meant that I needed to check how leaf blocks interacted with the two container blocks, but it reduced the number of things to check to just interactions with and between container blocks. That was a good feeling, knowing I had hit that mark. It increased my confidence that things were going in the right direction. It is still too early to tell, but I am now starting to hope for an initial release of PyMarkdown as a linter in the early parts of 2021. That felt good typing that. Real good. What is Next? With the work done to verify the leaf blocks, the next week was going to be full of me trying to reduce the issues specific to list blocks. Closer to the line I get!","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/11/23/markdown-linter-delving-into-the-issues-9/","loc":"https://jackdewinter.github.io/2020/11/23/markdown-linter-delving-into-the-issues-9/"},{"title":"Markdown Linter - Delving Into the Issues - 8","text":"Summary In my last article , I continued in my quest to reduce the size of the issues list. In this article, I take a bit of time to focus on adding some depth to the scenario cases table. Introduction As I get closer to the end of this initial phase of the PyMarkdown project, I find myself measuring the project's success differently than I did at various points in the past. Initially the success criteria statement was \"does it work at all\". Then it moved on to \"is it implemented for most circumstances\". Finally, in the last couple of weeks, it has moved on to the \"what did I miss\" stage. And wow, does it seem like it has taken both a couple of weeks and almost a year at the same time. While this phase might seem boring to other people, people that are Testers or Automation Developers 1 often enjoy times like these because of two words: exploratory testing. Our job is to make sure the thing we are testing is working properly. To a lot of us, exploratory testing is basically like leaving a kid in a room filled with hundreds of opened LEGO sets and saying to them \"show me what you can build!\" It is in those times that we get to \"play around\" and experiment. We use that time to try and understand where the issues are, and which scenarios give us the most benefit to test for the least cost. And as this phase is closing, this type of testing is pivotal in being able to close out the phase cleanly and with confidence. And as I have mentioned before, testing and test automation is not about trying to break something, it is about reducing the risk that the user of the product will experience that thing breaking. That is where my recording of the bulk testing in the scenario cases tables comes into play. Instead of looking for one issue at a time, those tables take a group of concepts and test them as a single group. I have found that the benefits of that approach are twofold. The first benefit that I have experienced is an increase in confidence. This is an easy one to explain, as I can concretely point to a collection of tests along a theme and know that any scenario along that theme is working properly. The second benefit is one of economy. The cost of finding an individual issue is expensive. It takes exploration or debugging to find the issue in the first place, with the extra debugging and logging to try and figure out what the issue really is, followed by the extra work required to fix the issue. That ends up being a lot of time. By amortizing the work over an entire group of tests, that cost is drastically reduced. Having experienced these benefits on this project, I decided to dedicate a weeks' worth of work to adding to the table, to increase my confidence and to accelerate my journey to having a shippable project. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 27 Oct 2020 and 31 Oct 2020 . Links/Images and Other Inline Tokens The easy part about coming up with a theme is the title of the theme itself. For the Series J theme, it was easy an easy theme to identify: Link/image elements followed by various other inline tokens. In the scenario-cases.md file, that description is right there after the name of the series. But the hard part of a theme is the act of trying to make sure that each scenario that you want in that theme is present. And often, I miss the mark. When I originally added Series J to the document, I thought that I had added each of the qualifying inline elements to the group. Going through the list in my head, I thought I had each of those newlines added when I created the group. But in retrospect, I did not have the right viewpoint as I missed a large part of that group: internal versions of the scenarios I had already added. I came about this when looking at the J8 test and experimenting by creating two new tests: | J9 | inline link with split emphasis in label | ` abc \\ n [ a * li \\ nnk * a ]( / uri \"title\" ) \\ ndef ` | test_paragraph_extra_e1 | | J9i | inline image with split emphasis in label | ` abc \\ n ! [ a * li \\ nnk * a ]( / uri \"title\" ) \\ ndef ` | test_paragraph_extra_e2 | Whereas the other tests in the Series J group focus on the inline elements after the Link elements and Image elements, I wanted to experiment with performing the same process on those token's link labels, inside of the tokens. And that experimentation bore fruit. The J9 test failed in the consistency check with an overcount on the line number. After a quick debugging session, I discovered that the rehydration_index that I have mentioned in previous articles was being added to, both in the link label and in the Link's encapsulated tokens. It was just a simple fix from: last_token . rehydrate_index += 1 to if not link_stack : last_token . rehydrate_index += 1 After that quick fix, the issue was addressed. But it outlined something to me that I wanted to get back to before the end of the week: inline elements within the link label. More on that near the end of this article! Adding the Series L Theme This work was the major focus of the week, focusing on links or images contained within the link label section of another link or image. Origin Story While I came up with the idea for this group recently, I have been thinking about this topic since at least 31 July 2020. It was at that time that I did the research that I would document in a section labelled Example 528 , followed by the work undertaken to fix that issue, documented in the section \"creatively\" labelled \"Fixing\" Example 528 . That scenario test, encapsulating the GFM Specification's example 528 , is a bit contrived but a good example nonetheless. Given the Markdown document: ! [ [[foo ] ( uri1 ) ] ( uri2 ) ] ( uri3 ) the expected output is the HTML document: < p >< img src = \"uri3\" alt = \"[foo](uri2)\" /></ p > The reason I say that this example is contrived is that I can visualize useful cases of a Link element within an Image element, I have a hard time coming up with a similar example for a Link element within a Link element. A practical instance of this example is the following Markdown: [ ![moon ] ( https : // nssdc . gsfc . nasa . gov / imgcat / midres / gal_p37329 . gif ) ] ( https : // en . wikipedia . org / wiki / Moon ) which is rendered as: < p >< a href = \"https://en.wikipedia.org/wiki/Moon\" >< img src = \"https://nssdc.gsfc.nasa.gov/imgcat/midres/gal_p37329.gif\" alt = \"moon\" /></ a ></ p > At the visual level, this HTML provides for a picture of a moon from the NASA archives. When that image is clicked on, the browser goes to the Wikipedia article on the moon. Useful element compositions like this is probably why there are multiple examples of a Link element within an Image element in the GFM Specification. However, in that same specification, only the above example provides for a Link element within a Link element within an Image element. As the GFM Specification provides a unified interpretation of Markdown, Example 528 is presented as a recipe on how to handle cases like that. My guess was that if that example was anything other than an outside case, there would be more examples outlining that pattern. Formulating the Test Group With the help of John McFarlane , I was able to figure out the part of the algorithm that I had misunderstood and fixed the error. Having invested all that research and work to fix that one issue, I wondered if there was a better way to handle issues with examples like that. That was when I really started thinking about how to cover all the cases that would lead to having a good group of tests around Example 528. The downside about that exercise was that as soon as I thought about how to cover all those scenario cases, a couple of negative things got in the way. The first big one was example 583 and the paragraph that follows it: Though this spec is concerned with parsing, not rendering, it is recommended that in rendering to HTML, only the plain string content of the image description be used. Note that in the above example, the alt attribute's value is foo bar , not foo [bar](/url) or foo <a href=\"/url\">bar</a> . Only the plain string content is rendered, without formatting. Basically, given the Markdown: ! [ foo [bar ] ( / url ) ] ( / url2 ) the specification suggests that the only content that should be used is the foo text contained at the start of the Image element's link label, and the bar from the link label of inner Link element. Therefore, after processing, the resultant HTML is: < p >< img src = \"/url2\" alt = \"foo bar\" /></ p > The downside of this information is that there are at least 64 \"simple\" combinations of links inside of links, images inside of images, links inside of images, and images inside of links. Those simple combinations are 4 types of links inside of 4 types of links inside of 4 combinations of link and image elements. That lays the groundwork for determining which combinations should be tested to address scenarios like example 528 but does not address example 528-like scenarios. Already taking the work required to create a single test for each combination into account, the bigger downside was going to be the verification of each of those tests. Increasing the cost of this downside was the possibility of finding issues that needed to be addressed while the verification phase of the tests was ongoing. It was daunting, but I felt strongly that it needed to be done. So, I started working on identifying the combinations that were needed, and added them to the scenario-cases.md file. It was then that the hard work for this issue would start. Working the Issue The bulk of the work on resolving this issue was done over 4 days of lengthy sessions. To reduce the cost of completing this work, I decided early on to come up with a simple strategy to hopefully allow me to copy-and-paste tests where possible, hopefully avoiding extra work. To that end, I figured that the combination of Link elements inside of Link elements was the best combination to start with. I just hoped that I could reuse a lot of the test code. The table that I created in the scenario-cases.md file was a good tool to create the tests from, but it lacked any Markdown that I could use as a template. Keeping it simple, I started with the Markdown a[foo [bar](/uri)](/uri)a , and transformed the Markdown for each scenario from there. Once I started working with non-inline Link elements, I added in a simple Link Reference Definition, including link referenced to that Link Reference Definition and to a non-existent Link Reference Definition. Following my usual pattern, I executed that new test and manually verified the tokens, before copying them into the test. After that, I executed the test again and copied the HTML output into the test, after once again manually verifying that it looked right. Even after that step, I used BabelMark against the Markdown for each test, comparing my parser's output against the commonmark.js output. This process was long, drawn out, and tedious… but it worked. The hard part about mentally processing a lot of these combinations is that because of the rule that Link elements cannot contain Link elements, I needed to do a lot of tedious parsing of each combination. It was not as simple as just looking at the Markdown and quickly knowing what the answer was. I kept a copy of the GFM Specifications implementation guide open in another window, just to make sure I was doing things in the right order. Even then, I double checked, and triple checked each transformation being running the tests, just to make sure I had things done correctly. After a couple of days of work in the evenings, I had finished this first part. For the other three parts, I was hoping I could leverage the work heavily to allow me to shave some time off the process. Completing Work On The Issue With Link elements inside of Link elements out of the way and committed to the repository, I started to work on Image elements inside of Link elements. The big change here was that while nested Link elements need to be parsed carefully, the parsing of Image elements inside of Link elements was more natural to me. The Link token's link label field contained the \"raw\" form of the link label, while the tokens between that token and the end Link token contained a processed version. With a decent amount of experience in reading Markdown due to this project, I was able to gain proficiency at those required changes quickly. It therefore followed that the verification part of the process went a lot smoother than with nested Link elements. Moving on to nested Image elements was a relatively easy step to take from there. As the Image elements create their alt attribute values by processing the link label instead of encapsulating it (as with Link elements), the two big changes were easy to consistently apply across each of the new tests. The first change was to remove any tokens that were being encapsulated between the start Link token and the end Link token, replacing them with a single Image token. The second change was to look at an example nested Image element and determine what the alt attribute was going to be. After the first two or three tests, I started to get pretty good at doing that before I started verifying the tokens, saving a lot of time. Finally, completing the group with the Link element inside of an Image element was almost trivial. As the different between a Link element inside of an Image element and an Image element inside of an Image element is one character ( ! ), the link labels remained constant between the tests. As such, only minor changes were required to these tests after copying them from the previous group. Dealing with Relatively Minor Issues To get all the test passing and verified was a chore, but the good news was that most of the work was contained within the scenario test process that I have already defined. Considering the scope of the group of tests, the number of issues found in the non-test parts of the project were very small. To be specific, there was only one change required. When adding the tests for Image elements within a Link element, the only change that I needed to do was to change the expression: if last_token . token_name == MarkdownToken . token_paragraph : to if last_token and last_token . token_name == MarkdownToken . token_paragraph : To be blunt, it was both confirming and unsettling at the same time. The confirming part of the process was that I had done the work on the project properly, with only a very slight change required. And hopefully it does not sound like I lack confidence, but it was also unsettling. After working on scenario tests across an entire theme, taking three to four days in the process, I somewhat expected the new scenario tests to find something that I missed. I was happy that it did not find anything, do not get me wrong. It just took a bit of getting used to. And it was still a validation of the parser code itself, as the change was only required in the consistency checks. After some thought, it sank in that at this late stage of the project's initial push, I wanted the results to be exactly this: the parser was being proved as validly constructed, again and again. Rounding Out Series J Based on the research that I did at the start of the week, I wanted to close out the week by responding to that research by rounding out the Series J group. As with my recent work in adding the Series L group of tests, I started out by scribbling down the combinations that I thought needed to be covered, looking for gaps that I had missed. While not a big gap, I added tests J2a and J2ai to fix a small gap where I did not have a newline in the Raw Html element. With that initial fix made, the rest of the changes were fairly in scope with the new test that I documented at the start of this article. Starting with emphasized text, I added scenario descriptions and scenario tests encompassing a wide range of inline tokens, including Hard Line Break elements. I double checked everything and then began my usual process of executing and verifying the tests. And boy was I glad that I did! While it was not a lot of code, I made changes to the __collect_text_from_blocks function and the __consume_text_for_image_alt_text function to properly handle these new cases. In the case of both functions, most of the inline tokens were handled, but the two Autolink inline tokens and the Hard Line Break tokens were not handled. While the extra code to remedy these issues was relatively small, it was a good find. It felt good that these issues were found directly because of this new group of scenario tests. It felt like validation of the approach I had taken. From a consistency check point of view, there were only a couple of issues that were found. Mirroring the change made for split emphasis at the start of this article, the __verify_next_inline_hard_break function was changed to only increase the rehydrate_index if the token was not inside of an active Link token. The other issue was a small, nitpicky thing: adding the text + 1 to the output for the main assert in the __verify_next_inline_code_span function. Other than those two changes, the consistency checks had a clean bill of health. What Was My Experience So Far? I have to admit that I wondered (out loud, to my dog, who did not help in the discussion one way or the other) whether this was a good investment of time once the week had ended. The broad sweeping groups that I added confirmed that the parser code was in good shape, as were the consistency checks that watched over that code. Maybe it was me still thinking I was in the middle part of the \"game\" of creating the project, and not the end game where I believe I am currently at. But as I mentioned above, I had both positive and negative emotions about the results. Happy that things were going well, but not as trusting of those results as the tests had proved out. Taking some time to think about it as I am writing this article, I do think my descriptions of \"middle game\" and \"end game\" are appropriate metaphors to where I am on the project. After a long time spent in the middle of the project, I believe it is just taking me some time for me to switch into the mode where I am wrapping things up to complete this first phase of the project. As such, I when I start that week's work, I believe that I am going to find more issues than I find, and then I turn out to be happy when I do not find many issues. I truly believe that when I properly switch my mentality to an end game mentality, I will be expecting the tests to verify the work already done. Does that mean the project will be properly tested? Nope. If you ask any person experienced with testing a project that question, they will not give you a solid answer. That is not due to lack of confidence, but because there is not one to give. There will always been edge cases that are not thought of and weird things that can show up. It is my belief that you can find most of the issues with any given project, but it is always a question of when that next issue will show up, and who will find that issue. In both professional life and for this project, my goal is the same. If not to find that issue before the customer does, then to make sure I have a solid assessment of the risk outlined and evaluated. And with these latest changes in the past week, I see that measure of risk going down, which is a good thing. What is Next? With a solid weeks' worth of \"big ticket item\" issues resolved, I decided to try and tackle a lot of the smaller issues. I just started with a couple of easy ones and kept on going until I stopped. Nomenclature can be everything, and changes from job to job. From my viewpoint, Testers are people that are very methodical and document what they did, in what order, and what the results are. Automation Developers like me, a SDET or Software Development Engineer in Test, take documented patterns and results, writing test code and test frameworks to make sure that those scenarios can be written as code which can be used to automatically validate the results. There are exceptions, but the general rule is that most Testers only have a small amount of the skills required for an Automation Developer, while most Automation Developers are very adept at most of the skills required for a Tester. Both skill sets are very useful on projects from a quality point of view. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/11/16/markdown-linter-delving-into-the-issues-8/","loc":"https://jackdewinter.github.io/2020/11/16/markdown-linter-delving-into-the-issues-8/"},{"title":"Autism, Stress, and Anxiety","text":"\"I don't know. I really don't know.\" Those were the last words I said before a solid minute's worth of silence. After being married for seven years, my wife and I had a \"heated discussion\" that was almost an argument. That was unusual in itself. But even more unusual was that I was at a loss of words. Her question? \"How can I help?\" Now, our marriage is an interesting one already. I am a software guy with a diagnosed case of Autism, with an anxiety disorder and sensory overload disorder to boot. My diagnosis is such that unless I tell people I have Autism or they know what to look for, I can often hide under the radar. But even so, it does have significant impacts on my day-to-day life. In contrast, my wife is a lawyer that has a heart of gold, preferring that people play together nicely. When they don't, she is the one that is not afraid to step in and do or say what needs to resolve the situation. But uniting us together is the fact that we both have big hearts and open minds. The combination of that with humor and laughter has always been a major component to our success in marriage. But in this case, I was stumped. Earlier in the day she had told me that she wanted to go on a vacation to Florida with her family. During a pandemic. On an airplane. With people she doesn't know around her. I really didn't know what to say. I remember ranting and raving about it, but as to what I said at that moment, I don't remember too much of that. It was all a blur. My logic and my emotions were all over the place. Here is where things get interesting: I know we have two different points of view on things like this. While my wife is weird (heck, she married me!), she is solidly neurotypical with only a middling amount of introvert. Even with her being cooped up in our house since April, she has been going into her workplace once or twice a week since our state allowed it during the summer. Even with this, I can tell that not being able to get out and stretch her legs among people is chewing away at her sanity. I know that her going on vacation with her family is a good thing for her. It is what she needs. For me, it is a completely different equation. Starting with the little things, going on a vacation with crowds of other people around is just not a lot of fun for someone with sensory overload disorder. Trying to mitigate the effects of that disorder is many times more exhausting than dealing with anything else. Adding to that, my anxiety disorder and my Autism are acting the part of tag team wrestlers. My Autism takes the first shot with an elbow, keeping me isolated from groups of people that I know care about me. Then my anxiety disorder takes control with an Irish Whip , either complaining that I am getting too little information about the current state of the pandemic and economy, or too much information. Then my Autism tags in with a high-five, hyper-focusing on one little thing that I think I heard or didn't hear. Finally, my anxiety tags in with a hurricanrana , trying to figure out all the possible situations that can happen and ways to deal with each of them. 1 I struggled to figure out what metaphor to use to explain the vicious cycle that escalates in my head. While I haven't watched professional wrestling in years, I believe it is the best metaphor by far. At its heart, professional wrestling is a show. For me to communicate with people every day, I must act a bit to communicate clearly. Professional wrestling contains the good guys, or faces , and the bad guys, or heels . Just like in the shows, sometimes my positivity and clarity win, and sometimes my anxiety and lack of connection with others wins. And as anyone who has followed wrestling will tell you, behind the bright white smiles, coifed hair, and shiny outfits are people that are often hurting doing the thing they love to do. Thinking about how to answer that question as I write this article, where am I now? I still don't know. I do know that in terms of where I am, I am currently fighting my own personal wrestling match day-by-day. There are some days where the good guys win and some days where the bad guys win. Some of the days I am acting for my audience, and some of the days I am trying to be my honest self with them. Just like a wrestler needs to understand where they are in the ring relative to the others, I need to understand where my wife is relative to where she needs to be. And yes, somedays I am controlling my anxiety and Autism, and somedays it controls me. And forgive me if I it seems like I am singling myself out from the people around me and the stress they are experiencing. But there are two things that I feel make a big difference. My Autism and anxiety do amplify each other, sometimes out of control. Also, I don't have the option of resorting to things like alcohol or other things to help me forget for a while. Quite the opposite, those things tend to amplify how I feel even more. I don't get a break… but I am okay with that. This is something I have dealt with every day of my life, this tug-of-war going on inside of my head. Yeah, it is exhausting. It is so unbearably exhausting at times. But the flip side of that is: I know I can handle it. While I still don't have a good answer for her question, I am working on it. What I do know is that our solutions to how to deal with the stress of this pandemic are going to be on opposite sides of the spectrum. As much as it would be a terrible idea for me to think about, I believe that her flying down to Florida with her family is one of the better things she can do for herself. In contrast to her approach, I do know that a lot of the things that I need to do are small and personal. More mental breaks. Increased exercise. Making better choices when eating. No big silver bullets, just little things that add up over time. And I guess that is okay. The important thing I realized in all this is that we are asking ourselves the right questions. \"How are you?\", \"What are you feeling?\", and the important one: \"How can I help?\" I know I don't have that answer for her on that last one, I am working on it. I do know that part of my answer is this article. It's making sure others, people with Autism or not, ask themselves, their friends, and their loved ones those questions. I grew up in Southern Ontario in the 80s. Yes, I watched wrestling. Everyone I knew did. ↩","tags":"Autism","url":"https://jackdewinter.github.io/2020/11/11/autism-stress-and-anxiety/","loc":"https://jackdewinter.github.io/2020/11/11/autism-stress-and-anxiety/"},{"title":"Markdown Linter - Delving Into the Issues - 7","text":"Summary In my last article , I continued in my quest to reduce the size of the issues list. In this article, I make good progress in removing items from the issues list. Introduction This article is probably going to be a lot shorter than the others in this series. It is not the case that I did a lot less work, just that there is just a lot less of that work to talk about. The bulk of that work was an effort by me to get the extra groups of scenario tests more organized. Sometimes the work I do on the project is easy to describe and takes a lot of words to explain, as usual. But in this case, it is just a lot of work that has a short description. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 23 Oct 2020 and 25 Oct 2020 . Keeping Things Organized For most of the time that I have spent on this project, I have maintained a very good grasp of where things are and what needs to be done. Sure, I use the issues list to keep track of all the little things that I need to fix or check, but I rarely forget about items on the list. Usually, within seconds of reading one of the items on the issues list, I remember a good amount of information about what the problem is and why I added it. But since the project has been going on for a while now, I sometimes forget when I added the item to the list. When it comes to the scenario tests, it used to be the case that I had an equally solid hold on the tests and what they covered. At first it was easy, one example from the GFM specification equals one scenario test. Then, as I was working through the validation of the parser, I added some extra scenarios by adding a letter to the end of the parent scenario and documenting the change. At that point, there was still a good link between the example that spawned the new scenario test and the parent of that test. And then came the \"extras\" scenario tests. Most of them were added to provide additional testing for the line/column numbers needed for the linter than for the parser itself. As the example provided for in the GFM specification are targeted for parsers, I felt it was totally reasonable to add extra scenario tests to ensure that the different in testing between the needs of a parser and the needs of a linter were covered. But what started out as a few scenario tests, rapidly grew to a set of tests that numbered over 140. The tests are their contents started to get jumbled up in my head on a frequent basis. What group of tests was the test I was just looking at a part of? Did I cover all the cases within in the group? Did I miss one? If I missed one, how could I tell? It just got more difficult to keep things clear with each case I added. Finding A Solution Recently, I had added an item to the issues list to start looking at this: - need comprehensive table with tests that qualify for each test case i . e . para_extra with different groups , links and images Now that I had a few weeks to think about it, I believed a solution was in order. The first big decision I need to make was to figure out the medium for this table, with Markdown being the natural selection. Creating the Markdown document scenario-case.md in the root folder, I tried four or five formats before settling down on the final format. The first column contains a unique identifier for that test. Starting with the letter id for the group of tests, I made sure that the rest of the identifier clearly described what the scenario test contained using a common id schema. Following that identifier column are the columns that contain a short description of the scenario test, the relevant Markdown from the test, and the actual function name of the scenario test. Starting by enumerating the scenario tests for the test_markdown_paragraph_extra.py module, I left space for a trailing column that would also include the function name of the scenario test for the test_markdown_setext_headings_extra.py module. I Really Needed This When I finished creating this document as the result of this multi-day task, it was extremely obvious why I needed it: there was a lot of information to process. There were a solid core of groups in the initial document, and by documenting each group, I easily found things that I had missed or not completed. By clearly delineating each group of tests, it also became a lot easier to see the patterns in the groups and why they were important. First, there were the basic groups, Series A to Series E. These were simple foundational tests for the complete set of inline elements. Series A tested the inline element at the start of a line, followed by text. Series B and Series C followed those tests by testing the same inline elements with text around the inline element and text followed by the inline element. While they would not be seen that way naturally, Series D provided tests for each inline element alone in a document by itself. Finally, Series E extended the Series D tests by testing those inline elements that can contain a newline character, rounding out the foundational tests. From there there were 4 groups of tests that I had added that all dealt with link elements and image elements. The Series F tests provided for a newline character in each part of all 4 link types. The Series G tests provided for the same type of tests but tested for the inclusion of a backslash escape sequence and character reference sequences. Due to some issues I encountered with Code Span elements and Raw Html elements inside of Link Labels, the Series H tests were added to provide a thorough testing of those combinations. Rounding out those tests, the Series J tests provided for various combinations of Inline elements with Link elements and Image elements, once again added due to some issues that I ran into. The Proof Is In The Pudding As they used to say \" the proof is in the pudding \". Basically, the only way that I am going to find out if this works or not is by using it and judging the results. However, I can say that this initial task of putting the table together has already yielded positive results. There were tests that were duplicated, and there were new distinct tests that were child tests of existing tests. Putting that table together helped me clean up the extra tests by fixing up those cases. In addition, it found a missing scenario that dealt with an inline link type that did not have a title but had whitespace after the URI. That scenario is one that I would not have found otherwise. I feel the balance point of this work needs to be mentioned, as the cost of putting this table together was a couple of days' worth of work. That cost would have be spread out over numerous issues if it was started at the beginning, but at that point, I am not sure if the benefit of putting this table together is something that I thought would justify the cost. As a matter of personal sanity, I try not to do the \"what ifs\" too often. From my point of view, while it might have been better to do this earlier, it was at this point that I started seeing that it needed to be done. After I made that observation, it was only a couple of weeks before I took the time to create the table. And that time was spent thinking about how I wanted to organize the table, so it was not wasted time. I guess the thing I am trying to say is this: do not kick yourself and do not rush things. If you can see something ahead of time, and the benefit is worth the cost to mitigate the issue, then mitigate. Otherwise, wait until the benefit becomes worth the cost, plan it out to make sure you can do it cleanly and clearly, and then work on it. Is A Character Entity the Same As A Character? In the section of the GFM on Entity References , the specification is very clear about how Character Entities are to be used: Entity and character references cannot stand in place of special characters that define structural elements in [Markdown]. For example, although * can be used in place of a literal * character, * cannot replace * in emphasis delimiters, bullet list markers, or thematic breaks. To be blunt, you cannot get clearer than that. If you want the Markdown document to be interpreted properly, your document must use the actual character used to invoke the behavior you want, not a character reference. For the most part, I was very comfortable with that, and I was sure that I had adhered to that rule throughout the parser. When it came to the newline character, I was not as confident. I was pretty sure that if I tested all the inline elements with the character entity newline, that the HTML output would look correct. However, while I was sure that I properly handled most of the cases, I was not as sure that I had properly handled all those cases. Therefore, enter Series K. To get this series started, I began with the Link elements. Starting with the inline Link element, I enumerated the 7 places where the character entity &#xa; could be inserted, followed by the 2 places for the full Link element and 1 a piece for the shortcut Link element and the Collapsed Link element. Once that was in place, I duplicated those scenarios, transforming the Link elements into their equivalent Image elements. Finally, I added cases for the Emphasis element, the Code Span element, the Raw Html element, and both Autolink elements. With a total of 27 new scenarios to cover, I started to work on creating these scenario tests, one at a time. As with other tests I have documented in previous articles, I was very precise and meticulous with the creation of those tests. When each test was created, I gave it a manual check before testing it against the parser's HTML output, but only after running the Markdown against BabelMark . In the end, 11 of those new scenario tests resulted in failures, with 16 of them passing right away. But at that point, it had taken me most of the day to add and verify the tests, along with all the things that life threw at me that Saturday. A bit reluctantly, I committed those tests as-is, after adding 11 new items to the issue list to track those tests I needed to finish. Then I went to sleep. Finishing Up the Series K Tests After a good night's sleep, I took a look at that week's article notes. I figured that the article notes were in a good enough state for me to look at those 11 new items I added the night before. Planning out my work to address those issues, a couple of things leapt out at me. The first thing that I noticed was that while I had added some good tests for the link types, I had not varied the type of character entity or numeric entity that I used in place of the newline character. While I was confident it had been covered in the base GFM Specification, I believed that for completeness it would be good if I also added it in this group of tests. When I was done fixing that, there were 14 new tests in the table, 7 new tests for Link elements and 7 new tests for Image elements. In addition, there was 3 extra scenario tests that I added to complete the coverage for the other inline elements. When that work was done, the Series K group contained 42 tests. The other thing that I did not realize the day before was that the failures were nicely bucketed into three groups: one that failed in the InlineProcessor class, ones that failed in the __verify_next_inline_handle_previous_end function, and ones that failed in the __verify_next_inline_text function. While it would have been nice if they all had the same root cause, three different causes to examine was still better than eleven! Attacking the Parse Failure Cases As luck would have it, this was the easiest problem to fix. In the code for the __calculate_inline_deltas function, there was an assert statement as follows: assert link_part_index > - 2 , \"Newline in link token not accounted for.\" When I originally added this code, I was being defensive, thinking that I had not properly handled a newline character occurring in the link label part of the Link element and the Image element. It took me a bit of time and some extra debug scenarios, but I was able to conclusively prove that newline character in link labels were already been handled by another part of the code. As such, I was able to comment out that assert and resolve two out of the eleven failures. On the the next one! Properly Handling Split Lines After a bit of looking at the remaining tests, one pattern leapt out at me. Immediately, it looked like the line/column numbers being calculated by the parser were correct, but the same calculation for the consistency checks was off. With only a small amount of looking at the problem, the cause for that result became obvious to me almost immediately. When any of the character entities or numeric entities are used in a normally processed part of the document, a replacement sequence is placed within the token to represent that sequence. For the Markdown generator, the original entity is persisted, and for the HTML generator, the replacement text is persisted. This is performed by using a replacement sequence such as \\a&#xa;\\a\\n\\a , making it clear which part is \"what is replaced\" and which part is \"what it is replaced with\". And therein lied the problem. In the __verify_next_inline_text function, the current_line variable is split as follows: split_current_line = current_line . split ( \" \\n \" ) From there, each part of that split line is processed. But due to the above replacement sequence, each entity sequence like &#xa; generates another entry in that list. Now, if the HTML output were being verified, that would work well. But as the consistency checks are used to verify the line/column numbers in the Markdown document, those newline characters confuse the issue. I realize that this may seem confusing, so consider a small Markdown document like: a &# xa ; b When this document is parsed, the above Python code will generate an array with two values in it: [ \"a \\a &#xa; \\a \" , \" \\a b\" ] If that does look right, it is because it is not right. It is a faithful interpretation of the request made to split the string a\\a&#xa;\\a\\n\\ab at its newlines, but it does not understand that it should only consider the first part of the replacement sequence, not both the first and the second parts. I needed to come up with a way to deal with this issue. Fixing the Issue With Split Lines That is when I started putting together the __handle_newline_character_entity_split function. Given that array of split lines, this function specifically looks for any entries that start with the replacement character ( \\a ), and the previous entry ends with the first part of the replacement sequence. Basically, as the data is split on the newline character ( \\n ), we want to look for cases where one entry ends with the \\a&#xa;\\a part of the sequence and the following entry starts with the \\a at the end of that sequence. When it was all said and done, the function looked like: def __handle_newline_character_entity_split ( split_current_line ): try_again = True while try_again : try_again = False for search_index in range ( 1 , len ( split_current_line )): if split_current_line [ search_index ] . startswith ( \" \\a \" ) and split_current_line [ search_index - 1 ] . endswith ( __create_newline_tuple ()): combined_line = ( split_current_line [ search_index - 1 ] + \" \\n \" + split_current_line [ search_index ] ) split_current_line [ search_index - 1 ] = combined_line del split_current_line [ search_index ] try_again = True break return split_current_line After testing this new function with a single scenario test, I execute all the failed tests again, and was rewarded with only four failing tests. Time to buckle down and get the last ones taken care of. Cleaning Up With four tests remaining, I was ready to put in a lot of work to figure out what the problem was. I made sure I had a good snack and a big glass of water, turned on the debug logging output for some of the tests, and proceeded to look at the failed scenario tests and the scenario tests that were around them and were passing. The first issues I noticed in the failed tests were in the __verify_next_inline_handle_current_end function, where the number of newlines characters are counted. Looking at the difference between the reported line numbers and the calculated line numbers, there was a direct correlation between the number of entity sequences used and how much the reported difference in line numbers. Having seen this pattern many times before, it was easy for me to see that instead of using fields in this manner: newline_count4 = ParserHelper . count_newlines_in_text ( current_inline_token . start_markdown_token . link_title ) I needed to change that to: pre_link_title = current_inline_token . start_markdown_token . link_title if current_inline_token . start_markdown_token . pre_link_title : pre_link_title = current_inline_token . start_markdown_token . pre_link_title ... newline_count4 = ParserHelper . count_newlines_in_text ( pre_link_title ) After a quick check against the tests, it was obvious that there were also some issues with the __verify_next_inline_handle_previous_end function, of the same type. Looking at that function, I needed to do the same with the link_uri field and the pre_link_uri field that I did in the above example with the link_title field and the pre_link_title field. While verifying those changes, I did notice that there was an omission that was not tested. In cases where an inline link contains an URI that is inside of \"angle brackets\" ( < and > ), I was not adjusting the counts to accommodate for that. After adding an extra test or two, the solution to this was easy, adding the code: if parent_cur_token . did_use_angle_start : part_3 += 2 to the __verify_next_inline_handle_previous_end function. What Was My Experience So Far? There were a couple of times during the creation and validation of the scenarios table that I wanted to give up. It was a large body of work with no immediate validation of its correctness. It was also a body of work that could easily fall out of sync with the actual scenario tests themselves, so there was the future work to maintain and revalidate the table to consider. But still, that table was, and still is worth it! I emphatically believe this with few reservations, if any. I do agree that I need to be careful in concluding about whether to add a scenario test or two versus adding a new scenario test group. That is a no-brainer. But this is another tool that I have in my toolbelt to help me make sure the quality of the PyMarkdown project is where I want it to be. Take the case with the group for the newline character entity sequence. If I had added only a couple of tests, I would have wondered if I had captured all the cases. By adding the tests as a group of cases, I carefully documented and tested the cases that I could come up with, but left the door open for more cases to be added at a future date. For me, that is a winning proposition for the right kind of scenario tests. And as I mentioned above, the proof is in the pudding. I have already cleaned up an old assert statement in the Inline Processor, something I would have overlooked without that group of tests. In addition, it found at least three issues with the consistency checks for the line/column numbers, making that watchdog code more complete. That small step towards completeness of the consistency checks means that I have more confidence that it will detect any issues that arise when something is changed. I know I am getting close to the end of the project, and my goal is to add both single tests and groups of tests with no changes to either the parser itself or the consistency checks. And this work brought me one step closer to that goal! What is Next? Now that I had the master table of scenario tests and their groups together, it was time to leverage that information and add to it. That was the focus of the next week's work, work that was indeed satisfying.","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/11/09/markdown-linter-delving-into-the-issues-7/","loc":"https://jackdewinter.github.io/2020/11/09/markdown-linter-delving-into-the-issues-7/"},{"title":"Markdown Linter - Delving Into the Issues - 6","text":"Summary In my last article , I continued in my quest to reduce the size of the issues list. In this article, I make good progress in removing items from the issues list. Introduction After having a week where I felt like I barely made any progress, it was nice to get a week where I was able to get some work done while keeping balance with my personal life. For some reason, the stop-and-go nature of last week was not repeated this week, and I was grateful for it. But this week was not really about any one of those issues, just getting some good, solid work in to reduce the size of the issues list. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 14 Oct 2020 and 18 Oct 2020 . Finishing Up the Work After a day's rest from writing that week's article, I looked at my notes and noticed there was one little issue that was left over from the last weeks' worth of work: - test_paragraph_extra_b5 - adding extra newline somewhere, possibly not clearing after previous image? Basically, I was not able to get the test_paragraph_extra_b5 function to pass before the end of the previous week, so I marked it with @pytest.mark.skip to disable the test until I could look at it. Not wanting to leave things undone, this was the number one thing on my list for this week. Looking at the Markdown document and the HTML output, everything looked fine. It was when I looked at the tokens and saw the issue that it came flooding back to me: there was a problem with the newlines. Specifically, there was a problem with the Text token following the test document's Image token: \"[text(3,19): \\n def:: \\n\\n ]\" , If things are working properly, the number of newline characters in the text field is equal to the number of new characters in the extracted field. In this case, the text field contains one newline character, but the extracted whitespace contains two newline characters. And once I saw that pattern, I was able to see it in other similar tests. In total, there were 16 Paragraph token tests and 12 SetExt Heading token tests that exhibited this behavior. The key was that in each case, the Text token followed an Image token and preceded the end of block token. Digging In Now I had a pattern, so it was time to fix it. I started by turning the debug log on and working my way through the data for the test. There was a substantial amount of data, but using my experience on the project, I was able to quickly narrow the search to something in the inline processing section. Taking a look at the tokens produced right before the inline processing started, everything looked right. I then decided to focus on the inline processing based on that information. Digging into the Inline Processor, I followed the processing along token by token, right up to and including the Image token and everything looked fine. Even the processing for the Text token itself looked fine if it was not for that extra newline character in the extracted whitespace field. To narrow down where that character was being added, I started with some debug to log the tokens at various stages of the processing, and the data confirmed my previous statement. From a token point of view, everything looked perfect. To me, that just left the tidying up after the tokens. Because of the way the parser is designed, the inline text is accumulated until an inline token is required. Once that token is created, the parser then grabs all the characters up to the start of that token, creates a Text token with that information, and adds it before adding the new token. I stared at the code that created the token, and wondered what the problem could be. inline_blocks . append ( TextMarkdownToken ( current_string , starting_whitespace , end_whitespace = end_string , line_number = last_line_number , column_number = last_column_number , ) ) It was on a hunch more than anything else that I decided to look at the end_string variable. This variable is where the extra whitespace stripped from the end of the line is stored. Looking at this variable's impact on the logs, I noticed a couple of \"holes\" that I filled with extra debug log statements. It was with that information that I was able to pinpoint that along the way to the creation of that Text token, the extra newline character was appearing. Tracking backwards, it soon became clear that the issue was that the end_string was being assigned that newline character, but it was not been cleared properly. Fixing the Issue After about 45 minutes of additional debugging and fiddling, the answer final came to me. When the inline_response.consume_rest_of_line field was set, four different fields and variables were being reset. The thinking here was that if the different handlers indicated that they had consumed the rest of the data on the line, there was not anything else to do. But one thing was forgotten: the resetting of the end_string variable. Sometimes a fix is rewriting some logic, and sometimes a fix is adding some new logic to handle a missed case. This time, the fix was just about completing the reset action. Adding a single line in the if inline_response.consume_rest_of_line: block was all that was needed: end_string = None Anticlimactic that it was only resetting one variable on one line, but that is the way it sometimes is. Lots of looking only to find out something small was missed. With the research I completed, it did make sense though. Without that variable being reset to None , the newline character from a previous line was being carried through to the Text token after the Image token. Once I verified that the fix worked with that one test, I ran the entire suite of tests with the new code in place. Comparing the test failures to the set of 16 Paragraph token tests and 12 SetExt Heading token tests, there was a 100% match! After approximately 3 hours of work, I had found that small but required change! Being More Precise The first part of this task was to perform some catch-up work on code coverage. Having forgotten to check the coverage with recent changes, I was happy that there were only two small areas that I needed to improve on. Even better, both were cases where I had added defensive code that was not being utilized. As such, removing that code from the __consume_text_for_image_alt_text function and the __complete_inline_block_processing function returned the code coverage numbers to their normal levels. With that task under my belt, I decided to address an issue that has been bugging me for a couple of weeks: the rehydrate_index assert. Located in the __handle_last_token_text function, I have improved this assert over the last few weeks. While it started off as a simple \"less than\" expression, it was now down to a comparison between the count of newlines in the tokens and the rehydrate_index field. My problem with it? As improved as it was, it was still checking to see if the rehydrate_index field was either equal to num_newlines or num_newlines + 1 . As documented in past articles, I have incrementally improved this check to the point where it was when I started this work. But the real target was always to get that assert statement down to a one-part expression. Specifically, based on its usage, the goal was to get the statement down to: assert last_block_token . rehydrate_index == ( num_newlines + 1 ) Debugging Starting the work on this task, the first thing I did was to figure out the amount of work I needed to do. In this case, the best way to do this was to make the change detailed at the end of the last section and look for failures. While it was not research in the typical way that I do it, it was useful. It pointed out that all the scenario test expect for approximately 15 scenario tests were passing. This was useful for both marking a starting point and for my confidence. As I started digging into the failures, those tests started to separate themselves into two distinct groups: those with code spans including newline characters and Text tokens occurring at the end of Paragraph blocks. While the work took a long time, in both cases the process was the same. I started by isolating one test in the group I was looking at, then looked at the consistency check output to find a pattern that made sense. Then I just kept on drilling and adding debug where needed until the pattern became visible to me. Now, as I have been working on this project for a while, the \"until a pattern became visible\" to me is guided by experience and enhanced by luck, and both just take time. After a while, the pattern that leapt out at me for the code span tests was simple: the line number was being updated properly, but the rehydrate_index field was not being updated. Adding some extra debug, I was able to validate that quickly, then adding the follow code to remedy that problem: if last_token . token_name == MarkdownToken . token_paragraph and not link_stack : last_token . rehydrate_index += num_columns Having found and fixed that issue, the second group was then easier to see. Seeing as a not updated rehydrate_index field was the issue with the problem, I gave it a short with this one, and it worked here as well! Once again, a quick fix solved the issue. Executing the tests with the new changes in place, the consistency checks were now testing against my intended target of: assert last_block_token . rehydrate_index == ( num_newlines + 1 ) Paragraphs Ending with Multiline Non-text Inlines That title is a mouthful, but accurate. The next issue that I tackled was: - repeat __handle_last_token_text with paragraphs ending with other inlines I was not sure if this was going to be a problem, but I can see why I added this to the issues list. Most of the existing tests were focused on a specific Markdown element and either ended with that element or text following that element. Even harder to find were tests where the element at the end of the block contained a newline character in the middle of it. So, while I found an isolated scenario test here and there that tackled some of these combinations, there was not a good solid \"here it is\" block of tests. Time to change that! To address this, I added test functions test_paragraph_extra_c7 to test_paragraph_extra_d4 . Quite simply, I added 8 tests, 4 tests for links and 4 tests for images. Within each group of 4 tests, I added a split Text element, a split Code Span element, a split Raw Html element, and a split Emphasis element. It was not rocket science, but it was good to have all those tests in one place, ensuring that I was addressing that concern. Even better? They all worked on the first try. Expanding Paragraph Tests to SetExt Heading Tests With a metaphorical bounce in my step after addressing the last issue, I decided to tackle the copying and transformation of Paragraph tests to SetExt Heading tests. While not officially on the issues list, my goal was to keep the extra Paragraph tests synced up with their cousins, the extra SetExt Heading tests. I am not sure if the right term is cousins, but that is how I thought of them, as the only difference between the two groups of tests was their parent blocks. The contents of the tests remaining the same, except for the addition of a SetExt Heading block terminator. As I knew that these tests were based off existing Paragraph tests, it was pretty simple to go through the new tests and adjust them to use a SetExt Heading token instead of a Paragraph heading token. While simple, even the creation of copied test functions test_setext_headings_extra_a3 to test_setext_headings_extra_d4 took some time to get right. But once again, I was rewarded for my hard work with tests that all passed, with no failures. Are Links Being Verified Properly? This task was not to solve the following issue, but to research it: - are links getting verified properly in checks? images are, cannot find link code for same Based on previous tasks, I have done extensive work on making sure that any image links were verified properly. However, I have previously noticed that the same rigor had not been applied to the Link token. While I did not want to solve the issue right away, I wanted to make sure I had the right information to resolve it later. And it did not take me long to finally figure out why this issue that had been bothering me for a while. The big issue here was that I was missing validation in cases where an end token was present. Specifically, while the end Emphasis token has a line/column number attached to it, the end Link token does not. That meant that when the previous token validation was occurring, the existing code was doing the equivalent of a digital shrug and letting it go. That was the real crux of the problem. Additionally, the Image token was of interest. Doing a double check of my findings, it became immediately obvious why it escaped the same fate: it is a whole token. Whereas the Link token has a start token, the inner text, and then an end token, the Image token is entirely self-contained. As such, everything is processed at one time, and no end token handling is required, thus the processing for an empty line/column number was avoided. It was clear to me that solving this would require some serious planning and thinking. Adding Proper Link Verification Given the research from the last section, I decided to take the time needed to tackle that issue properly. Dedicating a minimum of 4 hours to this task, I sat down and started to work the problem. To keep things simple, I am going to keep track of the pseudo-code for this solution instead of the actual code. While it is true that the code needs some major refactoring to occur, the more important reason is that it is just very verbose. The big reason for me to relay this information is that I found this useful during my development of the solution. While the algorithm at its base components is simple, I just found keeping it in my head in a clear and concise form was too much. Step 1 The first issue was that I needed to have an anchor to base the consistency check's line/column numbers from. As the end Link token does not have a line/column number associated with it, I first needed to add some code to look backwards in the token stream for the first valid line/column number. While I was initially worried about possible interactions between the end Emphasis token and the end Link token, I eventually determined that exploring that path was a dead end. Because the end Emphasis token has a line/column number, it can serve as an anchor token just as well as any other token. - search backwards from the current token for the last token to have a valid line / column number Step 2 With that anchor in place, it was then possible to apply the translations to the line/column number. This was mostly due to how the link is constructed. In the case of a simple inline link such as [link](/url) , the [ is processed, then the link label text link , and then the rest of the link text. When the end Link token is encountered, the remaining text ](/url) needs to be accounted for. Starting with the code for the handling of the Image token, I was able to quickly build up a similar block of code that provided the translations for the line/column number. As I had a pattern to work off of, this work went by fairly quickly. - search backwards from the current token for the last token to have a valid line / column number - apply the translations for the line / column number due to the rest of the Link token Step 3 With that work in place, I then went to handle the previous token in the normal case and hit an issue. The anchor line/column number did not seem to be correctly calculated. I double checked my code, and everything looked fine. Doing some deep digging, I started to understand why. When I figured out which token to use as the anchor, I did not take into account any changes introduced to the line/column number by the anchor token itself. Basically, if I was using a Text token containing text at (2,7) as an anchor, I was using that (2,7) as the starting point, ignoring any changes introduced by its contents. Instead, I needed to use (2,11) as the starting point, taking the length of the token's original Markdown into account. The saving grace here was that immediate calculations were okay, so I just added the code following the previously added code. While it does not affect the results, in hindsight I might want to move the translation code for the anchor token up to make sure that the code logically flows better. - search backwards from the current token for the last token to have a valid line / column number ( anchor token ) - apply the translations for the line / column number due to the rest of the Link token - apply the translations for the line / column number due to the anchor token Testing the Changes and Further Adjustments Having executed an isolated scenario test or two during these changes, I then went to execute those tests again, and the results were hopeful. From what I could see, the line/column number calculations were working properly, but another assert was getting fired. After a quick check, I came to an interesting observation: while looking at the line/column numbers and making sure they were being adjusted properly, I totally forgot about the rehydrate_index field. Most of the adjusting went off without a hitch, but a couple of outlying tests were still failing with the assert for the rehydrate_index field. It took me a good couple of hours of adding debug statements and scanning the consistency check output before I found the pattern. While I had just added code to adjust the rehydrate_index field to account for the Link token, I had failed to adjust the field to account for the anchor token. As such, if the token before the end Link token (the anchor token) contained one or more newline characters, those newline characters were missed. Fixing that problem took a bit of creativity. I already had the code to properly calculate the proper values, but they were in the __process_previous_token function. The problem with reusing that function was that it modifies the rehydrate_index field as a side effect, something I was not sure that I wanted to happen. In the end, I worked around this problem by doing a sample call to the __process_previous_token function and getting the change in the line number, then invoking that function again on the anchor token, applying the proper change and restoring the rehydrate_index field. It took a while to figure that out and get it working, but it was worth it. With that code in place, I ran those tests again, and the consistency checks for the small sample of tests I was using passed. Crossing my fingers, I ran it for the entire set of scenario tests and was happy to see that they all had passed. While it took quite the while to accomplish, it was good to finally put that issue to rest. Needing Some Extra Emphasis While the description for this issue was: - add verification for links with the endlink as the last element - add verification for links+emphasis as the last 2 inline elements I felt that it really did not describe the problem properly. Added when addressing the previous issue, what I believe I was trying to note is that I wanted more testing with the interaction with the end Link token and the end Emphasis token. At the time when I added that note, I remember thinking that there might be some weird interaction between the two end tokens. It was only later that I remembered that the end Emphasis token was the rare end token that has a line/column number attached to it. Still, it made sense to add some extra tests for emphasis and interactions with the link tokens, as I did not believe I had good coverage of that so far. To remedy that, I added test functions test_paragraph_extra_d7 to test_paragraph_extra_e0 , with emphasis around the the link and image elements followed by emphasis within the link label of the link and image elements. These tests were not at all complicated to add and I am also not sure they are actually needed. However, as I perceived testing in that area to be a bit thin, adding those tests just seems to me to be a good idea. And luckily, after running those new tests, everything passed without incident. Just Before the End I remember looking at the clock and noticing that I had around 20 minutes left before I started writing that week's article at noon. While I used to start writing first thing Sunday morning, as of recently I have tried to keep my notes in a way that I could more easily mine them for the articles. Still in the proving stage, it has helped me out with the writing process, allowing me to find my flow more easily on Sunday at noon when I now start writing. Anyhow, with somewhere near 20 minutes left before noon on a Sunday and me wanting to milk the clock for every minute that I could, I quickly looked at the issues list. I was hopeful that I could find some low-hanging-fruit to work on in the time remaining, and quickly came across this little gem: - code span - multiple lengths of ticks, whitespace I felt that this was a good candidate, but I still needed to investigate it. At the very least, I could get the research done and have it ready for next week. The Research After some quick research, I found out that while the scenario tests between (and including) the test_code_spans_338 function and the test_code_spans_359 function has some interesting variations, I felt that they were missing some basic coverage. Looking at those tests, it was pretty evident that tests with variations on the number of backticks and the amount of whitespace were in the minority. Doing some quick scans for the number of backticks in the tests, my suspicions were confirmed with only 4 tests dealing with double backticks and no tests dealing with more than two backticks. As for whitespace, the tally was similar: 5 tests with one whitespace character and 1 test with 2 whitespace characters. Fixing the Issue With that research in hand, I could either document the issue more thoroughly, or fix the issue. Looking back at the description of the issue, I inferred that what I was really looking for was a centralized place where those tests kept, not having to look for them all over the place. If that were the case, I figured I could solve the issue by adding 3 new tests, so I decided to fix the issue. The 3 new tests? I started with the text aa`aa`aa , adding one backtick and one whitespace in critical areas to get aa `` aa `` aa and then finally aa ``` aa ``` aa . I just kept it simple and created 3 new tests, each one clearly handling an increasing number of backticks and whitespace. A quick execution of the new scenario tests, and… no issues. Assuming my inference was correct, problem solved! What Was My Experience So Far? While there were only a couple of issues that made me really think about how to solve them, this was a good week for just getting some issues taken care of. At this point in the project, I am confident that I have 95 percent of the code working properly. My hope is that by adding new tests and making sure that existing tests are grouped properly, I can find those outlier scenarios that I have not thought of. Whether those scenarios end up working or failing is something I cannot control. But I can strive to find as many of those scenarios as possible. And I have noticed that this hope is being largely reflected in the work I am doing. Less and less work is in the project's parser itself, with more changes being attributed to improving the verification of the parser. From where I sit, that is a good place to be in. The other observation? In times when I do need to change the parser, those changes are usually small changes of five lines or less. That statistic alone fills me with confidence. Now, while I do want to find as many scenarios as possible, I need to balance that with trying to wrap up the initial phase of this project so I can use it and get the project out there. For me, that means I need to start buckling down and shifting my focus away from issues in the \"Nice to Have\" category and work on the other issues. Here's hoping I can action on that! What is Next? From my viewpoint, next week's article will be interesting as I talk about my efforts to group the tests together more concisely, with documentation to back up the reasons for those tests.","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/11/02/markdown-linter-delving-into-the-issues-6/","loc":"https://jackdewinter.github.io/2020/11/02/markdown-linter-delving-into-the-issues-6/"},{"title":"Markdown Linter - Delving Into the Issues - 5","text":"Summary In my last article , I started to add extra scenario tests that tested newline characters in various groups or themes. In this article, I continue working towards having a good level of group coverage for each of those groups. Introduction To be totally honest with any readers, the work that I did for this article was just more of the same stuff. Basically, look at the issues list , find something to work on, and work on it. However, as of late, I have been trying to find groups of issues to work on, instead of one-off items. As such, I was looking for issues that could help me resolve questions about a group of behavior for the project, not just a very specific behavior. And to continue to be honest, this work was done during a difficult week for me. With things to do around the house, I did not seem to get any really good stretches of time to work on the project until later at night when I was tired. It was just one of those weeks. But even though I was tired, I wanted to continue. While I would have been ecstatic to continue working with great velocity, what I needed to do was to maintain forward momentum. That meant picking items from the issues list that I could work on in phases, keeping that momentum up from phase to phase. That is why I wanted to focus on items I could deal with as a group. It was a good way to keep working, while not feeling I was forever stopping. For me, it was not about doing the work as much as maintaining the feeling that the work was progressing. That is what was important to me. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits 07 Oct 2020 and 11 Oct 2020 . Starting With Some Cleanup As usual, there were some issues left over from the previous week that I needed to take care of. While not pivotal to my efforts going forward, I spent a brief amount of time going through the Bugs - General - Uncategorized section of the issues list, making sure to remove anything that had already been done. This effort was mostly done for reasons of cleanliness and accuracy, though I will admit that getting rid of a handful of items that had been completed was good for my confidence. In addition, during the prior week's testing, I had added functions test_fenced_code_blocks_099k and `test_fenced_code_blocks_099l to test out different counts of multiple blank lines within a fenced code block. As those tests passed and seemed useful, I decided to keep them in the test suite, but did not have an issue to tag them with. It just made sense to add them at this point, before taking them any further. Ensuring Consistent Paragraphs The work in this section all came from one line: - make sure line/column is tracking text indenting on each line Not a fantastically descriptive line, but it was there. And it was enough for me to understand what it is that I wanted to look at. In previous articles, I have talked about how paragraphs are the base building block of any Markdown document. Being the default block container for the default inline element, I would guess that an average of 75% of existing Markdown document blocks are Paragraph elements. I have no hard facts to back it up, but as I look at a representative sample of the articles I have worked on, a SWAG leads me to think that 75% is a fairly good estimate. This perceived predominance of Paragraph elements in Markdown documents influenced my perception when I was designing the token system used by the PyMarkdown project. To accommodate this perception, I decided to place any newline handling elements in the Paragraph token instead of the encapsulated inline tokens. At the time, my thoughts were that the Paragraph element had rules that were different enough from the 2 header elements, the 2 container elements, and the 2 code block elements, that I needed to capture the element's information differently. As Paragraph elements largely ignore leading space on a given line, the Paragraph token seemed to be the right place to store that information. While it has caused a couple of issues in the past, I still believe that it is still the right decision. The Issue While I do believe it was the right decision to make, that decision has added extra headaches to the project. The big headache is confirming that the newlines in the inline token correspond to newlines in the whitespace that is extracted and stored in the Paragraph token. That is where the rehydrate_index field comes in. I have talked about it in passing, especially in a previous post in the section titled Verifying the Rehydration Index . In that article, I talk about how I was not sure if there were any errors because I was not sure that the field was named correctly. While I concluded that the field was named correctly, that name can still be confusing at times. The rehydrate_index field indicates the index of the next newline that will need processing. As such, once the processing of any text at the start of a line is done within the bound of a Paragraph token, that field is updated to 1 . This offset also means that when all processing for the text within the Paragraph token has been completed, the index should be set to the number of newline characters plus 1 . To move towards consistency of this field's value, I added the following code to the end of the processing of the end of a Paragraph block: num_newlines = ParserHelper . count_newlines_in_text ( last_block_token . extracted_whitespace ) if last_block_token . rehydrate_index > 1 : assert ( last_block_token . rehydrate_index == num_newlines or last_block_token . rehydrate_index == ( num_newlines + 1 ) ), ( \"rehydrate_index (\" + str ( last_block_token . rehydrate_index ) + \") != num_newlines(\" + str ( num_newlines ) + \")\" ) Basically, if the algorithm is at the end of a Paragraph block, any inline elements contained within the paragraph should have moved the rehydrate_index field to its proper value. However, I needed to add an alternate conditional to handle the case where it was 1 count short. While that was concerning, it was more concerning that there were some cases where the rehydrate_index field even fell short of that adjusted mark. I felt it was imperative to get those outliers addressed first. Addressing the Failures In debugging, it is not often that an answer almost literally jumps out and screams \"Here I am, please fix me!\" In this case, when I executed the failing scenario tests and looked at their failures, it was an easy observation to make. For some reason, the Text token after a Hard-Line Break token included a newline character in its text section, but did not include a newline character in its whitespace section. As a result of that mismatch, the tests failed as the Paragraph token's rehydrate_index field was not set to the correct value when verifying the tokens. It took me a while of careful tracing through the logs, but I finally found that in the handling of the end of the line for a Hard-Line Break token, it was not clearing the whitespace_to_add variable in both case. This code: whitespace_to_add = \"\" was being executed if the Hard-Line Break token was created by multiple space characters in the Markdown document. However, if the token was created by the backslash character at the end of the line, it was not. Making sure that code was in both branches solved some of those issues, but every test. There were still a handful of tests that failed. Covering My Bases While the tests for the SetExt Headings tokens were all passing, the discovery of the previous failures inspired me to add some similar tests for the SetExt Heading tokens. To do this, I started with the original scenario test, test_setext_headings_extra_22 : this was \\\\ --- From there, I added functions test_setext_headings_extra_22a to test_setext_headings_extra_22d to test the creation of a Hard-Line Break token followed with a Text token. To start, I added the first two functions that simply had both forms of creating a Hard-Line Break token with some simple text following it. In addition, to make sure that I was handling the next line's leading whitespace properly, I added two more variations that included a single space character at the start of the following line. While I am not sure how useful these four tests will be in the future, at the time they were important. As I had just fixed some issues with Paragraph tokens and extracted whitespace, I wanted to make sure that a similar format with SetExt Heading embedded text did not suffer from a similar problem. Continuing Forward With the experience from the last section in hand, I continued to look for the root cause of the remaining failed tests. As in the previous section, the tests were failing with a common theme: newlines that occurred within a Link token. While the solution to this issue was not as easy to arrive at as the solution for the last section, it was fairly simple to see that the problem had two parts: the tokens derived from the link's label, and the main body of the link itself. Almost as soon as I started looking at the logs, I noticed that the numbers were off, and I had to dig in a bit deeper. Digging Deeper The way inline links are constructed is as follows: [ link ] ( / uri \"title\" ) However, the token generation algorithms follow the way that the tokens are used to generate the HTML output, which for that Markdown is: < p >< a href = \"/uri\" title = \"title\" > link </ a ></ p > As the link label (the text link in the above sample) may contain any form of inline text except for another link, that information cannot be contained in its processed form within the Link token. Instead, there is a start Link token, followed by the processed version of the link label, followed up by the end Link token. From PyMarkdown's point of view, the token stream for the above Markdown document is: \"[para(1,1):]\", '[link(1,1):inline:/uri:title::::link:False:\":: :]', \"[text(1,2):link:]\", \"[end-link:::False]\", \"[end-para:::True]\", Unless the Link token contained a newline character somewhere within its bounds, everything was working properly, and the consistency checks were properly verifying the tokens. But when the Link tokens contained a newline character, those properly working algorithms were not working so well. Finding A Solution As I knew that this problem had two parts, I figured that the solution needed to have two parts are well. I believe that my realization of that conclusion is what enabled me to shortcut other solutions that did not work in favor of a split solution that did work. The first half of the solution needed to deal with the text contained within the link label. While this text is handled in the tokens between the start end end Link tokens, from a Markdown point of view, it occurs right after the opening [ character of the link. The good news here is that adjusting the rehydrate_index field for each enclosed token was easily done by adding some simple code at the end of the processing loop in the __verify_inline function. The second half of the solution was in the handling the Link token itself. As the inside inline tokens come first in the Markdown document, it made sense to handle the Link part of the solution when the end Link token is processed. This meant adding some extra code to the __verify_next_inline function, processing the end Link token at the top of the function if the current_line_token line/column numbers are both 0 . If the current_line_token variable is a end Link token and the code is processing within a Paragraph token, I added new functionality to calculate the number of newline characters encountered within the Link token itself. Running through this code in my head, and solving some simple issues, I executed the failing tests again, and was pleased to find that they were all passing. In that version of the code, I had four different sections, one for each type of link. However, after a quick examination of the code, I believed that the code could easily be parsed down to one block of code that just gathered the newline counts from each part. Eliminating all the other blocks except for the inline block, I was happy to find out that my guess was correct. The way I had set up the values for the other types of links allowed for a more simplified version of the code. After a quick check of that guess locally, a full scenario test run confirmed that there were no ill effects of these changes, I cleaned the code up and checked in that work for the first part of the week. Interlude It was after this point in the week where it became more difficult to find numerous good blocks of time to work on the project. The first day it happened it was not so bad, but the struggle to find good time to work on the project was draining. I knew I had to keep my priorities focused on the other tasks in my life, as they had priority. But even so, it was just hard not to get a good block of work done on the project. But I persevered. Inlines and New Lines The second half of the week was spent working on clearing up a related item from the issues list: - need other multiline elements in label and verify Piggybacking off the previous item, this seemed like a really good choice to pick off the issues list. While the previous work dealt with plain text inside of the link label, this issue took that work and improved on it. Instead of just simple text elements, this work adds testing support for Raw Html tokens and Code Span tokens that span multiple lines. Starting with Tests The start of this work was easy. I added scenario test functions test_paragraph_extra_a3 to test_paragraph_extra_c6 to cover all the new scenarios. Starting with simple tests, the first three tests that I specified were a normal link label [li\\nnk] , a code span [li`de\\nfg`nk] , and a raw html [li<de\\nfg>nk] . Moving on to variations of those tests, having completed those tests for the inline link type, I transitioning to examples for the full link type, the collapsed link type, and the shortcut link type. Once those variations were done, I copied each of those tests, replacing the start Link element character [ with the Image element character ![ . Work to setup the tests was also easy, but pedantic. After a quick survey of the inline element types, I was shocked to find out that only the three inline tokens named above allow for newlines with their elements. It did make the creation of the tests easier though, so I was happy to find that out. And maybe it was just my experience on the project, but the test setup went smoothly. I did the research, I figured out what I needed to test, and then cycled through the variations with speed and accuracy. Just felt good to have a solid grasp on the problems. Starting with the Inline Processor From a link point of view, the tests were generating their tokens successfully. The Text tokens and Code Span tokens were being handled properly, passing all their tests. However, when the Raw Html token tests were executed, the text text was being added to the token stream instead of the Raw Html token. Having fixed an issue in this area before, I had a feeling it was in the __collect_text_from_blocks function, where link tokens are translated back into their source text. After taking a quick look there, it was somewhat funny to see that I had handled the Code Span token case, but not the Raw Html token case. A quick fix, followed by another run over those tests, and the Link token tests that were failing started working again. That left the failing tests that dealt with Image tokens. While this took a bit more work, it was roughly the same process as with the Link token. In this case, the text for the alt parameter of the image tag is created by consuming the elements generated from the Image token's label field. This work is done by the __consume_text_for_image_alt_text function, consuming the tokens as the processing is done. The main difference with this process from the link process is that, according to the GFM specification, this text is preserved largely without any of the special characters. As such, I had to check the proper translation of each of the inline tokens against BabelMark to make sure that I had the right translation. But other than that extra bump in the process, it went smoothly. The __consume_text_for_image_alt_text function filled out quickly with each coded translation. With that task completed, the scenario tests were generating the HTML output that I expected. Onwards! Rehydrating the Markdown Text After I was sure that the tokens being generated correctly, I quickly ran each of the Markdown documents from the new tests through Babel Mark , verifying that the output HTML was correct. Encountering no problems, I moved on to the rehydration of those tokens into Markdown and was happy with the results. With only a couple of tests failing, I took a quick look at the failures and noticed the problem right away: rehydrating Link tokens and Image tokens that contained newlines was not working. Following the log files, I was able to quickly figure out that the problem was that the backslash escape sequences and replacement markers were not being resolved from the text before doing the final rehydration of the elements. In the end, the following lines: text_to_modify = ParserHelper . remove_backspaces_from_text ( text_to_modify ) text_to_modify = ParserHelper . resolve_replacement_markers_from_text ( text_to_modify ) were added to the __insert_leading_whitespace_at_newlines function to resolve those elements. With that code added, every scenario test was passing to the point of being able to regenerate its original Markdown. On to the consistency checks! Cleaning Up the Consistency Checks It was in this area that I spent a lot of time making sure things were correct. Due to the shortened time blocks in which I could work on the project, the solutions that I initially came up with were just not solid enough solutions to use. These solutions were either too complicated or failed to meet the criteria, leading me to throw each approach out. In the end, I just reset to use a simple approach, after which things started to work out fine. Learning from my previous work, I was pretty sure these changes were going to involve handling replacement markers and backslash escapes better. Specifically focusing on the link label and image label, I was able to quickly determine that the link labels and link text from the different link types in the __verify_next_inline_inline_image function needed to call the resolve_replacement_markers_from_text function to remove the replacement markers. After making that change, I followed a hunch to see if the other changes I made needed to be copied in some form to the consistency checks. I was rewarded to find positive benefit to extending the code under the if \"\\n\" in str(current_inline_token): condition of the __verify_inline function in a manner similar to the changes made to the __consume_text_for_image_alt_text function. It just made sense. What Was My Experience So Far? This was just a brutal week project-wise. The stop-and-go (but mostly stop) nature of this week reminded me of a time home when I worked in a cubicle farm for a year. Instead of people just looking over at you or emailing you, they would have to walk over to your cubicle and stand by your \"door\" to have a conversation with you. At least for me, it often seemed like I was just getting into a good work flow when someone else showed up to talk, causing another interruption. While I definitely had my priorities straight in dealing with the issues around my house, the stop-and-go nature of this week made it hard to get into a good flow. Even thinking about how I felt that week made the task of writing this article more difficult. It was just that jarring at times. That also made my current predicament that much more painful. I can see the issues list getting smaller and smaller, closer to a point where I know I will feel comfortable in releasing the project. And I want to get there, but I want to get there with what I consider to be solid quality. But I also want to get there soon. And I know that if I had more quality time during the week, I would have been able to resolve at least a couple more issues. But I am still happy with the momentum on the project, and where I am with it. And one of the promises that I made to myself at the start of the project is that I must have balance between this project and other priorities in my life. And this was just a week where I had to put my money where my mouth was. Hopefully next week will be better. 1 What is Next? Next week's article will be more interesting, as I was able to address my time allotments on the project, submitting 9 commits during that week. It was better. These articles track roughly 2-3 weeks behind the actual work, I know for a fact it got better. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/10/26/markdown-linter-delving-into-the-issues-5/","loc":"https://jackdewinter.github.io/2020/10/26/markdown-linter-delving-into-the-issues-5/"},{"title":"Markdown Linter - Delving Into the Issues - 4","text":"Summary In my last article , I started to add extra scenario tests that tested newline characters in various groups or themes. In this article, I continue working towards having a good level of group coverage for each of those groups. Introduction There are times during the development of a project where the tasks are all feature related, and then there are times where the tasks are all issue related. Instead of waiting for that second era to arrive, I instead elected to proactively invest extra time in trying to improve the scenario tests. While I know I will not find every issue, I want to make sure I do my best to throw as many combinations of elements against the wall, hoping none of those combinations will stick. 1 From my viewpoint, I am aware that this focus on quality is taking time and effort away from adding new rules to PyMarkdown. However, in my eyes, every minute spent on making sure the project is stable is worth it. Even with the knowledge that I will not catch every issue before it happens, I am confident that by being proactive and adding groups of tests, I will put the project in a great position to be a stable platform for linting Markdown documents. Even better, by focusing on groups of combinations instead of individual tests, I believe I can discover a large group of issues at the current stage, instead of in a bug report later. At the very least, that is my hope, and my confidence is increasing with each group I add. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits 02 Oct 2020 and 04 Oct 2020 . Cleaning Up After Last Week As the work I documented last week ran into the overtime territory, I decided at that time to disable those tests, with promises to work on them next week. As this is the next article, this is the next week. Time to get to work! With very few exceptions, the results of each scenario test have been checked at least three times. As such, I was very confident that I needed to update the consistency checks to deal with these changes, and not the results. As an ancillary task, since the code to calculate the line/column number was just introduced, I knew that there was a decent chance that I might find an issue or two that would need to be fixed. As the consistency checks were not passing, I was hoping that it was the checks that needed updating but prepared for changes in the parser itself. Only research would tell, so it was on to more research! Doing the Research Digging into the code for links, it seemed obvious that the issue was with the links and their column numbers was something simple. As I scanned more code, my confidence that it was something simple just increased. All the big pieces of code looked fine, but one thing was sticking out: replacements. Back when I was adding the Markdown transformer, I made sure that both the HTML-friendly and Markdown-friendly versions of the token's text were included in the Link Token. However, when I added the code to handle this into the consistency checks, I got confused. Part of this is my own fault, but the reason I was confused was due to the duality that exists in the tokens and the project. In inline text situations, replacement sequences are used to show both the before and after parts of the replacement. In link situations, a similar pattern is used, including both parts of the replacement in the token. However, in the link case, there are a couple of added wrinkles. The first wrinkle is that there are usually 2 different fields of the Link token that contain this information. Using the link_title field as an example, the processed version is stored in that field, while the raw text used to generate that field is kept in the pre_link_title field. To avoid useless duplication of fields, the pre_ version of the field is only used if they have a different value that their processed field. That is the second wrinkle. In processing that Link token information, it is common to see code likeL link_title = parent_cur_token . link_title if parent_cur_token . pre_link_title : link_title = parent_cur_token . pre_link_title Basically, set the link_title variable to the token's link_title field by default. But if the pre_link_title field is set, use that instead. Given that there are two different access patterns, sometimes I can get confused and use a process for dealing with the other access pattern. And that is where the third wrinkle shows up: dealing with link labels. In the case of link labels, the processed version of the text exists between the start Link token and the end Link token, with the raw version being kept in the start Link token's text_from_blocks field. And to be honest, while it does make sense in how each of the 3 use cases are used in practice, remembering which one is in play when I am writing code on a given field can be difficult. Fixing the Issues Hopefully there is enough information in the previous section to allow for this revelation to be easy to see in advance: I used the wrong pattern in one place and the wrong field in another place. While the fix was just a couple of lines of code in the newly refactored __verify_next_inline_inline_image_inline function, it took some debugging to properly address the issue by using the correct patterns. With both patterns adjusted, most of the tests were passing. It was in those remaining failures that I noticed that another problem that was hiding behind that problem: character references were not updating the column number. With all the confusion of the other problem out of the way, there were a couple of outlier tests dealing with character entities that were not adding any space for the Code Span tokens. That was fixed by adding the following code to the end of the handle_character_reference function: inline_response . delta_line_number = 0 inline_response . delta_column_number = ( inline_response . new_index - inline_request . next_index ) Once again, it was obvious that something needed to be done once I started looking at the function itself, but without something causing me to look there, it was overlooked. I was just grateful that the extra scenario tests pointed this out! Rounding Out Image Support While the support for images and their line/column numbers was added a long time ago, the proper validation support in the consistency checks was only recently added. In addition, while previous work tested the inline image support, there was very little testing of the shortcut and compressed image types, and no testing of the full image type. Adding The Tests The testing part of this support was easy to address. As part of finishing the work from the previous week, I copied over tests test_paragraph_extra_78 to test_paragraph_extra_89 , renaming them test_paragraph_extra_90 to test_paragraph_extra_a2 . Once that was done, I changed the example text from the link start sequence [ to the image start sequence ![ . Temporarily disabling the consistency checks, I then ran through each of the scenario tests, adjusting the token output and the HTML output for the change from a Link token to an Image token. From the token point of view, I was primarily looking for the Image token to take the place of the start Link token. In addition, I was checking to ensure that any tokens after the start Link token but before (and including) the end Link token were removed. Finally, I adjusted the HTML for the scenario from an HTML anchor tag to an HTML image tag, along with any tag attribute changes that were required. At this point, it should go without saying that as I was making those changes, I ran through the token output and the HTML output multiple times. I did these checks both mentally and my tried and true method of \"counting out loud\", verifying that both values were the same. As I knew the next task was to properly implement the consistency checks for the other image types, I also knew that an extra count or two would take less time than trying to debug any off-by-one issues with any of the line/column numbers. It just made sense to take an extra minute per test and do that extra level of verification. Reviewing Collapsed and Shortcut Images Containing only a link label, the shortcut and collapsed image types had already been coded into the consistency check. From a quick visual inspection, they both made sense and looked great: elif previous_inline_token . label_type == \"collapsed\" : image_alt_text = previous_inline_token . image_alt_text if previous_inline_token . text_from_blocks : image_alt_text = previous_inline_token . text_from_blocks token_prefix = 1 newline_count = ParserHelper . count_newlines_in_text ( image_alt_text ) if newline_count : estimated_line_number += newline_count para_owner . rehydrate_index += newline_count estimated_column_number = 0 split_label_text = image_alt_text . split ( \" \\n \" ) image_alt_text = split_label_text [ - 1 ] token_prefix = 0 estimated_column_number += 2 estimated_column_number += 2 + token_prefix + len ( image_alt_text ) Using the collapsed link type as an example, the algorithm was really easy to code up. A Markdown collapsed link type looks like this: [reference][] . To start with, the code to determine the content of the link label is used, as was mentioned in the previous sections. With that information in hand, the token_prefix was set to 1 to account for the opening [ , before proceeding to deal with any newlines. Dealing with the newlines used the same pattern that I used for each part of the inline image type. However, the big change here is that the token_prefix is set to 0 if a newline character is found. The rationale behind this is that if there any newlines in the link label, the length of the opening [ is no longer useful in determining the column number, as it comes before the part that has a newline character in it. With all that preparation in place, it is then time to compute the change in the column number. The initial estimated_column_number += 2 takes care of the [] characters at the end of the image text. The 2 from the first part of the following statement combines a +1 for the translation from an index into a position and a +1 for the ] character after the link label. The token_prefix variable is then added to account for the length of the opening [ label, as described above, followed by adding the length of the raw version of the link label text. Why Review This? It may seem weird that I reviewed this before going forward but let me explain. I reviewed both because I knew that I had plans to use either the shortcut code or the collapsed code as a template for the full image type. I knew that the only difference in the code between those two types was the addition of a link reference. To me, it just made sense to start with the code for either of those image types and just add the extra code to handle the link reference. Following that plan to reuse that code, the support for the full image types was added within a couple of hours. After making sure the consistency checks were enabled, I started running the newly added scenario tests against the newly added consistency check code. It was then that I started noticing some weird failures. Debugging The Alt Attribute Digging into those failures, it was initially hard for me to figure out what the issue was. Everything looked fine until I started doing a character by character mental reconstruction of the tokens from the Markdown. It was then that I saw it: the alt attributes on a handful of the image tags were wrong. The scenario test function test_paragraph_extra_73 is a great example of that. With the Markdown text: a ! [ Fo & beta ; o ]( / uri \"testing\" ) a and a simple knowledge of Markdown, the text assigned to the alt attribute of the HTML image tag was obvious to me. It should be Foβo . But when I looked at the token itself, that token was: [image(1,2):inline:/uri:testing:Foo::::Foo:False:\":: :] That was close, but I was expecting a token that was more like: [image(1,2):inline:/uri:testing:Foβo::::Fo&beta;o:False:\":: :] which contained both the processed version of the text Foβo and the unprocessed version of the text Fo&beta;o . This was not like some previous issues that I had already resolved where one of the other inline tokens was not being represented properly. This was a case of some very simple replacements needing to take place but being missed. In addition, after looking at some other cases, backslash escape sequences were also causing issues, though usually hidden. Function test_paragraph_extra_74 is a good example where the output HTML was correct, but the tokenization contained an o for the processed text instead of Fo]o . Fixing The Issue To get around this issue, I was faced with three different choices. The first choice was to code something up specific to this issue. I immediately rejected that approach as I felt that one of the extensions that I will need to support in the future may have to introduce some base level of character manipulation like the character entities. As such, I wanted to leave my options open. That left the other two options, or rather one option with two variations. In either case, I already had a mechanism for registering inline handling and using those handlers. As such, my second choice was to use the existing inline processor code for text blocks. The issue that I had with that approach was that I would need to pass an additional flag into that function that would limit its use to only the backslash escapes and the character entities. While that may have been possible with a smaller function, the size and complexity of the __process_inline_text_block function gave me have concerns about possible side effects. From three choices down to one, I went with a simplified processor approach to the __process_inline_text_block function. When the handlers register themselves, I added a new flag to denote whether the handler was for one of the two simple inline sequences. If it was, I added the text sequence to the __valid_inline_simple_text_block_sequence_starts . Then, in the imaginatively named process_simple_inline function, I added the code for a very pared down version of the __process_inline_text_block function. This function was purposefully crafted to only handle those simple inline sequences and the newline character, nothing else. It was when I went to wire the function up that I found another interesting surprise waiting for me. Circular References Suck! With a decent first attempt at a simplified inline processor in place, I did as I normally do and went to the place where I needed the function, the __consume_text_for_image_alt_text function in the LinkHelper module, and added the reference and the import to go along with it. After starting the test executing, there was a long pause, and I was then greeted with a simple error telling me: E ImportError: cannot import name 'LinkHelper' from 'pymarkdown.link_helper' (C:\\old\\enlistments\\pymarkdown\\pymarkdown\\link_helper.py) Having hit these a number of times, I was sure it was a circular import reference, but where was it? It was then that I thought about it. The LinkHelper module was already imported from the InlineProcessor module, as I used it to handle all the heavy lifting with the links. As Python does not have any concept of forward references, all referenced classes or functions must be loaded before a reference is made to that item. In this case, with the relationship already established, I could not add the reference in the other direction. I needed to find another solution. I was sure that I would be able to come up with a more elegant solution at a later time, but I wanted to finish this task up, so I took the lazy approach of passing the function as an object. As it was passed as an argument to each function, there was no restriction imposed and I was able to use that method at the target __consume_text_for_image_alt_text function. I may want to look at a better way to do that in the future, but at the time, it worked. Rounding The Corner At that point, all the scenario tests were passing. I was sure that there were other scenarios that I could find and test, but I was confident that I had a good collection of paragraph tests added to the project. I knew in future weeks I would need to expand that collection in size and cover the SetExt Headings, but that could come later. Duplicating Tests Without Duplicating Effort With those scenario tests completed for links and images contained within paragraphs, it was time to extend those tests to including inline processing within SetExt Heading tokens. To that extent, I copied over test_paragraph_extra_43 to test_paragraph_extra_a2 , creating tests test_setext_headings_extra_43 to test_setext_headings_extra_a2 . In terms of the Markdown documents for each test, the only change I made was to add --- after the Markdown being tested, transforming it from a Paragraph token test to a SetExt Heading token test. As I expected, there were some issues that I needed to resolve to get those tests working. As the original versions of the tests used Paragraph Tokens, and the handling of those Paragraph tokens relied on the rehydrate_index field of the Paragraph token, that was an obvious change that needed to be made. To be specific, I needed to add code to each reference of the rehydrate_index field to only reference that field if the Paragraph token was the container for the inline text. For the consistency check and the Inline Processor, this meant protecting any use of the para_owner variable, and the owning_paragraph_token variable for the Markdown transformer. With that change made, the only other thing that needed to be dealt with is the change in how the leading whitespaces are handled within a Paragraph token and within a SetExt Heading token. To allow better processing of paragraphs, the Paragraph token collects any leading whitespace in the Paragraph token, whereas the processing of inline tokens within the SetExt Heading token stores that information with the inline tokens themselves. That handling was changed to ensure the information was properly being mined from each inline token. And while it probably took a couple of hours to resolve, it felt like these changes only took a couple of minutes. Compared to some of the other changes I have done in the last couple of weeks, these changes were completed quickly. I am sure that there were a couple of issues in the code that I needed to fix, but as I said, the time just flew by. What Was My Experience So Far? In the introduction, I mentioned my increase in confidence for the project, mostly due to my switch from individual scenario tests to scenario test groups. From a work point of view, instead of many small issues, I am lucky if I can get 2 to 3 medium or large sized issues done in a week. But as I am covering many combinations in one task, I feel that the benefit easily outweighs the cost. And that benefit is what helps me drive forward. And especially with today's climate, I find that I sometimes need some help in maintaining focus on this project and driving it forward. At times like that, when I need some extra focus, I find it is good to think about the positive parts of the project. Instead of focusing on what is left to do, I focus on the progress I have made in getting to this point. Instead of focusing on the quantity of tasks I can get done in a week, I focus on the quality that those tasks being to the project. And most importantly, instead of focusing on what I cannot change around me, I choose to focus on this PyMarkdown project, and the benefits I can provide to Markdown authors with this project. So, as I move forward with the project, focusing on quality, I choose these areas to focus on. What is Next? Staying with the theme of testing blocks, I tackle another couple of testing blocks in the next article. Throwing Spaghetti . ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/10/19/markdown-linter-delving-into-the-issues-4/","loc":"https://jackdewinter.github.io/2020/10/19/markdown-linter-delving-into-the-issues-4/"},{"title":"Markdown Linter - Delving Into the Issues - 3","text":"Summary In my last article , I continued the long journey to remove items from the project's issues list. Still without a better title than the equivalent of \"chapter 3\", this article details more pruning of that list. Introduction While this process feels like just one of many stops on a long journey, I do find that I am finding this part of the project particularly satisfying. Sure, I am fixing issues that I have found along the way, but that is a good thing. I am not finding a lot of \"I need to redesign this from scratch\" issues, just a lot of \"wow, did I forget to…\" or \"wow, I didn't think to test the combination where…\" issues. And I find that observation both calming and a boost to my confidence. Is it taking up time? Sure. Would I like to get back to implementing and adding rules? Yup. But, with each issue that I resolve, my confidence that this is the right course of action increases. The project's collection of different Markdown documents and test data just keeps on growing. As I manually check each scenario test out against what is expected, verify the output HTML against the reference implementation, and have consistency checks in place, any new issues just give me that much more information that the project is working properly. Mind you, properly does not mean it is perfect, just that it is headed in the right direction. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits 21 Sep 2020 and 27 Sep 2020 . Small Cleanups and Large Aspirations The first commit in this week's work was an easy one. During the writing of the article on that week's work, I realized two things: I wasn't taking show_debug arguments out of tests after debugging, and test test_raw_html_632 needed a child test with a bit of change, creating test_raw_html_632a . Both changes were accomplished within 15 minutes, so they were good cleanup issues. But after that, I was left with an interesting problem to deal with: Newlines and various inline elements. There were a couple of issues that I wanted to deal with: - HTML and Fenced Blocks and better handling of capturing newlines to avoid counting token height - verify which inlines cannot contain newlines and verify with line/col - autolinks - raw_html To break these issues out a bit more, I want to make sure I convey the difference between the PyMarkdown project and a normal Markdown parser. As the project's focus is to be a Markdown linter, there are additional requirements imposed that are tested with every commit. Those requirements are that any element of interest, once translated into a token to represent it, must include the line number and column number of the original Markdown. While the examples provided by the GFM specification are great for testing a parser, they fall a bit short for testing a linter. To be clear, that is not a problem with the GFM specification. The section called About this document specifically starts with: This document attempts to specify Markdown syntax unambiguously. It contains many examples with side-by-side Markdown and HTML. The specification specifically deals with syntax and how a given Markdown document should be represented by HTML. Full stop. It is this project's additional requirement of being able to examine the tokens that are then translated to HTML that add that extra level of complexity. And that extra complexity comes with a cost. Changing That Cost-Benefit Ratio The cost of this complexity has not been large until this point. Hitting an issue where I thought needed some extra testing, I created a child test by adding a letter to the end of the parent scenario test and made the required variations to the child test for that situation. In cases where there were more variations, I introduced multiple child scenarios, each one testing something specific. This approach did not significantly increase the complexity of the tests, due to its focused nature. But with this week's work, it was different. Instead of having one test here to verify and one test there to verify, I wanted to make sure that all inline tokens that followed a given pattern were being handled properly. I wanted to have one central location where I could look to make sure inline tokens and newline characters were being handled properly. This was different in that its scope was going to make it complex. New Tests - First Batch The first batch of these new scenario tests were the group from test_paragraph_extra_43 to test_paragraph_extra_46 . To reiterate the message from the previous section, the purpose of these tests was not to provide one off tests for each type of inline element, but to provide a group of tests along a given theme. In this case, that theme was an inline element surrounded by simple text on either side, with a newline in the middle of the element. Far from being tricky, adding these tests was simple. I started with test test_paragraph_extra_43 and support for the Code Span token, adding support for the Raw Html token, the URI Autolink token, and the email Autolink token, ending at test_paragraph_extra_46 Those were easy tests to add. In each case, it was just a simple Markdown document like: a ` code span ` a It was when I started looking at the links and images that I realized that I was in for a world of hurt. Well, maybe not hurt, but it was going to be a lot of work. New Tests - Links and Newlines Starting off with the links, I began with inline links, as they are what I mostly use in my documents. In terms of changeable parts, the inline links have 6 elements: the link label, the whitespace before the URI, the URI, the whitespace before the title, the title, and the whitespace after the title. To test these parts, I added tests test_paragraph_extra_47 to test_paragraph_extra_52 , one for each of the parts. In addition, for each whitespace test, I added variations on the tests that included whitespace before the newline, whitespace after the newline, and whitespace before and after the newline. Once those tests were added, it was time to focus on the other 3 link types. The full link type was tested by adding a test for newlines in the label ( test_paragraph_extra_53 ) and newlines in the link reference ( test_paragraph_extra_54 ). A new test was added for a shortcut link with a newline in the link reference ( test_paragraph_extra_55 ) and a collapsed link with a newline in the link reference ( test_paragraph_extra_56 ). Finally, to address some concerns I had about a newline at the start of those elements, I added test test_paragraph_extra_57 that includes a collapsed link with a newline at the start of the link label, and test test_paragraph_extra_58 that includes a full link with a newline at the start of the link reference. With that group of tests added, it was time to shift to the next group of tests: images. After adding test test_paragraph_extra_59 to place a newline between the ! character and the [ character of the image link, tests test_paragraph_extra_60 to test_paragraph_extra_67 were added as versions of tests test_paragraph_extra_47 to test_paragraph_extra_52 , modified for an image instead of a normal link. In addition, tests test_paragraph_extra_60 and test_paragraph_extra_65 were added to test an image link without any link title being provided. To me, this was a good start to some solid tests. I was focusing on a particular group of tests, and make sure that pattern was cleanly covered. Adding in Replacements and Backslashes Finding that the pattern of adding new tests in the previous section was working well as a template, the next group of tests followed a similar pattern. In this group of tests, I wanted to focus on newlines as part of replacement sequences and backslash sequences within different parts of the inline links. My concern was that the special character handling would play havoc with the determination of the column number for the token following the token with the special character. It just seemed to be a good idea to test these thoroughly. Tests test_paragraph_extra_68 and test_paragraph_extra_69 provide for testing in the inline link label, with a newline around a replacement sequence and a newline near a backslash sequence. Then test test_paragraph_extra_70 tests a inline URI containing a space, followed by tests test_paragraph_extra_71 and test_paragraph_extra_72 testing for newlines within a replacement sequence and a backslash sequence. Then tests test_paragraph_extra_73 to test_paragraph_extra_77 repeat those tests, but with image links instead of normal links. To round out this testing tests test_paragraph_extra_78 to test_paragraph_extra_89 repeat the link testing with the variations based on the other 3 link types. As I was compiling the list of tests for this article, I did notice that it appears that I have some tests that are done more than once. To combat this, I added a new item to the issues list to create a table or spreadsheet to more accurately track these variations. Addressing the Issues All in all, things went well with adding the new tests. For the initial group of 4 tests, the only thing that the tests uncovered was that the Code Span token was not handling the newline properly. That was quickly resolved with a couple of small changes in the handle_inline_backtick function. The bigger issues arose in the handling of the links. While not a design change, most of that section either needed to be added or enhanced to properly handle links with newlines. The way the code was implemented before that week's work, if there was a newline character in the current token, the logic was invoked to properly determine what the line/column number should be. There were two problems with that code. The first is that it was only setup to handle newline characters in an inline link. This meant that the code to handle full links, collapsed links, and shortcut links needed to be added. The second issue was that the initial code added to handle the inline links was only partially complete. In trying to figure out why that was for this article, my notes refer to an extreme lack of examples in the GFM specification that have newline characters in any parts of the Link elements themselves. Part of the push to add this current series of tests was to try and shore up that deficiency, adding those tests myself. Breaking Things Down As it is the link type that I use most often, I started with the inline links and their component length arrays. This array, in a variable named link_part_lengths , contained all 5 lengths of the variable parts of the inline links: pre-URI whitespace, URI, pre-title whitespace, title, and post-title whitespace. The link labels were already handled before reaching that point in the code, so there were no issues there. That code started off with some initialization of the array and its values: link_part_lengths = [ 0 ] * 5 link_part_lengths [ 0 ] = len ( active_link_uri ) + len ( current_token . before_title_whitespace ) if current_token . inline_title_bounding_character : link_part_lengths [ 1 ] = 1 link_part_lengths [ 2 ] = len ( active_link_title ) + 1 link_part_lengths [ 3 ] = len ( current_token . after_title_whitespace ) link_part_lengths [ 4 ] = 0 These values were not always going to be the right values, but they proved to be a good place to start for each link part. Basically, I started with an array of 5 integer values, each set to reflect a certain section of the inline link. While those values are not a direct 1-to-1 mapping of inline link part to length of that part, they are keyed to handle a newline character showing up in any of those inline link parts. A good example for this is handling a newline character in the inline link title part. newline_count = ParserHelper . count_newlines_in_text ( active_link_title ) if newline_count : delta_line += newline_count para_owner . rehydrate_index += newline_count split_active_link_title = active_link_title . split ( \" \\n \" ) link_part_lengths [ 2 ] = len ( split_active_link_title [ - 1 ]) + 1 link_part_index = 2 For the title part, if there are any newlines discovered, the count of newlines found is added to the relevant variables. Once that is done, the title is split on newline characters, and the length of the entire part is set to the length of the last part of that split. Finally, the link_part_index variable is set to the part itself. Finally, once all the parts are examined in this manner, the following code is executed: if link_part_index >= 0 : link_part_lengths [ 4 ] = len ( split_paragraph_lines [ para_owner . rehydrate_index ] ) link_part_lengths [: link_part_index ] = [ 0 ] * link_part_index repeat_count = - ( 2 + sum ( link_part_lengths )) Essentially, this is the magic part of the algorithm that pulls everything together. At the start of the final processing, the last element in the array is set to include any whitespace prefix from the last line that was added. Once that is done, the elements before the last element containing a newline are set to 0. Basically, if we have a title part that has a newline, anything before that title is not useful in determining the column number. Finally, the algorithm then calculates the column number (in the repeat_count variable) by summing up each of the values in the link_part_lengths array. Show Me This in Practice Applying that algorithm to the following Markdown a [ Foo ] ( / uri \"test ing\" ) a Given this information, the array was initialized as follows: link_part_lengths[0] = 4 + 1 link_part_lengths[1] = 1 link_part_lengths[2] = 8 + 1 link_part_lengths[3] = 0 link_part_lengths[4] = 0 When the newline in the title is detected, the link_part_index variable is set to 2 and the 2nd element is reset to 4 : 3 for the length of ing and 1 as per the rest of the adjustment equation. Without any more newlines, when the final part of the algorithm is executed, no adjustments are made to the 4th element, any element before the 2nd element is zeroed out, and the sum of the remaining elements is then 4 . Following the final part of the algorithm, 2 is added to that value, ending with a value of 6 . In double checking the above text, the letter a on the second line does indeed start at column 6. Summary For me, this algorithm is very effective. It starts with good defaults, updating those values as each part is evaluated. As the column number cannot be determined until after the last part is evaluated, that evaluation is only done at the very end. And by using an array, the algorithm can use the sum function to add those elements up, instead of manual summation. It took me a while to get to it, but it is and accurate algorithm and an efficient one. Adjusting the Consistency Checks Like the changes that needed to be made to the Markdown parser, its consistency checks needed to change to test the new criteria. I initially wanted to just copy over the work I had done in setting the new line/column numbers with newlines, but after 15 milliseconds, I realized the fallacy of that approach. The problem was that because I knew of the approach I already used; it was hard to come up with another way to do it. In the end, I just started the code with a less optimal starting point for the inline links, growing it from there. Instead of a nice array that I could easily sum, I used a series of 5 boolean variables and 5 string variables that contained the content of each part of the link. While I knew this was almost the same approach, I had confidence that I was picking a path that was enough different that I wouldn't be tempted to look at the previous code base to figure things out. The approach did not have to be optimal, it just had to be independent from the original algorithm, at least for this phase of the project. After all that work, adding the shortcut links and collapsed links were simple, being simple sums of the component parts, both constant and variable. But that brought up things that I knew I was going to have to work on soon. First off, all this verification was happening in the __verify_next_inline_inline_image function. I made a note in the issues list to check out why, but there was not any verification present for the link tokens. Secondly, any support for validating the full link type was missing. After checking the entire list of tests, there were no image tests that were specified using a full link format. Noted, and continued. Finally, there were the existing calculations for the collapsed and shortcut tests. While there was something there, I was sure that those checks wouldn't handle any newline characters in any of their components. Once again, I noted an item in the issues list, and moved forward. Besides those missing test areas that I discovered, there were a handful of tests that I could not get working by the end of the week. While it may seem like the tests were only small variations on each other, it did take quite a bit of work to get them working to the extent I did. Not include child tests, there were 47 scenario tests that I added. While some of them worked right away, most of them required research and small changes to get them working. Any even with a good push to get all the work done, I needed to leave some of the tests unfinished. Leaving Something Undone It was a hard decision to document the cases that I had missed and leave them for later, but I thought it was an important thing to do. As it was, the commit was done on Sunday morning, already digging into my article writing time for that week. I had no clue if the remaining tests were going to take 5 minutes to resolve or 5 days. I needed to draw a line somewhere and put those tests into the bank for the following week. While I was not 100% sure about the groupings, I had a bit of information that I used to group them together into 4 groups. The first group was a set of 7 tests that contained a character entity and seemed to have their column number off by 6. The second group was a set of 2 tests that contained a backslash and seemed to have their column number off by 2. I was not sure about the 3 tests in the third group, but they seemed to fit together. Finally, I recognized that I had started work on image versions of some of the links, but I needed to make sure that each of the tests at 78 and over needed image versions. With that information added to the issues list for the following weeks' work, I ended the week with a good amount of work done. What Was My Experience So Far? One thing that I am learning about in this phase of the project is that I have wells of patience that I can tap into if need be. To be honest, it is surprising me that I have this ability. I usually want to forge ahead with things and make sure they get done to the 90 percent mark, and struggle with that last 10 percent. But this project is different. I am not sure if it is my self-imposed strictness on the project requirements or my personal investment into this project, but I just find I am a lot more focused on making this project a solid, well-tested application. I know I am not going to be able to catch every issue at the beginning, but I also know the cost of catching those issues once I release the application to the community. For that, I am prepared to make a good-faith effort to do a thorough job in testing, even if it delays the release of the project by a couple of months. In writing this article though, I am realizing how boring it may appear to others. That possible appearance was made clear to me as I worked on parts of this article. Yes, I added over 40 new scenario tests, debugged them, and got most of them working. And because they are in themes, it was harder to write something interesting about them. No more \"hey I caught this, and here is the interesting part\". It was just one or two themes, and then numerous variations on that theme. But boring as it may be, I believe it is important. Going forward, I want to have a cohesive collection of tests that I can use as a foundation to improve on. I firmly believe that this work, and the work that follows in the next couple of weeks, builds that testing foundation. What is Next? No surprise here, more issues to address, and still focusing on newline and links. Stay tuned!","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/10/12/markdown-linter-delving-into-the-issues-3/","loc":"https://jackdewinter.github.io/2020/10/12/markdown-linter-delving-into-the-issues-3/"},{"title":"Markdown Linter - Delving Into the Issues - 2","text":"Summary In my last article , I started to tackle a few of the items on the project's issues list. As the imaginative title for this article suggests, this article details more of the effort to reduce the items on the issues list. Introduction As I mentioned in the last article, this part of the project is not about anything stellar, just going through the issues list and removing one item at a time. Far from moving a mountain by itself, it is all about moving that mountain one pebble at a time. The work for this week was no different. It was just about dealing with 6 items from the issues list and either proving they are not an issue or fixing the item if it really is an issue. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 15 Sep 2020 and 20 Sep 2020 . Rounding Out Coverage While the entry for this issue: - each inline token surrounded by text was short, I knew that this issue was all about completing the code coverage on recently changed code. I was already aware that more than half of the cases in the consistency check __verify_first_inline_setext function had the default assert False code put there as a placeholder. In addition, as soon as I looked the rest of the code, it was obvious that I had not tested a lot of the inline sequences within an Atx Heading element or a SetExt Heading element. That entry was a reminder for me to circle back around and fix it! To accomplish that, I started by working with each inline element and its relevant position in a line of the Markdown document. Tests 1 to 11 dealt with each inline element at the start of the line with text after it, tests 12 to 20 dealt with each element with text on either side of it, and tests 21 to 31 dealt with each element at the end of the line with text before it. For good measure, I added tests 32 to 42 which were each inline element by itself on a line. Once that was accomplished for the simple case, the Paragraph element, I cloned those tests into 2 other sets of tests, one for Atx Heading elements and one for SetExt Heading elements. Once cloned, I introduce a small change for both elements, the Atx Heading tests starting with the sequence #{space} at the start of each line and the SetExt Heading tests having the sequence --- follow the test text on the next line. What Did I Find? Running the tests after completing those changes, I found two expected results and one unexpected result. The first expected result was that the new tests rounded out the __verify_first_inline_setext function by providing good examples for each of the inline elements, except for the Hard Line Break element. The Hard Line Break element has some special rules, so in most cases the setup caused the text to not be interpreted into a Hard Line Break token, and just placed in a normal Text token. The second expected result was that several of the __handle_last_token_ functions needed adjustment when evaluated within the bounds of a SetExt Heading element. During the creation of those functions, as evidenced by the work required in the __verify_first_inline_setext function, testing within a SetExt Heading element was almost entirely missed. While it was not difficult to address, the testing clearly showed that I needed to add the following code to each __handle_last_token_ function to compensate: if last_block_token . token_name == MarkdownToken . token_setext_heading : inline_height += 1 Finally, the unexpected result was that the normal Paragraph processing and the Atx Heading processing were fine, requiring no fine tuning. While I had hoped for that outcome, it was nice for it to happen. I had expected at least one or two small issues to show up, but none appeared. Fixing A Simple, Yet Wincing Issue Yes, I said a \"wincing\" issue. While there are more colorful names for these issues, I prefer the name \"wincing\" as I believe it is clear, descriptive, and obvious. Quite simply, these are a class of issues that are so simple that when I see them, I cannot help but wince that the error made it through any of my commit processes without being detected. In this case, it was a typo where I typed estiated_column_number instead of estimated_column_number , before I copied it into many places. Yeah, I winced. While I was at it, I also decided to also tackle the issue: - 52 e - make new case with different indent levels for each ` Like my reasons for doing the work in the previous section on Rounding Out Coverage , I just had doubts about whether I had missed anything with the scenario test 52e. With previous issues, I had seen a handful of false positives occur from a test having a constant number of indent spaces. I believe my feeling was that to truly test this scenario, I needed to add a test where the number of ident spaces varied from line to line. As such, I cloned scenario test 52e into 52f, altering the indents on the second and third line to 3 spaces and 1 space, respectively. What Did I Find? One of the amusing things that I found out is that I get really embarrassed by simple typos. Other than that, while I was right to question the indents from scenario test 52e, the newly added scenario test 52f confirmed that the code was handling the indents properly. Verifying the Rehydration Index In looking at this issue, I could easily tell beforehand that it was going to be a research issue: - why ? shouldn ' t each one be of the proper length? ``` if split_extracted_whitespace and last_token . rehydrate_index < len ( split_extracted_whitespace ) : ``` Introduced when I was working on the Markdown transformer, the rehydrate_index field of the Paragraph token is not serialized with the other fields in the token. It is used exclusively for tracking the index into the Paragraph token's extracted_whitespace field. When I reviewed this code for another issue, I believe that I saw something about this statement that made me question why it was needed or what it was used for. What Did I Find? After adding extra debug statements and running through the tests, I started to understand why I might have had issues with this code. Normally, when software developers talk about an index, they are referring to a number that is 0 based instead of a position which is 1 based. Basically, the difference is whether the counting starts with 0, as with an index, or the counting starts with 1, as with a position. From a purist point of view, this field is named properly: if the field has a value of 0, it is referring to the set of characters before the first newline character in the extracted_whitespace field. Another way to look at this is that when the line is split on the newline character into an array, as I often do for processing, the 0th element is the first element. However, when this field is used, it has the appearance of being a position. The extracted_whitespace field holds any leading whitespace for a Paragraph token. Accordingly, any whitespace before the first newline in that field is applied as leading whitespace to that text string as soon as that string's processing starts. As such, if there are multiple lines in the paragraph, the rehydrate_index field is set to 1 almost from the very beginning of processing. So, while it is properly called an index, it can appear to look like a position based on its usage. Following that information back to the code sample above, my question now made sense. However, while the naming could perhaps be better, it is indeed an index that somewhat behaves like a position, with that duality causing the confusion. In the end, after a decent amount of review, I resolved this issue without any changes except for an extra assert statement and some debug. I felt that the name of the variable and the variable's use were both exactly what they should be. Fenced Code Blocks and Blank Lines This issue was an interesting one: - verify that 2 blank lines solution ` if previous_inline_token . token_name != MarkdownToken . token_blank_line ` does not affect single line and 3 + line solutions - why needed ? From the start of the work on this issue, I knew it was going to either be very simple or very complex. The easy part of working on this issue was adding 4 new scenario tests, 99f to 99i, to check for any abnormalities. Based on the information contained in the issue's text, those new tests included multiple blank lines as well as mixing blank lines and text lines within the Fenced Code Block element. What Did I Find? The first thing I did after running the tests was to adjust the produced tokens to the new values, after manually verifying each one individually. Considering how the coalescing processor merged the Text tokens together, the text and the line/column numbers for each token seemed fine. What was immediately different was the HTML that was produced. Instead of being a direct interpretation of the Markdown, the resultant HTML had a few newlines missing. Far from being easy to find, this issue took me 2 days of research to find and diagnose, revealing problems with both the HTML transformer and the Markdown transformer. This issue just proved to be very difficult to isolate and identify. Addressing the Issue For the HTML transformer, additional logic needed to be added to track past characters. For whatever reason, when there are 2 blank lines in a Fenced Code Block element, one of the newline characters is not emitted. To properly address this issue, newlines needed to be added in 3 areas. The first area was in the handling of the text token, where a Text token preceded by 2 Blank Line tokens, requiring a newline to be added. The second area was in the handling of a Blank Line token to make sure that the newline was only added in the right cases. Finally, at the end of a Fenced Code Block element, another check needed to be added for the multiple Blank Line tokens, adding a newline if they were found. After completing that grueling episode of debugging, my focus turned to the Markdown transformer and consistency checks. While I was dreading another 2 days of research into the issue and how it affected the Markdown transformer, the results there was very easy to see and diagnose. When a switch is made from rehydrating a Text token to rehydrating a Blank Line token, a newline is omitted in the process. As such, it was easy to identify this case and add an extra newline before focusing on the rehydration of the Blank Line token. Having fixed that issue, I reran the scenario tests and got a series of weird errors complaining about current_token and None . That was quickly tracked down to working code for handling the last token in a consistency check, and checking to see if the inline_height variable needed to be increased by 1 to take care of the end Fenced Code Block token. After adding a quick check to make sure that it was not None , all the tests ran without issue. I had mixed emotions about this issue as I worked through it. On one hand, it was not a simple issue, so it was good to get it out of the way. Whether it was pride or confidence, I was pretty sure that most of the simple issues had been cleaned up, and this easily was not a simple issue. On the other hand, to find an issue like this near the end of the first phase of the project was a bit disheartening. In the end, I felt that the positives of finding and fixing this issue outweighed the negatives. It took a while to clean up, but that scenario was now running properly. Testing Backslashes The item for this one was simple, yet descriptive: - backslashes - 600 - verify with before and after with all valid inline During the various passes I have made through the code, one thing that I worried about was whether each of the inline sequences properly handled the backslash character. While I was confident that the start sequences of inline elements were decently tested, I still retained a shred of doubt that I had missed something. As for the end of the inline elements, the confidence level was somewhere between a shred of doubt and a hunch that I missed something. Regardless, it was a good thing to check out and then check off the list. To complete this task, the first part was simple: I needed to come up with a series of scenario tests to cover all the cases for backslashes and inline character sequences. So, starting with the Code Span element, I started working my way through the inline elements. In each case, the main test was to make sure that a backslash character before the start inline sequence prevented the inline element from starting. Then, if there was an end inline sequence, I added an additional test to make sure that a backslash used right before the end of the sequence had the intended result. Finally, to make sure all inline sequences were covered properly, I replicated each of the tests for all three inline blocks: Paragraph elements, Atx Heading elements, and SetExt Heading elements. Tests 1 to 13 were for Paragraph elements, tests 14 to 26 were for SetExtHeading elements, and tests 27 to 39 were for the Atx Heading elements. What Did I Find? For this issue, I lucked out. Each of the tests passed, without any issues. But far from being a disappointment, researching this issue and adding the additional tests helped me put to rest any doubts that I had about backslash handling. Properly Escaping Special Characters Unlike a lot of the other issues in this block of work, this item was clearly a task: verify that any special characters used can be recognized and specially escaped To get to this point in the development of the project, I had to enable certain characters to have additional meanings. From the backspace character (\\b) meaning \"do not consider the previous character\" to the replacement character (\\a) allowing for a substitution to be documented, there were in all 5 characters that fit into this group. According to the Github Flavored Markdown (GFM) specification, a character is : a Unicode code point. Although some code points (for example, combining accents) do not correspond to characters in an intuitive sense, all code points count as characters for purposes of this spec. Following this logically, any character that is a classic ASCII control character, that is a character between 0 and 31, is considered part of the Unicode control characters group. As all the special characters are classic control characters, they also exist as valid Unicode code points. Therefore, if I wanted to use them as special characters, I had to make sure to escape them if they appeared in input. So now, to complete this task, I just needed to figure out how to do that. Walking on The Shoulders of Giants Many languages face the same problem that I was then facing: how do we represent a control character in various formats while keeping the string readable? In the case of strings in languages such as Python, the backslash character is the go-to choice. A good example of this is the definition of the various special characters themselves. __backspace_character = \" \\b \" __alert_character = \" \\a \" whitespace_split_character = \" \\x02 \" replace_noop_character = \" \\x03 \" blech_character = \" \\x04 \" In the first two cases, the backslash precedes a character which is a stand-in for the control character. In the two first cases, the \\b character is standing in for the backspace character (ASCII 08) and the \\a character is standing in for the bell or alert character (ASCII 07). In the remaining cases, the \\x informs the Python interpreter that a 2-digit hexadecimal number will follow that is the actual character to insert into the string. In all cases, there is a clear understanding of what the character is, due to the backslash character escaping the readable form of the character to insert. For the processing of Markdown documents, the backslash was not a good character to use. The biggest advocate against its use was that it was already being used as an escape character for Markdown's special characters. To efficiently escape the desired character sequences, the chosen sequence would have to be one that rarely showed up in the normal Markdown document. Unimaginatively, I ended up choosing the character \\x05 . I could try and explain why it was the best choice, giving lots of made up reasons, but in this case, the reason was simple: 5 follows after 4. It made a lot of sense to pick another control character for the escaping character, as the other characters were all in that group. At that point, after eliminating any control characters that were commonly used, there were roughly 25 characters left. Any character would have been just as good as the others, so I just picked the next one in the sequence. Sometimes you need to solidly think something through, and sometimes you just need to pick something that just works. This was definitely a \"just works\" moment. Making The Change From the outset, I knew that if I did this change properly, the bulk of the work would be contained within the ParserHelper class and the InlineProcessor class. The changes to the InlineProcessor class were easy to define. When parsing inline blocks of text, I would need to specifically look for any of the special characters, escaping them before moving forward with the rest of the string. For any processing after that change had been made, I knew that is where the ParserHelper class came in. Adding the following declaration to the top of the class: escape_character = \" \\x05 \" the next function was easy. To avoid a cycle in the module dependencies, the valid_characters_to_escape function was added to allow the InlineProcessor class query the ParserHelper class for the characters to escape. After that, to make sure that I had coded things properly, I added a series of 24 scenario tests prefixed with test_textual_content_extra_ to perform special character tests. Starting with a simple test with an example that contained each special character, those tests quickly moved to testing various combinations of each character and normal text. Each section picked a specific type of escaped character, including it in a simple string and interspersed with examples that produced the unescaped versions of those same characters. A good example of this is the function test_textual_content_extra_4 . Used to test the replacement character \\a , the Markdown document is as follows: \\ a & \\ a & \\ a To prove that the code is working properly, an escaped replacement character must appear on either side of an actual replacement sequence, also using the \\a character. This results in a Text token containing the text \\x05\\a\\a&\\a&amp;\\a\\x05\\a\\a&\\a&amp;\\a\\x05\\a , as predicted. It starts with the sequence \\x05\\a to provide for an actual \\a character to be output. The following sequence, \\a&\\a&amp;\\a then documents that the & character was replaced with the text &amp; . From there, it just repeats enough times to make sure it works well. Once that simple sequence and others like it with the \\a character were verified, this pattern was then repeated with the other inline containers: Atx Heading elements and SetExt Heading elements. Cleaning Up While these changes sound good in theory, in practice there were some issues that needed to be addressed. The major change was the inclusion of the function remove_escapes_from_text . To ensure that those character were properly represented, I needed to be able to present a text string and have any instances of the escape character removed from them. Basically, if I supplied a Markdown document of \\aa , that \\a character is then escaped, producing a token with the text \\x05\\aa When I go to use that text, I need to remove the \"protection\" of the escape character before rendering that text. That was easy to do, and as I started adding calls to the remove_escapes_from_text function, there were some previously working tests that started failing. After doing a bit of research, the cases that were failing were a portion of the tests that relied on the remove_x_from_text and resolve_x_from_text functions. In each of those functions, something like the following was being done: def remove_backspaces_from_text ( token_text ): return token_text . replace ( ParserHelper . __backspace_character , \"\" ) While that was acceptable before this change, there was one glaring error with it after this change: it was removing all backspace characters, including escaped ones. Thought it took a bit more code, that one function was transformed into the following: def remove_backspaces_from_text ( token_text ): start_index = 0 adjusted_text_token = token_text [ 0 :] next_backspace_index = ParserHelper . __find_with_escape ( adjusted_text_token , ParserHelper . __backspace_character , start_index ) while next_backspace_index != - 1 : adjusted_text_token = ( adjusted_text_token [ 0 : next_backspace_index ] + adjusted_text_token [ next_backspace_index + 1 :] ) start_index = next_backspace_index next_backspace_index = ParserHelper . __find_with_escape ( adjusted_text_token , ParserHelper . __backspace_character , start_index ) return adjusted_text_token While it was not as simple, it was correct. Create a copy of the string, and then look for the first occurrence of the backspace character that is not escaped. When that is done, adjust that copy to remove that backspace, and then continue searching from there until there are no more backspaces to remove. Finally, after all that work, there was only one set of cases left, special characters within a code span. This was a special case because the characters needed to be preserved exactly, with no substitutions. In the case of the code span, once the code span start character sequence is found, a scan is made to locate a matching end character sequence. If it is found, the handler simply grabs and characters between the end of the start sequence and the start of the end sequence. To properly escape these characters, the escape_special_characters function was added. After a lot of work, and a fair number of changes, the parser was cleanly handling the escaped characters. From my point of view, that was a good, solid issue to end the work for the week on! What Was My Experience So Far? While there were many little things to learn, the big thing that I am learning with this phase of the project is that the bulk of this work is in the research. If I am having a good day, the research will take anywhere from 5 minutes to an hour, followed by at least an hour of retesting, or adding a slew of new tests and then retesting. When that happens, I am grateful that it goes by so quickly. And those positive experiences prepared me for experiences like the one that I had with Fenced Code Blocks this week. They are frustrating and can make me want to pull my hair out. I actually prefer the complex issues over these types of issues. With those issues, I can more clearly see the issue by breaking down that complex systems into simpler systems that can then be analyzed individually. For the Fenced Code Block issue, it was just a small nitpicky issue that I had to calmly analyze, looking within a small and complex set of variables to figure out. It is at moments like that, when I am faced with tough issues, that I take the time to step back and try and get a better perspective. While there are some issues that are more difficult like the Fenced Code Block issue, the bulk of the issues are relatively easy to diagnose given the right debug statements. And fortunately, the bulk of the project's design has held up nicely, so only a couple of small redesigns needed to be done. All in all, it is not a bad place to be in. Stepping back and really appreciating those parts of the project is what I believe keeps me sane. In the end, the severity of the issues and features evens out, even reducing in scope the further I get to the end of the project's first phase. Looking ahead, I know that I have a good handful of tough issues to deal with. Knowing that I was still able to keep a good perspective and keep my wits around me, I have confidence that I will be able to solve those issues. Some of them may take an hour to solve, some may take a couple of days to solve, but I will solve them! What is Next? Not to sound like I am repeating myself, but this part of the project is all about keeping the momentum going and solving another handful of issues.","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/10/05/markdown-linter-delving-into-the-issues-2/","loc":"https://jackdewinter.github.io/2020/10/05/markdown-linter-delving-into-the-issues-2/"},{"title":"Markdown Linter - Delving Into the Issues - 1","text":"Summary In my last article , I completed the last bit of work needed to complete the consistency checks. However, as I accumulated some items in my issues list, I decided to take some time and make a sizable dent in that list. This article details those issues that I investigated and their results. Introduction When I am teaching archery at the camps in my local area, I must constantly keep a level head and strive to find ways keep the kids motivated. Faced with children possessing various levels of archery skill, not only do I have to tailor any assistance to each individual child, but I also try to figure out how to get that child to retain some part of that assistance. Luckily, I have a couple of tricks up my sleeve that helps me in this area. The most useful trick involves the difference between moving a mountain and moving a mountain's worth of pebbles. When I ask the camper how easy it is to move a mountain, they usually look at me like I am the most stupid parent on the planet and then proceed to express how impossible it is in various colorful forms. As a follow up, when I then ask them if they can move that mountain one pebble as a time, they state that it would take a long time, but eventually they would be able to move that mountain. Granted, the description of how long that effort would take differs from camper to camper, some more colorful than others, but they all convey that it is ultimately doable. At that point, I calmly talk to the camper and explain that we are going to start working on each pebble of their archery skills, one pebble at a time. At open range events, I let each group of kids know that me and the other coaches will be there all day, and will be as helpful at the end of the day as we are at the beginning. Admittedly, a bit crazier near the end, but we try our best to remain helpful in the middle of that craziness. The reason I mention this story is that when it comes to the items on the project's issues list, the list definitely looks more like a mountain than a pebble to me. But by taking the same approach with the items that I do when teaching archery, I can approach that list calmly and not be hit with a large wall of anxiety. Instead of seeing the list as a large mountain, I can choose to see it as a large group of pebbles, moving them one at a time. My goal at this point is not to get rid of all those items at once, but to make steady progress in reducing the size of the issues list, once pebble at a time. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 09 Sep 2020 and 12 Sep 2020 . Starting with An Easy One Now that I have the consistency checks in place, when I am faced with a problem with a line number being off or a column number being off, it is a question of whether the parser's calculation is wrong or the check's calculation is wrong. While I hope that it is not an error with the consistency checks, I feel that it is a valid question to ask myself with each issue. That is the mindset that I had when I started looking at this issue. Leftover from my previous work on consistency checks, this was an issue where 15-20 minutes of digging into it with little success caused me to sideline it for later. The Markdown sample in question was a modified example 143 : > < div > > foo > bar bar At issue was the tokenization of the Blank Line element at the end of the Block Quote element. According to the parser, that Blank Line element was tokenized to [BLANK(4,1):] . However, the consistency checks were stating that the proper tokenization should be [BLANK(4,3):] . Which one was correct? What was the miscalculation that was causing one of the two to be off? Digging In Performing a quick visual check of the tokenization, things seemed to be valid on the parser side. The Blank Line element was enough to terminate the HTML element, but not to also terminate the Block Quote element before the Blank Line element itself. That part of the test looked good. In addition, the Markdown text does not contain any other text on that Blank Line, so the position of (4,1) looked to be an accurate position for the token. This meant shifting my focus to the consistency checks. Looking at the debug information for the consistency checks, something immediately leapt out at me. The debug output read: >>blank_line>>split>['> ', '> ', '> ', '', ''] >>blank_line>>index>1 >>current_position.index_number>>1 >>current_position.index_indent>>0 >>1 + init_ws(2)>>3 Matching that up against the code that produced the debug, it stated that the index_number variable was set to 1 while the consistency check calculated the number 3 for that same value. That matched up with the previous information, so looking at that last line, I saw that the calculation was determining that there was an initial whitespace of 2 applied to get to the value of 3 . That did not seem right. Adding some temporary debug statements, I was able to quickly determine that the reason that the check was adding that indent of 2 characters was due to a bad index on the Block Quote token. With every newline that occurs within a given Block Quote, the leading_text_index field must be updated to point to the current line. In this case, the HTML Block had a number of newline characters within its data but had not updated the index. As a result, instead of the index being set to 3, it was set to 0. This meant that it was picking up the 0th element of the array 1 , >{space} , with its length of 2 instead of the 3rd element in the array, an empty string with its length of 0. Fixing the Problem Having learned a decent amount of information from my research, fixing the issue was relatively simple. To increase the index by the right amount, I had to count the number of newlines in the text and apply that number to the leading_text_index field. newlines_in_text_token = __count_newlines_in_text ( current_token . token_text ) print ( \">>newlines_in_text_token>\" + str ( newlines_in_text_token )) container_block_stack [ - 1 ] . leading_text_index += newlines_in_text_token To err on the side of caution, I also decided to add a couple of extra scenario tests with further variations on the example. Just to make sure that the right thing would happen with an extra line of text, I added an extra line of text and created test test_html_blocks_143b . This addressed my concern that there may be something special with 2 lines of text, and a third line of text would either highlight or eliminate that concern. Then, to make sure that Block Quote lines and their indents were working properly, I added test test_html_blocks_143c . This test alternated the indents for the Block Quote element between 0 spaces and 1 space, stressing that part of the fix. The Start of A Series This next issue was the start of what I would later refer to as the 518 series of tests. The process started when I looked at example 518 and I wrote down a note check if all the combinations were covered. To give more context, I scribbled down \"518, are we sure?\" which I interpreted as \"Am I sure that I have all the combinations covered?\". While it may sound like a weird logically jump to make from a couple of scribbles, in my own way, I wrote down what was needed to make sure that I followed it up. My thinking was simple. A standard link, such as the following Markdown from example 518: [ link ] ( / uri \"title\" ) has 6 parts: the link label, the whitespace before the URI, the URI, the whitespace before the title, the title, and the whitespace after the title. All other parts of that link are single characters in nature and required elements. So, what I did with this series of tests is to start changing where the newline(s) were, seeing if I could break anything. To add some extra flavor to the tests, I also added a couple of tests that included backslash characters. What Did I Find Out? The primary thing that I found out was that the parser itself was not handling the newline characters within links properly. Specifically, the line numbers and column numbers were not reflective of the Markdown document that was parsed. To address this, the __handle_inline_special function was modified to handle an alternate calculation of the line/column number if a Link end token was found. While the initial calculation of new_index - next_index works for any Link tokens that do not contain a newline character, the calculation for those that included a newline character was going to be a bit more complicated. After trying out a couple of solutions in my head, the only one that gained the most traction with me was a staggered approach. In this approach, I preset an array with the lengths of various parts of the link, as outlined above. The function then goes through each of the parts of the link in their order, checking for newlines for each part as it goes. If at least one newline character is found, the line variables are updated and the link_part_index is set to that part's index number. At the end, all values before that index are reset to 0 and the column number is adjusted by the sum of each value in the array. While there are small fixes along the way to make the algorithm work properly, this algorithm works well as it keeps things simple. As each part is checked in turn for newline characters, the change to the line number variable is accurate. By setting and resetting the link_part_index variable to the last element that had a newline, only the elements after that index get added to the total, accurately reflecting the number of characters after the last newline character. After that was done, the only extra adjustment that needed to be added was accounting for whitespace at the start of lines within Paragraph elements, making sure that it gets applied properly. That entailed tracking whether or not the parser was currently processing a paragraph token and using the rehydrate_index field of the Paragraph token. With that in place, manual verification of the newly added case confirmed that the algorithm was calculating things properly. But What About…? Finishing up the work, there were two things that I knew I needed to work on: consistency checks and other types of links. Strangely, the only changes I needed to the checks were to change Link Reference Definition checks to ensure that it accounted for the newlines in various components. Other than that, the everything seemed to line up. As for other types of links, that answer was clear. Based on the parser code and the consistency check code, the only type of link that was being tested for newlines inside of the link were plain inline links. To combat this, I added extra items to the issues list, as well as made a mental note to revisit this later. Fenced Code Blocks and Blank Lines While this did not take a long time to solve, it was a good issue to get out of the way. At issue was the Markdown for example 99 : ``` { space }{ space } ``` where {space} stands for an actual space character. While the actual item reads fenced, 99 with more blanks , I knew that I was concerned about not having more examples with different types of blanks inside of the fenced code block. To fully test this, I created many variations on this one test, differing the number of blank lines and the amount of whitespace on each blank line. I was happy to find out that the work on the parser stood up to this extended testing, and the consistency checks only required a small change. To be precise, the only change that it needed was to reset the column number to 0 if the first inline token inside of a Fenced Code block was a Blank Line token. I did discover something somewhat puzzling though. In a Fenced Code block, if the first token is a Text token, the rest of the body of the block is coalesced. If the first token is a Blank Line token, then the tokens are not coalesced at all. Rather than focus on that at the time, I just noted it down in the issues list, and hoped to find time soon to figure out if that is the correct solution. The First Inline Token After an Atx Heading Token Having noticed this one a while ago, it was always something that I wondered about: why does the parser sometimes insert a whitespace character right after an Atx Heading token? I knew I must have a good reason for it, but I could not recall why. I did find a couple of scribbles in old notes about Atx Headings, but that was it. I needed something more to go on for me to understand this and wrap it up. Doing the Research Going way back in the project's commit log, I noticed two things: The first thing was that this project has been going on for a long while, and the second was that in that long while, very few of the commits refer to Atx Headings in their commit messages. To me, this means that Atx Headings are very stable, something that I felt proud of. That stability helps me to understand why I did not make any notes around what I was doing: quite probably they just were not needed. There are only 4 times where Atx Headings have been mentioned in the commit logs: Adding atx and setext headings. initial addition for Atx Heading support Fixing Atx headers to allow better inline processing. moved most text out of the token to allow normal inline processing to take place Adding line/column support for tax headings. adding line numbers and column numbers to the tokens Added testing of every inline at the start of a Atx heading… this issue This summary made it really easy for me to come to the observation that when the change was made to allow better inline parsing, the initial whitespace between the Atx Heading and its enclosed text was placed in the whitespace section of the text token. By looking at the consistency checks, I also observed that there are zero instances where an Atx Heading is not immediately followed by a Text token. After looking at those two observations and commit history, I do not feel it is a leap to say that this was a design decision that I made but never recorded. Further, I feel that the worst case is that it is a pattern that has a lot going for it and could easily be adopted as a new design decision. Either way, it seems to be a winning design. Backing Up the Design Decision Another thing that I learned from my research was that there was only one case of a non-Text element following the Atx Heading characters in any of the Markdown examples. The Markdown for example 183 is: \"\" \"# [ Foo ] [ foo ] : / url > bar which produces an Atx Heading element with a Link element right after it. Based on the previous research, I expected the tokenization to include that whitespace character, and it did not disappoint: expected_tokens = [ \"[atx(1,1):1:0:]\" , \"[text(1,3):: \\a \\a\\x03\\a ]\" , \"[link(1,3):shortcut:/url:::::Foo:::::]\" , \"[text(1,4):Foo:]\" , \"[end-link:::False]\" , \"[end-atx:::False]\" , For readers that have not been following along, the \\a character in an inline text string represents a replacement of one character for another character. In this case, the {space} character between the first two \\a characters is being replaced by the character \\x03 used as a NOOP character. To me, this means that when the parser tokenized that part of the line, it purposefully replaced a space character with an empty string. Based on the research, this was the right thing to do. The rest of addressing this issue was just replicating various forms of this example for each inline sequence. Starting with function test_atx_headings_extra_2 and ending with function test_atx_headings_extra_12 , I just cycled through each of the newline sequences, include Hard Line breaks. And with two exceptions, each of the new test functions passed. The first case where I needed to fix something came even before I added the extra functions: example 183. To make sure this was working properly, I needed to add an extra line to the __process_inline_text_block function of the InlineProcessor class. In the specific case where I was adding that special replacement, I determined that I was not clearing the starting_whitespace variable, and that caused the extra text to occur within the link instead of before the link. That caused the rehydration to fail as the space character was in the wrong position. The second case was in the __verify_first_inline_atx function of the consistency checks. The function was first cleaned up removing all the extra assert False statements and replacing them with an assert that the first token processed within the Atx Heading was a Text token. Once that was done, I just reordered the remaining lines in the function to keep functionality together, and it was done. What Was My Experience So Far? I went into this work knowing that at best, I would be able to knock a handful of issues of the list in one week. Meeting that goal, it also started to sink in that the research for each issue was going to take the lion's share of resolving each issue. If I was lucky, I figured I would stumble upon what I needed to do early in the research. But I prepared myself for the probability that, most often, I would need to add debug statement, add some extra scenario tests, execute all relevant scenario tests, examine the results of those tests, and iterate. Sometimes it would only take 1 or 2 iterations, but I figured that most often it would take upwards of 5 to 10 iterations. Based on my experience with this week's work, that set of expectations was a healthy set to start with. At this point, I was feeling decently okay. Not to sound too vague, but what I went through was right down the middle of what I expected to happen. As such, I was not feeling over positive or overly negative, just… well… okay. If anything, I was hoping that this \"down the middle\" result would apply most of the time. Having too many quick issues would get my hopes up for the remaining issues and having too many long running issues would stop any momentum I was gaining. But more importantly, I just wanted to keep on working through the issues list. I was okay with moving issues to the \"nice to have\" section that I had created. But I was only okay with that if I honestly though it was not required. If I kept myself honest, it was a good way to move forward without jeopardizing the project. And with that, I ended the work for the week, on a mostly positive note. What is Next? Having completed resolving a small number of issues, I was hoping to keep up some momentum by continuing to solve more issues in the next week. Simple as that! A simple reminder that when most people talk about arrays, they refer to an index into that array. As such, the first element of the array in the zeroth (0th) element of the array. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/09/28/markdown-linter-delving-into-the-issues-1/","loc":"https://jackdewinter.github.io/2020/09/28/markdown-linter-delving-into-the-issues-1/"},{"title":"Markdown Linter - Adding Remaining Inline Tokens","text":"Summary In my last article , I completed the addition of proper support for line and column numbers for the text token and emphasis tokens by finishing the consistency checks. In this article, I talk about the efforts and issues required to finish implementing the line and column numbers for the remaining inline tokens, including their consistency checks. Introduction From a wholistic point of view, I felt that the accuracy and consistency of the tokens were getting more solid with each change. While I expected a fair number of tests to fail when I started to add the consistency checks, I was now at a point where a failing test would be a novel thing. And that was good! But even with that positive outlook on the project and the consistency checks, I knew I still had a way to go to finish things up properly with respect to the tokens. After having finished adding the line/column numbers for the Emphasis tokens and the Text token, the remaining inline tokens were the only things left in the way of finishing that work. After the work I had done on that group of tokens, I was hoping that this would be an easy batch of work to complete. But only time would tell. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 04 Sep 2020 and 09 Sep 2020 . Remaining Inline Tokens Having taken care of the Emphasis tokens and the Text token, all the other inline tokens remained: Raw-HTML, Links, Images, Autolinks, Code Spans and Hard Line Breaks. Before starting work on each of these tokens, I was not sure if the effort required to implement each one would be more like the Emphasis tokens or more like the Text token. I hoped it would be a simple case and easy to work on, but there were no guarantees. With some optimism in mind, and my fingers crossed, I started my work. Raw HTML and Links As I write this article and look back at my notes, I fully admit that I am a bit stumped. Picking one of these two inline tokens to work on makes sense to me. I have no notes to myself saying, \"two for the price of one\" or \"these will be simple\". I am left scratching my head as to why I decided to work on both at the same time. Regardless of why I decided to do both, they were both completed. I believed that working on both items at the same time would just be asking for something to go wrong, so I chose to focus first on the Raw HTML token. The initial change was easy, changing the creation of the token from: RawHtmlMarkdownToken ( valid_raw_html ) to: RawHtmlMarkdownToken ( valid_raw_html , line_number , column_number ) thereby passing the line number and the column number to the constructor for the RawHtmlMarkdownToken class. Once that was done, another simple change was made to the handle_angle_brackets function to pass the current line number and column number as arguments, as such: new_token , after_index = HtmlHelper . parse_raw_html ( between_brackets , remaining_line , inline_request . line_number , inline_request . column_number , ) Testing and Iterating Running the tests for the Raw HTML token, it was immediately obvious to me that in certain cases, the column number was off by a bit. After a bit of research, I noticed that in cases where there was a Text token before the Raw HTML token, the new Raw HTML token had the same line/column number as the Text token. Digging a bit deeper, it appeared that in those cases, the remaining_line field of the inline_request object had the correct number of characters to make up the difference, but they were not being applied. To address that inadequacy, I made a small change to the above example. Following the logic of the inline algorithm, once a new token is created to be inserted, the text leading up to that token is determined, largely based off the remaining_line variable. While this seems slightly out of order, it ensures that the proper Text token with the proper line/column number is inserted in the correct order. However, because the new token is created before that Text token is inserted, it does not have the right column number. By simply adding the length of the remaining_line variable to the column number, the difference is accounted for. This was accomplished by first calculating that new column number: new_column_number = inline_request . column_number new_column_number += len ( inline_request . remaining_line ) and then passing that new new_column_number value into the call to the HtmlHelper.parse_raw_html function in the previous example instead of the inline_request.column_number argument. New Lines Inside of the Token After running through the tests again, almost all the tests were passing, except for the test for example 500 : [ link ] ( < foo bar > ) This case may look weird, but everything is computed properly as a series of Text tokens and a Raw HTML token. Because of the newline in the URI, the text is not eligible to be a link, but since the URI part is enclosed in \"angle brackets\", it is eligible to be a Raw HTML token. But even with the Raw HTML token being parsed, the Text token containing the trailing ) character was off. Instead of being reported as (2,5) , it was being reported as (1,17) . To investigate this further, I created the new scenario test test_raw_html_634a . This new test was a more isolated case of example 500, a copy of the test function test_raw_html_634 with a newline character inserted inside of the b2 HTML tag, as follows: < a />< b2 data = \"foo\" >< c > I started to look at this issue and it turned out to be an easy issue to overcome. Fixing Newlines With this isolated scenario test, it was very easy to see that the issue was with the raw_tag field of the Raw HTML token. When the token contained a newline character, that newline was treated as a normal character and added to the character count. What I needed to do was to make sure that the algorithm understood that the newline character was special and to handle it differently. So, to address that behavior, I introduced some extra code to the handle_angle_brackets function: if ( new_token and new_token . token_name == MarkdownToken . token_inline_raw_html and \" \\n \" in new_token . raw_tag ): split_raw_tag = new_token . raw_tag . split ( \" \\n \" ) inline_response . delta_line_number += len ( split_raw_tag ) - 1 length_of_last_elements = len ( split_raw_tag [ - 1 ]) inline_response . delta_column_number = - ( length_of_last_elements + 2 ) else : inline_response . delta_column_number = ( inline_response . new_index - inline_request . next_index ) return inline_response Basically, since the existing code already handled the case with zero newlines perfectly, I did not need to change that aspect of the function. However, in the case of a Raw HTML token that contained a newline in its raw_tag field, I needed special processing to kick in. The first thing I needed was a clear picture of the raw_tag field and its newline characters, so I split the string on newline characters into the split_raw_tag variable. Then I addressed the line number calculation first, correcting the line number calculation by adding the number of newline characters found to the inline_response.delta_line_number variable. 1 After I was sure that the line number was being correctly calculated, it was time for me to focus on the column number. While each of the lines in the raw_tag field were important, their content was already mostly covered by the calculation for the change to the line number. Each line except the last line that is. That last line was the new information that would lead the text after the Raw HTML token. As such, the column number was at least as many characters along as the length of any text past that last newline character, as calculated for the length_of_last_elements variable. With that calculation completed, all that was required was to add 2 to that value for constant element overhead: 1 for the length of the closing angle brackets ( > ) and 1 to translate the value from an index to a position. 2 Conveying That Information Back to The Caller With everything else completed, I then had to decide how I was going to get the newly calculated column number back to the calling function. According to the debug statements that I had added, the value was being calculated properly. While there were a couple of options on the table, I decided to go for a simple approach: a negative number. I am not sure if this choice is pythonic or not, I believe that it conveys the right information in an efficient manner. If the column number is zero or positive, it represents a simple change or delta to the column number, a simple value to be added to the current column number to arrive at the new column number. However, if the column number is negative, it represents an absolute number that should be used for the column number. For example, if the token contains a newline character, it makes sense that the returned value would indicate a value from the start of the new line, not from the last know position. Why a negative number? While I could have returned an extra value that determined whether the number was referential or absolute, that seemed too bulky. For me, this was keeping it lean within its limited scope. Adding Validation After all that work, the validation was very anti-climactic. It may appear that I cheated and copied the calculation from above as-is into the new __verify_next_inline_raw_html function. Rather than being a cheat, I worked through the calculations again on paper, making sure that I did not miss any weird boundary conditions. After generating the algorithm in the __verify_next_inline_raw_html function from scratch, I compared the two algorithms together and the algorithms themselves were the same. Rather than cheating, I considered it a validation that I had derived the right algorithm twice. What About the Links? As I mentioned at the start of this section, I am not sure why I decided to work on these two tokens together. I can only guess that perhaps I thought that adding line/column numbers to the Link tokens would uncover something important that adding line/column numbers to the Raw HTML tokens would not. The reality was that after completing the Raw HTML token work, the changes needed to implement the line/column numbers for Link tokens was trivially easy. Unexpectantly, this would foreshadow the following work on the other inline tokens. Autolinks, Code Spans, Images and Hard Line Breaks I expected some manner of difficulty in implementing the line/column numbers for these tokens, however the previous work made the implementation of the new code easy. There were issues that needed to be properly addressed for each specific type of token, but the hard work had already been done. As such, the work was more akin to copy-and-paste-and-adjust than anything else. In the implementation of each of the tokens, the initial calculation for each of the tokens included values for the length of the constant part of the element and the variable part of the element. Once that was complete and the easy tests were passing, any multiline parts were addressed, with progress being made to get closer to having the remaining scenario tests passing. To finish that work, consistency checks were added that were simply verifying the algorithms used previous and verifying the work. This process was a simple rehash of the work that I did for the Raw HTML token, and then again for the Link token. But it was working and working well. While a part of me was saying \"this is too easy, what's wrong?\", I double checked all my work to quiet that voice and assure myself that I had not missed anything. While it was simple work, it did take a couple of days to complete. But at the end of that work, each relevant token had a line number and a column number, and they had been verified. Even more interesting, while some extra scenarios were added to deal with missing cases (mostly to deal with multiline element parts), no new issues had been found. Things were looking good. Almost. Last Inline vs Next Block With all the inline tokens supporting line/column numbers, I felt as if a bit of a load was taken off of my shoulders. I was not really worried that there was something wrong, but as I mentioned in the last article : I know that I am fallible. There was no proof that I could see that I had missed something, but I just had a nagging feeling that I had left something out. Trying to get rid of that feeling, I went through the work that I had just completed and checked it again, finding nothing out of the ordinary. On top of that, I had automation in place to catch any miscalculations that I made, something that was welcome. After that extra checking was completed, I could not find anything wrong and I was ready to move on. But as I was getting ready to start working on some of the items in the issue list, I noticed something. Reading my previous article to gain some extra perspective on where I was in the project, I noticed the part where I stated: So, whether I liked the idea or not, validation of the first element in the list was mandatory. The last element is a different story. While it would be nice to tie the last inline token to the following block token, I felt that it was not as important as the verification of the first element. However, I added in a placeholder to the code to make sure that I would follow up on it later. I remembered! The Other Anchor for the Inline Tokens As I mentioned before, I started with the outer token and the first token because I wanted to ensure that was able to anchor the list, and the anchoring the first token was the easiest solution at the time. Having finished that task off and also having finished validation of the inline tokens within the list, it was now time to work on anchoring the other side of the list: the last inline token and the following block token. That is what the nagging feeling was! That is what I was trying to remember. Starting to research what I needed to do to resolve this anchor issue, I came to an interesting observation. While all groups of inline tokens start after a block token, not all groups of inline tokens end with a block token. Because of the way tokenization is performed, I decided not to expose line/column numbers for any of the end tokens that did not add something to the data stream. This means that except for the Emphasis end token, none of the other end tokens have a line/column associated with them. Why is that observation on tokenization important? A good example is the Markdown document for example 364 : foo * bar * From that Markdown, I can surmise that the tokens for that document will start with a Paragraph start token and end with a Paragraph end token. Inside of the paragraph, there will be a Text token containing foo , an Emphasis start token, a Text token containing bar , and an Emphasis end token. This is backed up by the realized tokens for the example, which are: expected_tokens = [ \"[para(1,1):]\" , \"[text(1,1):foo:]\" , \"[emphasis(1,4):1:*]\" , \"[text(1,5):bar:]\" , \"[end-emphasis(1,8)::1:*:False]\" , \"[end-para:::True]\" , ] From looking at this tokenization, the last token with a line/column number attached to it is the Emphasis end token, an inline token. Getting an actual block token to appear after those tokens is as simple as changing the Markdown to: foo * bar * This adds a new Blank Line token to the end of the array, adding the tokenization '[BLANK(2,1):]' . However, I knew the real trick would be to determine that value without having to add that extra token. Focusing on That Line Number Working through the issue from the previous section helped me understand something else about the relationship with the last inline token and the possible following block token: only the line number was important. Because the inline tokens are always contained within a container block element or a leaf block element, the last token in an inline token group is guaranteed to be either an end token for a previously started block element or a start token for a new block element. If the next block token after the inline tokens is a block start token, because of the work up to this point, a line/column number is guaranteed. If the next block token is a block end token, one of two things happens. Either a block start token follows with the start of a new block element, or the end of the document is reached. If a block start token follows, the line/column number is guaranteed as above. In the case of the end of the document, as no token with a valid line/column number follows, some other calculation is needed to determine the line number to compare to. The good news is that only the line number is important. Because only the line number is important, there is another available number that we can use: the number of lines in the Markdown document. As such, if there is a block start token after the inline block, I used the line number from that token as the line number to compare against. If no such token existed, I used the number of lines in the document. I tested this approach with a handful of scenarios, some with eligible following block tokens and some with an absence of eligible following block tokens. On paper it seemed to work without fail. The only thing that was left was to test that approach with actual code. Completing the Checks To calculate the line number to compare to, I added the __verify_last_inline function with my usual development pattern. Following that pattern, I started adding handlers for each of the inline tokens it encountered, just trying to get to a steady state where all the handlers were present. Once that was achieved, I started adding the content to each handler to calculate the height of the inline token. Now I wish I could say it was good planning, but it was just serendipity that I did this work right after the work on adding the line/column number support to most of the inline tokens. Based on that recent work, adding the calculations for the heights of each of the tokens was exceedingly easy. While it was easy, it took a couple of days for me to work through each case and verify each twist and turn of the token. But in the end, with only one planned failure 3 to address and one or two items to look at later, the token validation was complete! What Was My Experience So Far? Getting to this point in the PyMarkdown project was a momentous achievement for me, measured in months rather than days. Along the way, I had developed a Markdown parser, ensured that it emitted tokens that include line numbers and column numbers, verified its output against both expected HTML output and the original Markdown input, and had a good array of consistency checks on the line numbers and column numbers. Phew. It was a long list, but the project has come a long way. I was relieved that I got to this point with my original design mostly intact. I was aware that I was going to have to do some refactoring in the future to make the code more modifiable, but I believe it is in a decent position to make that happen. Besides, when I start doing that, I have almost 1400 scenario tests that will make sure any changes are not negatively impacting the code. With all that good stuff in mind, I started to look at the issues list, and paged through it. At just over 200 lines of text, it was a bit daunting to look at initially. But as I progressed with the project, any idea or question I had was put into that list. There were ideas for better tests, questions on whether something was done right, and notes on things I wanted to check because they looked weird in some way. And during the project's development to date, I had taken proactive efforts to get any serious issues out of the way. Being the optimistic, I hoped that I was left with a solid set of enhancements. Regardless of what remained in the list, I was sure that I could tackle it. And sure, there might be some rewrites that I would need to do, but they would make the project stronger, leaner, faster, and more maintainable. So how was I feeling? Very optimistic. There were quite the number of items in the issues list, but if I tackled them one at I time, I could get through them. And going through them would either fix an issue or confirm that the project did not have that particular issue. And to me, both outcomes were positive. What is Next? Having completed all the consistency checks, I now had operating scenario tests with values and line/column numbers that I was very confident about. But along the way, I had accumulated a decent number of items in the issues list. Before getting back to filling out the set of rules, it was time to address those items. Python's split function works as expected. If you have a string that does not contain the sequence to split on, it returns an array with one element. If you have a string that has one or more instances of the sequence to split on, it returns an array with each element being any text between those instances. As such, if a string has one newline character and is split, it will result in an array with a length of 2. Therefore, I used len(split_raw_tag) - 1 to figure out the number of newline characters found in the raw_tag field. ↩ By their nature, an index starts at 0 and a position starts at 1. As a column number is a position on a line but was being computed as an index, I needed to add 1 to the value to transition it into being a position. ↩ Ahead of time, I had already determined that the scenario test test_inline_links_518b was split over multiple lines and would be addressed by this validation. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/09/21/markdown-linter-adding-remaining-inline-tokens/","loc":"https://jackdewinter.github.io/2020/09/21/markdown-linter-adding-remaining-inline-tokens/"},{"title":"Markdown Linter - Adding Consistency Checks for Emphasis and Text Tokens","text":"Summary In my last article , I started to add the proper support for line and column numbers for both the text tokens and the emphasis tokens. In this article, I increase my confidence in the line and column numbers for those two inline tokens by adding the consistency checks for those tokens. Introduction I know that I am fallible. It therefore stands to reason that any code that I write will have some issues with it. Those issues may be obvious issues, or they may be issues that only occur under a bizarre set of circumstances, but they are there. Rather than fight against them, I embrace the attitude that good test automation will help me to identify those types of issues as early as possible. For the PyMarkdown project, this test automation takes the form of scenario tests containing consistency checks. These consistency checks validate that the Markdown documents in the scenario tests are properly interpreted by the PyMarkdown project. But while these consistency checks are beneficial, the consistency checks have taken a long while to complete. After 3 calendar months have passed, it can easily be said that my decision to add consistency checks to the project removed 3 months of project development time and replaced it with 3 months of project test time. Plain and simple, those statements are facts. My confidence about the project and its ability to work correctly is an emotional and abstract statement. However, with effort, I have been able to move it in the direction of being more of a fact than a feeling. The consistency checks are a form of test automation that apply a generalized set of rules over a group of tokens, looking for each group to behave in a predictable manner. Before this work, my confidence was expressed as a feeling: \"I believe the project is stable\". With this work nearing its completion, I can now point to the scenario tests and consistency checks that run within those scenario tests. I can state that each of the scenario tests is passing a rigorous set of criteria before it is marked as passing. That confidence can now be expressed as: \"Here are the tests that are passing and the checks that are being performed on each test.\" From that point of view, it made sense that before I start working on setting the line/column numbers for the remaining inline tokens that I would implement the consistency checks for the Text token and the Emphasis tokens. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commit of 02 Sep 2020 . Getting Started With Inline Token Validation At the start of the week, the code used to verify the consistency of inline tokens was extremely simple: print ( \">>last_token:\" + ParserHelper . make_value_visible ( last_token )) next_token_index = last_token_index + 1 while actual_tokens [ next_token_index ] != current_token : print ( \"-token:\" + ParserHelper . make_value_visible ( actual_tokens [ next_token_index ]) ) next_token_index += 1 Added in as a placeholder to allow me to see what was going on with the inline tokens, it served its purpose well. But as I started to work on the inline tokens and their line/column numbers, I needed to facilitate better consistency checking of those inline tokens. To start the work off, I removed that placeholder code from two places in the code and replaced both with a call to a new function verify_inline . The only difference between the two invocations of the function were the fourth argument, current_token . Called for the first time from the_ __verify_token_height function, the current_token variable is set to the block token after a series of inline tokens. The second time it is called, it is called at the end of processing to capture any inline tokens that are within one of the valid text elements, but at the very end of the document. When it is invoked from that location, that same argument is set to None . In both cases, the inline tokens to be validated were clearly outlined for the verify_inline function. Clearly Defining the Problem Before doing any real processing with the inline tokens, I needed to create a simple list containing the actual inline tokens that I wanted to check. I could have done that with the main list of tokens and the previously document outlining. However, I thought about it and decided that it was clearer to have a separate list that just contained the tokens that I was concerned about. Once I had all the inline tokens between the two block tokens in that new list, there was a small amount of work to do before the list was usable. While it was not difficult, the new list had some extra end tokens at the end of the list that needed to be removed. Working around those extra end tokens would have been okay, but I just felt that it was simpler to remove them from the list before I did any further processing. Having a simple list of the inline tokens to work with, the first iteration of the checking algorithm started with an easy outline to follow: if inline_tokens : for token_index , current_inline_token in enumerate ( inline_tokens ): if not token_index : __verify_first_inline ( last_token , inline_tokens [ 0 ]) else : __verify_next_inline ( last_token , inline_tokens [ token_index - 1 ], current_inline_token , ) # verify_last_inline(inline_tokens[-1], current_inline_token) From my viewpoint, the processing of the inline tokens had 3 distinct phases: the first element in that list, each element after it, and the last element in that list. Based on their locations, the first and last elements are special in that they anchor the other inline tokens to the block tokens on either side of the middle elements. Without those anchors, the middle elements lack a foundation with which they can based their positions on. Based on those observations, I chose to implement the check for the first inline token against the previous block token, and not the check for the last inline token against the following block token. Without validating the first element, validating any of the elements on the inside of the list would be useless. So, whether I liked the idea or not, validation of the first element in the list was mandatory. The last element is a different story. While it would be nice to tie the last inline token to the following block token, I felt that it was not as important as the verification of the first element. However, I added in a placeholder to the code to make sure that I would follow up on it later. Validating the First Element Following the pattern that I have used for validation in the past, I created the __verify_first_inline function with my standard starting template: def __verify_first_inline ( last_non_inline_token , first_inline_token ): if < something > : pass else : assert False , last_non_inline_token . token_name As this function is comparing the starting position of the first inline token to the last valid block token, the <something> in the above code sample was quickly replaced with: if last_non_inline_token . token_name == MarkdownToken . token_atx_heading : assert False elif last_non_inline_token . token_name == MarkdownToken . token_setext_heading : assert False elif last_non_inline_token . token_name == MarkdownToken . token_paragraph : assert False elif last_non_inline_token . token_name == MarkdownToken . token_fenced_code_block : assert False elif last_non_inline_token . token_name == MarkdownToken . token_indented_code_block : assert False elif last_non_inline_token . token_name == MarkdownToken . token_html_block : assert False else : assert False , last_non_inline_token . token_name and one by one I added the validation functions to replace the assert False statements. Following that same pattern for resolving these as I have before, I ran the scenario tests over the entire project using the command line: pipenv run pytest -m gfm Each time, I just picked one of the failing tests, and worked on that tests in that group until they were all passing. For each validation function, I repeated the same pattern with the first inline token that was observed. For example, the __verify_first_inline_atx function quickly evolved to look like: def __verify_first_inline_atx ( last_non_inline_token , first_inline_token ): \"\"\" Handle the case where the last non-inline token is an Atx Heading token. \"\"\" col_pos = last_non_inline_token . column_number + last_non_inline_token . hash_count if first_inline_token . token_name == MarkdownToken . token_text : replaced_extracted_whitespace = ParserHelper . resolve_replacement_markers_from_text ( first_inline_token . extracted_whitespace ) col_pos += len ( replaced_extracted_whitespace ) assert first_inline_token . line_number == last_non_inline_token . line_number assert first_inline_token . column_number == col_pos elif first_inline_token . token_name == MarkdownToken . token_inline_hard_break : assert False ... else : assert ( first_inline_token . token_name != MarkdownToken . token_inline_link and first_inline_token . token_name != EndMarkdownToken . type_name_prefix + MarkdownToken . token_inline_link ), first_inline_token . token_name What Did I Discover? Predictably, I discovered that there are 2 groups of text within block tokens: ones that support inline tokens other than the Text token, and ones that do not. The ones that do not support inline tokens are mostly easy: assert that the inline token is a Text token, and then assert on a simple calculation of the first line/column number. The validation of the HTML Block token and the Indented Code Block token both followed this pattern, with very simple validation. def __verify_first_inline_html_block ( last_non_inline_token , first_inline_token ): assert first_inline_token . token_name == MarkdownToken . token_text leading_whitespace_count = len ( first_inline_token . extracted_whitespace ) assert last_non_inline_token . line_number == first_inline_token . line_number assert ( last_non_inline_token . column_number + leading_whitespace_count == first_inline_token . column_number ) The Fenced Code Block tokens required a bit more effort, but not much. As the Fenced Code Blocks can start with 0-3 space characters that then need to be managed on any subsequent line in the code block, the owning block token's leading_spaces variable holds the information on what leading spaces were already removed. As such, when calculating the proper position of the first Text token inside of a Fenced Code Block, that removed space needs to be accounted for. To properly facilitate that, the last_token_stack argument needed to be plumbed through so the verification function could calculate the proper owning blocking token. The second group of block tokens were the more interesting group of tokens to deal with. This group of tokens includes the Atx Heading tokens (as shown in the above example), SetExt Heading tokens, and Paragraph tokens. The __verify_first_inline_atx function and the __verify_first_inline_setext function ended up looking similar: the Text inline token case was populated, but all the other types of inline tokens were handled with assert False statements. The __verify_first_inline_paragraph function was similar, but also slightly different. The same template was used to generate the function, but each of the conditions in the if-elif-else block were met at least once. However, since only the Text token and the Emphasis token have line/column numbers, allowing this comparison to be performed for them: assert first_inline_token . line_number == last_non_inline_token . line_number assert first_inline_token . column_number == last_non_inline_token . column_number All the other inline tokens, the ones that did not currently have a line/column assigned to them (yet), used the following comparison: assert first_inline_token . line_number == 0 assert first_inline_token . column_number == 0 It was not much, but it gave me two important bits of information. The first was that there was at least one case where each available inline token was the first inline token inside of a Paragraph token. The second was that both heading tokens, the Atx Heading token and the SetExt Heading token, only contained scenario tests that started with Text tokens. I made a note of that observation in the issue's list and moved on. Verifying the Middle Tokens With the validation of the first element out of the way, it was time to start working on the __verify_next_inline function. Now that the middle tokens were anchored at the beginning, each of the middle inline tokens could be validated against the inline token that preceded it. Since I knew that most of the inline tokens had not been handled yet, I started out that function with a slight change to the template: def __verify_next_inline ( last_token , pre_previous_inline_token , previous_inline_token , current_inline_token ): if ( previous_inline_token . line_number == 0 and previous_inline_token . column_number == 0 ): return if ( current_inline_token . line_number == 0 and current_inline_token . column_number == 0 ): return estimated_line_number = previous_inline_token . line_number estiated_column_number = previous_inline_token . column_number if previous_inline_token . token_name == MarkdownToken . token_text : assert False ... else : assert False , previous_inline_token . token_name assert estimated_line_number == current_inline_token . line_number , ( \">>est>\" + str ( estimated_line_number ) + \">act>\" + str ( current_inline_token . line_number ) ) assert estiated_column_number == current_inline_token . column_number , ( \">>est>\" + str ( estiated_column_number ) + \">act>\" + str ( current_inline_token . column_number ) ) The first set of if statements made sure that if either the previous inline token or the current inline token was one that I had not worked on yet, it would return right away. While this assumed that the line/column numbers were correct to a certain extent, I was okay with that assumption in the short term. The second part computed a starting point for the new line/column numbers, and then went into the usual pattern of dealing with each of the eligible tokens by name. Finally, the third part compared the modified line/column numbers against the actual line/column numbers of the current token, asserting with meaningful information if there were any issues. Emphasis Tokens I thought it would be quick to get emphasis out of the way, and it was! As both the start and end Emphasis tokens contain the emphasis_length , it was a quick matter of adjusting the column number by that amount. As both tokens are confined to a single line, there was no adjusting of the line number to worry about. Text Tokens As mentioned in a previous section, there are two major groups of block tokens that contain Text tokens: ones that allow all inline tokens and ones that do not allow inline tokens except for the Text token. The ones that do not allow inline tokens are simple, as all the information about the Text token is contained within the token itself. It is the other group that are interesting to deal with. The easy part of dealing with the Text token is determining the new line number. With the exception of a Text token that occurs right after a Hard Line Break token, the calculation is simple: split the text by the newline character, subtract 1, and that is the number of newlines in the Text token. If the token before the Text token was a Hard Line Break token, it already increased the line number, but the Text token that followed also started with a newline character. To remedy this, that pattern is looked for, and the current_line variable adjusted to remove the newline character at the start of the line. 1 Determining the column number is a more interesting task to undertake. For any Text tokens occurring within a block that does not allow for extra inline tokens, the column number information is already in the token itself, and the calculation is as simple. The column delta is equal to the number of text characters stored within the token 2 . If there was a newline in the token's text, this count is started after the last newline character. The second group of block tokens that can contain text are the Atx Heading token, the SetExt Heading token, and the Paragraph token. Since the Atx Heading token can only contain a single line's worth of data, no extra calculations are required to handle multiple line scenarios. In the case of the other Heading token, the SetExt Heading token, the starting whitespace is stored in the Text token's end_whitespace field. The processing of this information is a bit tricky in that the starting and ending whitespace for the Text tokens within the SetExt Heading token is stored in that field using the \\x02 character as a separator. Still, determining the proper indent and applying it to the column number is relatively simple. Dealing with a Text token within a Paragraph token is a lot more work. Due to other design reasons, the whitespace indent for these Text tokens is stored within the owning Paragraph token. While that is not difficult by itself, keeping track of which indent goes with which line is a bit of a chore. Luckily, when I was working on the Markdown transformer, I introduced a variable rehydrate_index to the Text token. When rehydrating the Text token, I used this variable to keep track of which stripped indent needed to be added back to which line of any subsequent Text tokens. Given the prefix whitespace for any line within the Paragraph block, calculating the column number delta was easy. Blank Line Tokens That left the Blank Line tokens to deal with, and I hoped that the effort needed to complete them was more in line with the Emphasis tokens than the Text tokens. I was lucky, and the Blank Line tokens were easy, but with a couple of small twists. Intrinsically, a blank line increases the line number and resets the column number to 1. That was the easy part. The first twist is that if the current token is a Text token, that text token can provide leading whitespace that needs to be considered. That was easily dealt with by adding the following lines to the handler: if current_inline_token . token_name == MarkdownToken . token_text : estiated_column_number += len ( current_inline_token . extracted_whitespace ) The more difficult problem occurred when 2 blank line tokens appear one after the other within a Fenced Code Block token. Because of how the numbers added up, I needed to adjust the estimated_line_number variable by one. if current_inline_token . token_name == MarkdownToken . token_blank_line : if previous_inline_token . token_name != MarkdownToken . token_blank_line : estimated_line_number += 1 estiated_column_number = 1 With that tweak being done, all the tests were then passing, and it was time to wrap it up. Was It Worth It? The interesting part about defensive code is that sometimes you are not aware of how good that defense is. Using the analogy of a castle, is a castle better defensible if it can withstand attack or if it deters others from attacking the castle? While I did not have any information about potential attacks that were stopped ahead of time, there were 2 actual issues that the current round of consistency checks did find. Issue #1: Image Link The first of those issues was an issue with the column number for example 600 as follows: !\\ [ foo ] [ foo ] : / url \"title\" Before these inline consistency checks were added, the text for the ] character was reported as (1,6) . By simply counting the characters, the ! character starts at position 1 and the second o character is at position 6. As such, the ] character should be reported as (1,7) . Doing some research, I concluded that the handling of a properly initiated Image token was being handled properly. However, with a failed Image token sequence, the ! character followed by any other character than the [ character, the ! character was being emitted, but the column number's delta wasn't being set. Adding the line inline_response . delta_column_number = 1 at the end of the __handle_inline_image_link_start_character function solved that issue. Issue 2: A Simple Adjustment The second of those issues was more of a nitpick that an actual issue. In the tokenization for example 183 : # [ Foo ] [ foo ] : / url > bar the first line was tokenized as: \"[atx(1,1):1:0:]\", \"[text(1,3):\\a \\a\\x03\\a:]\", \"[link:shortcut:/url:::::Foo:::::]\", \"[text(1,4):Foo: ]\", \"[end-link:::False]\", \"[end-atx:::False]\", Having a lot of experience sight reading serializations for all the tokens, the information in the Text token leapt out at me right away. In that token, the extra data associated with the token is composed by adding the self.token_text field, the : character, and the self.extracted_whitespace . Based on the above tokenization, that meant that the text sequence \\a \\a\\x03\\a was being considered as text instead of whitespace. To understand why I thought this is wrong requires an understanding of the existence of that character sequence. The \\a sequence is used to denote that a sequence of characters in the original Markdown document was interpreted and replaced with another sequence of characters. The \\x03 character within the second half of that sequence means that the {space} character in the first part of the sequence is being replaced with the empty string. Basically, to properly represent the space between the # character denoting the Atx Heading element and the [ that starts the Link element, I needed to add a space character that would not appear in any HTML transformation. And here is where the nitpicking comes in. When I originally added that sequence when working on the Markdown transformer, it made sense to me to assign it to the token's self.text_token field. But since then, I have grown to think of that sequence as being more extracted whitespace than token text. To resolve that, I decided to move the call to generate the replacement text from the self.token_text field to the self.extracted_whitespace field. It wasn't a big move, but it was something that I thought was the right thing to do. What Was My Experience So Far? While this batch of work wasn't as laborious as last week's work, the effort required to make sure it was correct was equal to or exceeding last week's work. I knew that if I made any mistakes last week, they would be caught when I implemented the consistency checks. Well, these were the consistency checks that would capture any such issues that slipped through. I am both happy and proud that I am coming to the end of implementing the consistency checks. It has been a long 3 month voyage since I decided that consistency checks were the best way to ensure that the quality that I wanted in the PyMarkdown project was maintained. And while there were times that I questioned if I made the right decision in dedicating this large block of time to this aspect of the project, I was confident that I had made the right decision. But looking ahead to what I needed to do after the consistency checks, I saw a fair number of items in the issues list that would need researching and possibly fixing. While I could start to release the project without them, I didn't feel comfortable doing that. I wanted to give the project the best chance it could to make a first impression, and then move from there. And that would mean more work up front. So while I was happy that the consistency check work was coming to an end, there seemed to be a deep pool of issues that would need to be research… and I wasn't sure how much I was looking forward to that. I still believe that adding the consistency checks was the right move. Of that I am still certain. Instead of a feeling that I have the right code in place to do the Markdown transformations, I have hard, solid checks that verify the results of each and every scenario test. It also gave me the interesting bit of information that the scenario tests did not include any cases where the Atx Heading token and the SetExt Heading token were followed by anything other than a Text token. Something interesting to follow up on later. To me, adding more of those checks for the inline tokens was just another solid step forward in quality. What is Next? Having completed the hardest inline token (Text token) and the easiest inline tokens (Emphasis tokens), it was time to buckle down and get the remaining tokens done. If I was lucky, the foundational work that I had already completed would make completing those tokens easy. If I was unlucky, there would be a whole selection of edge cases that I needed to account for. Realistically, I was expecting something square in the middle between those two scenarios. The next batch worth of work would answer that question! This has been noted in the issue's list, and I am hoping to look at it soon. ↩ That is, after removing any special characters and leaving the original text used to create those special characters. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/09/14/markdown-linter-adding-consistency-checks-for-emphasis-and-text-tokens/","loc":"https://jackdewinter.github.io/2020/09/14/markdown-linter-adding-consistency-checks-for-emphasis-and-text-tokens/"},{"title":"Markdown Linter - Starting to Add Line/Column Numbers For Inline Tokens","text":"Summary In my last article , I took care of completing the consistency checks by verifying the height of all block tokens. In this article, with all the block tokens now properly covered, I start to add proper support for line and column numbers for the text inline tokens and the emphasis inline tokens. Introduction As I mentioned in the last article: To properly verify any of the inline tokens, the tokens around it needed to be verified to give that token a solid foundation. Without those other tokens as a foundation, any attempt at verifying inline tokens would be shaky at best. With that foundation now firmly in place, it was then time for me to start adding the line/column numbers to the inline tokens. The scope of what I was about to start was not lost on me. From the outset, I knew that adding the line/column numbers to the Text tokens was going to be expensive. Starting with the obvious, the Text tokens are the default \"capture-all\" for anything Markdown that does not firmly fall under another token's purview. That alone meant there were going to be a fair number of scenarios in which Text tokens were present. Add to that number the various forms of text that the token contains, and each form's way of dealing with the Text tokens within their bounds. Also, as I wanted to have a good gauge on how hard it was going to be to add the other inline tokens, I added support for Emphasis tokens to the worklist. I was clear about the scope of this change with myself from the outset. It was going to be a long trek to complete all this work in one week. I did contemplate updating the consistency checks to accommodate the changes to the inline tokens, but discretion got the better part of me. This work was going to be tough enough on its own, no need to add some extra tasks to the list. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commit of 29 Aug 2020 . Framing a Big Problem in a Better Light Before starting with this monumental task, I wanted to take a step back and really understand the task and its intricacies. When I started looking at the sheer depth of this task, I will admit I was a bit scared at first. The work this task requires is daunting. Doing a simple search over the project's scenario tests, I found 1577 instances of a Text token in a scenario test and 161 instances of Emphasis Start tokens in a scenario test. That meant between the Text tokens and both Emphasis Start and Emphasis End tokens, I was looking at 1899 instances that needed to be changed and manually verified. That was indeed overwhelming. This is where my experience with test automation came in handy. I took a breath and started to look for equivalence partitions that I could use. While the number of discrete instances of Text tokens and Emphasis tokens were facts that I could not change, I decided to apply equivalence partitioning and reduce the effective number of instances down to a more manageable number. How Does That Work? Let me take a small sample function that is in the ParserHelper class, the is_character_at_index function. This function is as follows: @staticmethod def is_character_at_index ( source_string , index_in_string , valid_character ): return ( 0 <= index_in_string < len ( source_string ) and source_string [ index_in_string ] == valid_character ) The function is simple in that given a large variation on the parameters, it will simply return a True response or a False response. 1 While the number of variations are largely finite 2 , they do fall into a number of categories. Starting with the index_in_string argument, those groups are: less than 0, equal to 0, greater than 0 and less then len(source_string) , equal to len(source_string) , and greater than len(source_string) . Of those groups, only if the index_in_string argument is in the equal to 0 group or the greater than 0 and less then len(source_string) group do I need to check to see if the character at the specified index is equivalent to the argument valid_character . As the value to compare against is a single character, the only two groups for that part of the comparison are that it matches that single character or it does not. Based on this information, I can use those groups as equivalence partitions or equivalence groups or to partition the data to test into 7 distinct test groups. The first 3 equivalence groups are the ones that cause the first comparison to fail: less than 0 , equal to len(source_string) , and greater than len(source_string) . For this group, the negative group, a simple test with one value in each group is required. For the other 2 tests, the positive group, in addition to the comparison to get it into one of the 5 groups, one test is required where the index specifies the a character matching the valid_character argument, and one where it does not match. In all, 3 tests in the first group, and 2 sets of 2 tests in the second group, for a grand total of 7 tests. This works well because it reduces the scope of the testing to a manageable number. Given the less than 0 group, it does not matter if the index_in_string argument is -1 , -2 , or any other negative number. They all fit into that group and they all evoke the same behavior: they cause the expression to be evaluated as False . By applying this process to many testing problems, it can quickly break down the problem from an unmanageable number of possibilities down to a smaller number of more easily handled cases. How Does That Apply to This Work? No matter how it is viewed, having to change the serialization of 1577 instances of a Text token is a big job. That part of the work I cannot change. However, I can make the manual validation part of the changes more efficient by applying equivalence classes to those changes. While I was not sure at the onset what those classes were going to be, I was confident that I could work out those groups would be one-by-one. But it was still a big task, just not as big. Looking back at my notes, I have a scribble that says: ~40 variations for text, ~10 for emphasis I believe that was a note to myself to boost my confidence by estimating how many equivalence classes that I believed I would get the tests down to. As I wrote this article and looked at that scribble, for a second, I was back at the point in time when I wrote that down. Like an echo, I vaguely remembered the feeling of optimism that washed over me when I wrote those numbers down. While I am not 100% certain of what I was thinking at the time, I am confident that it was something like: 1600 validations is insane! On the other hand, 40 is okay. I can do 40. At that moment, it was not about whether those numbers were accurate, just that I had confidence that those numbers were in the general vicinity. While having to validate each of approximately 1600 variations of Text tokens filled me with dread, having to validate approximately 40 variations of those same Text tokens and approximately 10 variations of Emphasis tokens was something I had confidence that I could easily handle. Updating the Text Token Before I was ready to start with the Text tokens, I needed to get ready. Not a lot of work, but some solid foundational stuff to make the rest of the processing go easier. Getting Ready My main drive for updating the Text token to support line/column numbers was never about the small stuff. It was that boring work, stuff was easy to do and quickly finished, that I wanted to get out of the way. Adding the ability to pass in either a position_marker argument or the line_number and column_number arguments? Done. Making sure they got copied along with the other information when the create_copy function was called? Done. Changing the InlineRequest and InlineResponse classes to handle line numbers and column numbers? Done. If my memory and notes are accurate, those changes were all completed in the first half-hour that I used to work on these changes. Then, to ensure things were setup to verify the consistency of the changes in the future, I made some changes to the verify_line_and_column_numbers.py module. While I knew I was going to write the actual validations in a separate task, I wanted to make sure that I had a good view of what inline tokens were going to be handed off to the future consistency validators. To accomplish this, I added two sets of print statements: one as part of the __verify_token_height function and one at the end of the verify_line_and_column_numbers function. My plan here was to not only set myself up for the inline consistency checks to come, but to be able to see what the group of inline tokens to be processed was, to allow me to plan future sets of equivalence classes. With that setup work done, it was on to the actual classes. Starting with the Paragraph Tests With that foundational work completed, I decided to start with the tests in the test_markdown_paragraph_blocks.py module. Since the Paragraph Block tokens are the default containers for Text tokens, I figured that this was the best bet to get started with some of the simple stuff. That bet paid off with the first equivalence class, a Text token following a Paragraph token. If I had to point out the simplest case of a Text element in a Markdown document, I would definitely point to an example similar to example 189 . Despite its high index number, to me this is the simplest example of all Markdown documents: aaa bbb Simply speaking, it is two paragraphs separated by a single newline. While it is true that a single line of text would be simpler, to me, that is not a realistic example of a Markdown document. To me, a document means multiple paragraphs of text that conveys some information. From experience, it is very hard to convey anything except the most basic forms of information in a single paragraph. Also, as a realistic example, example 189 shows how you can separate two paragraphs in a Markdown document. As such, I consider this the root example. As this was the root example to me, it also contained the first and root equivalence class: a Text token contained as the first token after a Paragraph token. While there are numerous variations of this equivalence class, for me this is the base class itself. And as I looked through the code on how to isolate this equivalence class, I came to an interesting observation. It should have been an obvious observation, but it took me a bit to work through from \"huh?\" to obvious. I forgot that equivalence classes deal with input and output, but that source code rarely follows those same patterns. This Is A Good Thing When I started to look for the source code behind my first equivalence class, I found that it was hard to isolate the source code to just that equivalence class. But as I looked at the source code more, that made sense. One reason that it made sense was that if the cases were isolated based on equivalence class, it would mean that there was a lot of duplicated code in the project. Another reason was that such separation would force distinct paths through the source code that would not be natural from any other viewpoint than that of equivalence classes. The way the project was designed was to have an initial parsing phase to get all the raw information together, then a coalesce phase to combine any text tokens where possible, and finally an inline parse phase to handle the inline tokens. Dragging any artificial grouping of output across those phases seemed very counter-productive to me. But I still needed to figure things out. It was time for a bit of a regroup. Rethinking My Approach After performing a global search for TextMarkdownToken( on the project, I was rewarded with a small number of occurrences of a TextMarkdownToken being created within the project. This was good because it meant the number of actual changes that I would need to make was small, and hopefully each change would carry over multiple equivalence classes. The __handle_fenced_code_block function and the __handle_html_block function (through the check_normal_html_block_end function) were both responsible for handling the additional Text tokens as part of container processing, so they were the first to be changed. In addition, the parse_indented_code_block function, the parse_atx_headings function, and the parse_paragraph functions all included the creation of new instances of the TextMarkdownToken . Making those changes took care of all cases where the Parsing Processor created Text tokens. From there, a quick check confirmed that the Coalescing Processor only modified existing Text tokens and did not create any new ones. After a bit of double checking to make sure I did not miss anything, I acknowledged that the preparation work was done, and it was now onto inline processing. How The Inline Processing Works When the Inline Processor starts, it loops through all the tokens, explicitly looking for Text tokens, as they are the only tokens that can contain inline sequences. Once such a Text token is found, a further check is done to make sure that the Text token is within a Paragraph element or a SetExt Heading element (the only two block elements in which inline tokens are allowed) before proceeding with the actual processing of the Text Token. For any readers that have not been following along on the project's journey, let me provide a bit of a recap on how that processing works. Back in the article on starting inline processing , I go through the algorithm that I use in the inline processor: 3 set the start point to the beginning of the string look from the start point for a new interesting sequence if none is found emit the rest of the line from the start point and exit if one is found emit the text from the start point to the start of the interesting sequence handle the current interesting sequence update the start point to the end of the interesting sequence go back to the top From the Text token perspective, the important parts of that algorithm are the emit the rest of the line part and the emit the text from... part. When most of the other parts of the algorithm emit their own token 4 , a check it made to see what text has been \"emitted\" before that point. Then a new Text token is created with that emitted text, followed by the newly created token that represents the interesting sequence , followed by the algorithm looks for the next interesting sequence to deal with. In the end, there were only 4 places where I had to change the creation of the Text tokens to provide the line/column number information. In all, there were only 9 places in the project where I had to change the creation of the Text token. Far from being lulled into thinking the hard work was done, I figured it would be in the updating of the scenario tests that things would get interesting. And I was not disappointed! Scenarios With the code changes made to the Inline Processor, it was time to focus on the scenario tests and getting their data changed and manually validated. Using the command line: pipenv run pytest -k test_paragraph_blocks I executed each of the paragraph specific scenario tests, looking for the expected failures in each test that contains a Text token. Except for three tests, each of these tests were simple cases of the base equivalence class, which meant that they were quickly updated and verified. Of those three tests, two new equivalence classes emerged: the first Text token within an Indented Code Block token, and a Text token following a Hard Break token. The scenario test for example 195 is as follows: aaa bbb which was properly parsed into new equivalence class of an Indented Code Block token containing a single Text token and a normal Paragraph token containing a single Text token. As code blocks do not contain any inline processing and no extra inline processing was specified, this was an easy validation of that new equivalence class. Quick, easy, done. The other failing scenario test, the test for example 196 is as follows: aaa { space }{ space }{ space }{ space }{ space } bbb { space }{ space }{ space }{ space }{ space } where the sequence {space} was replaced with actual space characters. I replaced the tokens with what I thought was their proper line/column numbers and was surprised to find out that the tests were still failing. As I started to work through the research on why this was happening, I came to an interesting conclusion. I was not going to get away from handling the other inline tokens after all. The Truth Always Wins Based on the above Markdown, the tokens that were generated for that scenario test were a Text Token, a Hard Line Break token, and another Text Token. The first Text token was fine, I had that covered, and the Hard Line Break token was not what we were focusing on, so the fact that it did not have a line/column number associated with it was fine. But that left the second Text token in a bit of a conundrum. Based on the code at that time, the line/column number was 1,4 , which based on the existing logic was correct. But from a validation point of view it was incorrect: it should be 2,1 . It took me a bit to realize that if I was going to change each Text token, I would at least have to partially handle the other inline tokens. In this case, unless I added some code that understood the Hard Line Break token, the source code would correctly state that the line/column number was 1,4 . To be clear, it is not that the line/column number of 1,4 is actually correct, but according to the information that the algorithm has, that is the correct value to compute for that token. So, while I did not have to output the line/column number for the other inline tokens yet, I at least had to figure out what change that token was going to impart to the stream of inline tokens in that group. And It Happened with The Most Complicated Inline Token The Hard Line Break token just happened to be the token I needed to figure out. And it would end up being the most difficult inline token to figure out the new line/column number for. One reason was that, for whatever reason, I placed the newline for the Hard Line Break token with the following Text token, and not the Hard Line Break token itself. 5 This meant that to properly deal with that token, I needed to reduce the vertical height of the following Text token by 1, as the Hard Line Break token had already increased the line number. The other reason for it being complicated is that the proper setting of the column number relied on checking with the owning Paragraph token, grabbing any leading space for that next line from that token. All in all, in took a bit of work, but not too much before all the tests in that scenario test group were passing. While I knew there were 10s of hundreds more changes to make, I knew I could do this. Yeah, it would be slow, but if I just kept my focus on the task at hand, I could do this. Lather-Rinse-Repeat While I could go through each of the other equivalence classes that I discovered and processed, I will leave that for a future article where I talk about the inline consistency checks. It was enough of a brutal and time-consuming process that I will not make it more so by talking about it. Each time, I literally picked a new section of scenario tests, replaced the test_paragraph_blocks in the command line with the prefix for another group of tests and ran it again. With the results of that test run, I picked off one of the failing tests, correcting the line/column number for the Text tokens, and running the tests again to repeat the process. As I went, I manually validated each test's changes, and I rechecked my results as I staged the changes into the project's GitHub repository. A good example of this process was the next group of tests that I tackled: the Hard Line Block group. The first couple of tests were a rehash of what I had already done, so determining the proper line/column numbers for those tests were easy, and quickly verified. That left tests that included Emphasis tokens and Text tokens within Atx Heading tokens. I just buckled down and followed at the same process as documented before, adjusting as I went. Yes, it was slow, but it was also good. While it dragged on, I was getting predictable results with the application of my process. In my mind, I had confidence that it was no longer a matter of \"um… around 1600 tokens? how am I going to…\". I was making that transition to \"how can I get these done more efficiently and reduce my time on each test without sacrificing quality?\" Updating the Emphasis Token Compared to the work required to change the Text token, updating the Emphasis token to include line/column numbers was trivial. As the work had already been done to determine the width to apply to the start and end tokens, the main change was to pass the line/column number information to the constructor of the EmphasisMarkdownToken and the EndMarkdownToken. With that change in place, I started running the scenario tests in the emphasis group and only had to make one small change. In the cases where the end Emphasis token were completely consumed, everything was fine. But in the cases where an end Emphasis token were partially consumed, the column number was off by one. That took a bit of puzzling, but after some thinking, the answer leapt out at me. I will not kid you though, without me scribbling down the various cases and working through the scenarios, it would have taken me a lot longer. For the start and end Emphasis tokens, the Inline Processor creates a Special Text token that contains either the * or _ character and the number of those characters found. Because emphasis is processed from the inside out 6 , the emphasis characters taken from those Special Text tokens occur at the end of the Special Text token for the start Emphasis token and the beginning for the end Emphasis token. As a result of that, the start Emphasis token's column number needed to be adjusted by the number of characters consumed, to ensure it pointed at the right location. Once that adjusted was added, the remaining scenario tests passed. While I was not sure if the other inline tokens would be as easy as the Emphasis tokens, I was hopeful. And in a long project, that is a good thing! What Was My Experience So Far? When I start to write these sections in my articles, I always refer to my notes and try to put my mind back into the frame of mind I was in at that time. While there are sparse notes here and there about this section of work, there is really only one of those notes that properly sums up the work: Phew! While there were times during that slog that I did not think it would ever end, I was adamant that I was going to get through it and complete it. In my mind, it was not a question of confidence, it was a question of endurance. My change algorithm was simple enough, and I had confidence that the validation part of that algorithm was solid. It was just a matter of working through what seemed to be a mind-numbing number of changes until they were all done. But I persisted and got through it. And while I believe it was the right decision to only focus on the Text token and the Emphasis tokens, in hindsight, it might have been okay to add the other inline tokens at the same time. With all the work to make sure their space was properly accounted for, I believe that most of the work that I have left with those tokens is to plug in the calculated line/column numbers into the inline tokens themselves, changing the serialized text, and writing the consistency checks. Be it as it may, unless I messed up a calculation, the hard part of making sure the calculations work has already been done. On the technical debt point of view, I am a bit worried, but not too much. The list of things to check in the issues list is a bit larger than I like it, but there are some future ideas and a lot of double-check reminders on there. At the very least, I am sure I can start to make short work of a lot of those issues, or properly prioritize them for later, whichever is best for that issue. What is Next? With the Text tokens and the Emphasis tokens out of the way, I decided that it was better that I add the consistency checks for those tokens before progressing forward. After having to do a fair amount of work to support \"bypassing\" those tokens to properly calculate the line/column number of any following Text token, I had a feeling it would come in handy if I moved it up the priority list a bit. This function makes simple assumptions that the source_string argument is a string of any length, the index_in_string argument is an integer, and the valid_character argument is a string of length 1. Because the argument names are explicit enough and their usage is within a predefined scope, I decided to not verify the type of value for each. As such, the statement that the function will either return True or False assumes that those assumptions are followed. ↩ For more information, see Wikipedia . The short answer to this is that I would start with the first argument containing an empty string, for a count of 1. Then I would move on to a string with 1 character, and have to populate that string with every viable Unicode character. Moving on to strings with 2 characters, I would need to take every 1 character string, and repeat that same process with the second character. Repeating this process, the number of variations on possible strings is not infinite, but mathematically it is called a large finite number, or largely finite. ↩ Looking back at it myself, it is a bit hidden, but it is in the section on code spans in the fourth paragraph. ↩ The two exceptions to this are the handling of the backslash sequence and the character entity sequence, both which add to the text that is being accumulated. ↩ Yes, I did add an item to the issues list for this. ↩ For a good example of this, look at example 422 . ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/09/07/markdown-linter-starting-to-add-linecolumn-numbers-for-inline-tokens/","loc":"https://jackdewinter.github.io/2020/09/07/markdown-linter-starting-to-add-linecolumn-numbers-for-inline-tokens/"},{"title":"Markdown Linter - Adding Consistency to Token Heights","text":"Summary In my last article , I took care of completing the consistency checks by verifying for the second half of the container block tokens: the Block Quote tokens. In this article, I fill out the line/column number consistency checks by adding support for determining and verifying the height of all block tokens. Introduction From a high-level point of view, I believe that the project is coming together nicely. Each of the examples in the base GFM specification have been tested, and their proscribed HTML output verified. The consistency check that verifies that that Markdown tokens contain the correct information is also in place, covering all existing tokens. What was left was a bit of unfinished business with the consistency checks to verify those same tokens. Two parts of that check were left: verifying the token height and verifying the inline tokens. I had always intended the verification of inline tokens to be the final verification. That was always immediately clear to me. To properly verify any of the inline tokens, the tokens around it needed to be verified to give that token a solid foundation. Without those other tokens as a foundation, any attempt at verifying inline tokens would be shaky at best. It only made sense for me to start working on the verification of token heights before verifying the inline tokens. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 15 Aug 2020 and 19 Aug 2020 . Looking Back at Last Week's Issue After finishing the work on last week's article, I took it easy for a few days, working on the project when I could, but not pushing to hard. As someone who uses his brain heavily in both his professional capacity and his personal capacity, it was a bit of a wakeup call for me. When I saw the end of the project getting closer and closer, I started to put in extra effort towards the project, thinking that I could get there faster. This normally would not be a bad thing. But to achieve that extra effort, I diverted some of my energy away from the energy that I normally use to take care of myself. I did not really think about it before I started doing it, it just happened. While the result was me taking an extra day to complete the article, it could have been worse. From an article point of view, I needed to rework 2-3 sections, but it was not too bad. From a personal point of view, if I had continued to push myself, I believe it would have multiplied the time needed to recover substantially. I forgot that taking care of yourself and your mental well-being is very important, especially in times of crisis. In normal times, I might have been able to make that trade off work, but currently, I do not believe it is a viable option for me. I just do not have tons of extra energy to spare. Like it or not, those are just the times we are living in right now. And that is okay. It is taking me a bit to accept that, but I am working on it. What is important to me is this project and writing about it… all about it. Not just the rosy stuff you read in other blogs, but the actual problems I encountered and how I approached them. At that includes problems like these. Getting Ready for the Changes Knowing that I was going to be making a lot of changes to the verification logic for line/column numbers, I wanted to prepare for that work by moving that code into its own module. Having seen this feature organization work well for the Markdown transformer and the HTML transformer, I figured that moving the line/column verification code into its own module was an easy choice. Further increasing the benefit of that choice was the fact that I was going to add more logic to the code. Keeping all that logic in one place just made a lot of sense to me. The movement of the code was a pretty painless process, will all functions that handle the verification of line/column numbers being moved into the verify_line_and_column_numbers.py module. Once moved, any functions that did not need to be public were prefixed with __ and their invocations were also changed to add the __ prefix. Having a good set of scenario tests added to the ease of this change, as I was able to verify that the code was still working properly at each stage of the change. With the new module created, it was time to add to that new module. Verifying the Height of Tokens As with any of the consistency checks, the verification of the token height started as a very small and compact function. At the beginning, the __verify_token_height function had 2 parameters: current_token and last_token . Knowing that I had the SetExt Heading token to process, I encapsulated that logic within that function, calling the __validate_block_token_height function to do the heavy lifting. This encapsulation allowed me to replace the line_number and column_number variables used by the other tokens with the original_line_number and original_column_number variables used by the SetExt Heading token. That verification function, the __verify_token_height function, needed to be called from 2 locations: after the __validate_new_line was called and at the end of the normal processing. The call after the __validate_new_line function was called ensured that the height of any 2 block tokens not on the same line was calculated. If the 2 tokens were on the same line, one of them was a container block token, and they would always be on the same line. As such, I could just focus on the tokens on different lines without losing missing anything. The call at the end of processing would ensure that the height of the final block would also be verified. It was a good start. With all the little stuff out of the way, it was on to the heavy lifting part of the change. Doing the Heavy Lifting The __validate_block_token_height function was always intended to be a function that needed to know about every block level token. From the initial design for this change, I figured that it was best to have one big function that could be refactored later, than to have duplicate code in separate handler functions. As I have had good success with that pattern so far, I decided to use it again here. Like my other uses of the pattern, I started off the function with a large if-elif-else statement containing all the leaf block token names, one to each if statement. Each if statement contained a single line: assert False and a final: else : assert False , \"Token \" + last_token . token_name + \" not supported.\" Just like before, I had a plan. I started running tests in groups based on their name. So to start, I used the command line: pipenv run pytest -k test_paragraph_blocks_ to run all the tests that dealt with paragraph blocks. If I hit a type of leaf block that I had not worked on yet, the assert False triggered with the line number indicating which token type failed. If I hit a type of block that I was not expecting, the assert False in the final else would be triggered, letting me know which token I missed. And it was a lot of lather-rinse-repeat . I ran the tests over and over again using the above command line. If any tests failed, I picked either the first test or the last test and examined why the test failed. If it was the first time that I was dealing with that specific token, I coded a good guess as to what the height formula should be. Otherwise, I examined the existing formula, and tried a variation of the code that would handle the new case. Once all the tests for a given group were passing, I picked another group. This repeated until all the scenario tests in the project were passing. For the most part, that seemingly endless processing loop worked. But as in any project, there were things that needed to be handled separately. Counting Newlines Shortly into the changes, I figured out that I needed a simple helper function to calculate the number of newline characters in each string. Based on my observations, I was going to need to count a different set of newline characters for most of the tokens. Rather than implementing the algorithm multiple times, it made sense to put it into one function and in one location. After a couple of tries, I ended up with: def __count_newlines_in_text ( text_to_examine ): original_length = len ( text_to_examine ) removed_length = len ( text_to_examine . replace ( \" \\n \" , \"\" )) return original_length - removed_length I forget where I encountered this pattern initially, but it was a useful one to remember. While it does include the possibly expensive creation of a new string, the algorithm itself is simple. The number of a given characters in a string is the difference between the original length of the string and length of that same string with all that specific character replaced with an empty string. Say for example I need to know how many a characters are in the string maybe a good day to die , which has a length of 23. If I remove all the a characters, I am left with the string mybe good dy to die which has a length of 20. Subtracting the second result from the first result leaves a value of 3, the number of a characters in the string. For Paragraph tokens, the use of this function was simple: token_height = 1 + __count_newlines_in_text ( last_token . extracted_whitespace ) as it was for Indented Code Block tokens: token_height = 1 + __count_newlines_in_text ( last_token . indented_whitespace ) For Link Reference Definitions, it was even more useful: token_height = ( 1 + __count_newlines_in_text ( last_token . extracted_whitespace ) + __count_newlines_in_text ( last_token . link_name_debug ) + __count_newlines_in_text ( last_token . link_destination_whitespace ) + __count_newlines_in_text ( last_token . link_title_raw ) + __count_newlines_in_text ( last_token . link_title_whitespace ) ) After dealing with those tokens, there were a handful of other tokens that were easily handled by very simple calculations. After those tokens were handled, there were only two troublesome tokens left: the HTML Block token and the Fenced Code Block token. Leaf Block Stacks and Tracking Tokens To properly process the height of those two troublesome tokens, a pair of concepts were introduced almost at the same time. I tried splitting these two concepts into their own sections, but I found each attempt to do that complicated by their dependencies on each other. The first of those concepts was the ability to track the leaf block that was currently active at any point. The primary driver for this concept was to provide context to the tokens that occurred within the HTML Block element and the Fenced Code Block element. As these two block elements handle their own text parsing, I needed to avoid any \"extra\" checking that occurred within these blocks. After trying 4 or 5 other alternatives, the tried-and-true block stack was easily the best, and most reliable, solution. The second concept was closely tied to the first concept and dealt with properly tracking the right tokens. To finish the handling of the HTML Block element and the Fenced Code Block element, I needed to make sure that the concept of the \"last\" token was correct. To get that working properly, I added code to check the stack and only set the new \"remembered\" token variable if that was not set. How Did This work? Unless there was anything to do with HTML Block elements or Fenced Code Block elements, this code was not invoked. Except for the 4 block tokens that do not have close tokens (Blank Line, New List Item, Link Reference Definition, and Thematic Break), any start token was added to the stack. When an end token was encountered, if the name of that end token matched the top token on the stack, that top element was removed. Simple stack management. After a test revealed that I had forgot to add one of those 4 block tokens to the \"do not stack\" list, the stack worked flawlessly. The tracking of the tokens to avoid duplication worked as well. When one of the two special blocks were encountered, the stack logic would add them to the stack. Once added, the algorithm assumed that the handling of the HTML Block tokens and Fenced Code Block token would handle any encapsulated tokens and did not track any of those encapsulated tokens. When the end of the block occurred, it was popped off the stack and the normal processing occurred. There were a couple of small issues at the start, but after they were cleaned up, it was smooth sailing after that. Why Did They Need Special Processing? Although I tried a number of different options, the only thing that worked for determining the height of these two special block tokens was a brute-force iteration of all the inline tokens. While there were other tokens that persisted information on how many newlines were contained within their Leaf Block, these two Leaf Block tokens did not. Without that information, the only option left was to iterate through each of the encapsulated inline tokens, counting newline characters as I went. But with those tokens already counted, I needed to avoid counting them a second time. It was not a great solution, but it was the one that I ended up with. Up to this point in the project, there was no reason to change how those two Leaf Blocks stored (or did not store) any newline, it just was not a problem. While it was not a great solution, at this stage it was an efficient solution. But to check to see if I could do it better, I created a new item in the issues list, and moved on. There Was One Additional Thing As I finished up my work validating the Fenced Code Block token heights, there was one scenario test that snuck up and surprised me: example 97 ````` ``` aaa This example, and the ones around it, show how an open block is closed when the container block that owns it is closed or when the end of the document is reached. While everything else was working properly with this example, the token's line height was off by one. After double checking the math for the consistency check and for the existing tokens, I confirmed that it was an off-by-one error. Given that error and the section that the example was in, the next step was an easy one: craft an example that included the end of the Fenced Code Block element: ````` ``` aaa ````` Running that test, it immediately worked, which meant only one thing to me: the algorothm needs to know if the end token was forced. Determining If A Token Is Forced Right away, I was aware that determining if the end token was forced was not going to be an easy task. I immediately figured out one approach but dismissed it as too costly. But as I spent hours looking for any other approach that worked, I was coming up empty with each attempt. I found some partial solutions, but nothing that worked in all required cases. In the end, it was that costly approach that I returned to as my only solution. What was that solution? Costly as it was, that solution was to add a field to every end token that indicates whether it was asked to be closed or forced close. Why was it costly? A quick scan of the scenario tests revealed almost 200 instances of an end token… for only the scenario tests starting with test_markdown_a and test_markdown_b . Extrapolating from that sample, I believed that it would realistically mean changing between 1250 and 1750 end tokens throughout all the examples. It was not a decision that I made lightly, but with no other viable options, I started to embrace it. Making the change was the easy part. Before the was_forced field was added to the EndMarkdownToken class, the compose_data_field function looked like this: def compose_data_field ( self ): display_data = self . extracted_whitespace if self . extra_end_data is not None : display_data = display_data + \":\" + self . extra_end_data Once the was_forced field was added, it changed into: def compose_data_field ( self ): display_data = \"\" if self . extra_end_data is not None : display_data += self . extracted_whitespace display_data += \":\" if self . extra_end_data is not None : display_data += self . extra_end_data display_data += \":\" + str ( self . was_forced ) self . extra_data = display_data If this looks like I snuck something extra in, it is because I did. Given the number of changes that I was going to make, I wanted to ensure that I could efficiently make those changes and verify them. While having optional parts of the token serialization left out was more compact, it did make the serialization harder to read. I figured that if I was making a thorough change like this, being explicit with each of the fields would reduce my chances of getting one of the serializations wrong. Instead of asking myself \"what was the default value of the second and third fields\", I just added those fields and ensured they were serialized. Basically, I gambled. The cost of making the change to the token's serialization was cheap. It would be in the verification of that change where most of the expense would come. And my bet was that serializing all fields explicitly would make that verification easier and faster. Was It Worth It? If anyone would have asked me that question at the start of that process, I would have hemmed and hawed , giving an answer that would be uncertain at best. But by the time I had finished the first group of scenario tests, that answer was easily becoming a more solid \"Yes!\". Was it painful? Yes! I purposefully did not keep track of how many changes I had completed and how many I had left to go. I felt that keeping track of that number would be discouraging and focusing my mind on something other than the obvious thing: this was the right change to do. I was aware this was going to take a while, and as the hours ticked by, that was hammered home multiple times. By the time I got to the end of this process, it took 4 days and many long hours to complete. But, in my eyes, it was worth every step of it. As this change touched on every scenario test, I needed to manually verify each scenario test before going on to the next test. And while I had test automation in peace, I had not manually gone through each of the tests and verified for myself if the tokens looked correct. For me, I found peace in having inspected the tests for myself, knowing that the consistency checks were providing me with the same results as my manual verification. Does that mean I want to do it again? Not really. But would I do it if I felt that I needed to? In a heartbeat. In terms of confidence, this was me doing a final read of my article before publishing it. This was me running the Static Code Analysis against my code one more time just to make sure that I did not miss anything. This was me looking over the changes I am about to commit to the repository before I submit the commit. It is easy to argue that each of those actions is more emotional than logical, but that is the point. Sometimes, it is just logical to do an action for a positive emotional reaction. And that is okay too. What Was My Experience So Far? I know I will only get one chance to make a good impression with this project, so I want to make that impression a good one. If I want to provide a good impression by having a good linter as the outcome for this project, I will need to test as many of the combinations of Markdown elements with each other as possible, in as many circumstances as possible. From the beginning of the project, a simple calculation made it clear that the scope of testing required would be difficult if I was lucky. If I was not lucky, that hopeful guess of difficult would grow by at least 2 orders of magnitude. But even given that prediction that testing everything would be very difficult, I wanted to give the project the best chance to succeed. The best way that I know of doing that is to fully commit to making the right changes to solve the problems properly. For the first set of changes, this meant working with what I had, as it was only the token height calculation that needed that logic. For the the second set of changes it meant changing all the tokens, because there really was not any alternative. In a strange way, both sets of changes brought me peace, increasing my confidence about the project. In the grand scheme of things, the iterative calculations for the height of the two Leaf Block tokens was not too expensive, and it is localized to that one module. Ammortizing the cost of those calculations over all the Leaf Block tokens makes it even less expensive. From that point of view, those changes were definitely cost effective in my eyes. And while people can say I am pedantic and a perfectionist, I somewhat liked spending that time going through each scenario and verifying it. Before that review, I had a lot of trust in those consistency checks, but there was always a question of whether I had missed something important. With the existing checks in place and a manual review of those scenario tests, the chances of any major issues being left are drastically reduced. I will never say that any piece of software has zero bugs in it, but I do know that I feel that I am eliminating many of the paths for bugs to form in this project. And that I am confident about! What is Next? After completing the token height verification for block tokens, it was time to start working on the line/column numbers for the inline tokens. I was not sure how much of a chore it would be, but it would be gratifying to get them done!","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/08/31/markdown-linter-adding-consistency-to-token-heights/","loc":"https://jackdewinter.github.io/2020/08/31/markdown-linter-adding-consistency-to-token-heights/"},{"title":"Markdown Linter - Adding Consistency to Block Quotes","text":"Summary In my last article , I took care of completing the Markdown transformer checks for one half of the container block tokens: the List Block tokens. In this article, I tackle both the line/column number consistency checks and the Markdown transformer checks for the Block Quote tokens. Introduction While the implementation of list tokens went easier than I thought it would, I remained cautiously optimistic about adding consistency check support for Block Quote tokens. During the initial development of the line/column number checks, I noticed that the Block Quote tokens did not keep any information about removed data. After having completed the List Block token support, I knew that it would be pivotal to get that right. And that meant more work. A lot of nitpicking work. To add the consistency checks, I was going to have to accomplish 3 things: capture the removed line start text, use that information to validate the line/column numbers, and then write the Markdown transformations for it. The last two items were the easy part, those parts I had confidence I could complete. It was the capturing of removed checks that I was not confident about. And the capturing of text was the change that I needed to tackle first. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 12 Aug 2020 and 14 Aug 2020 . Let's Talk About the Obvious I am generally very disciplined when it comes to writing my blog. I keep notes when I am writing the code, curating those notes into an outline on the Thursday to Saturday before I post my article. If I am lucky enough, I will start the actual writing of my article on Saturday night, but most often I start writing it early Sunday morning. This allows me to take my time to enjoy my Sunday while ending up with a solid version of the article by Sunday night. Then, on Monday night, I take that article and add extra code samples and do some wordsmithing to form the article into its final form. It may not be the way others write their articles, but it is a process that works for me. Until this week that is. Everything was on track until Monday, when I started my edits with a headache that I just could not ditch. But process is process, so I started doing those edits, pushing on until a couple of hours later. At that point, I looked at what I was doing and went… huh? I started looking at my edits and tried to figure things out, using even more edits to fix any problems I encountered. Far from making things better, I was making them worse. But I would not see that until later. It was a while later when I finally stopped and locked my computer. Above all else, this was the best thing that I did in that entire situation. It took me a while, but I stopped. However, by that point, a certain amount of damage was already done. I had written some things twice and other things not at all. In some cases, I had sentences that looking at them later, I wonder if I had written them while drunk. I knew I could recover from that position, but it would require a clear head, some extra time, and some hard work. And it was not going to happen that night. I came to the realization that I was not going to be able to meet my usual posting deadline of Monday night. At first, I was angry with myself. Many weeks of not missing a posting deadline and now I had a bad record. \"I always post on Monday nights, except for…\" Argh! But after thinking about the purpose of my blog, to educate and help others, I started to find peace with it. None of us is perfect, and we all need to take care of ourselves. Me included. My health versus my article's deadline. I needed to take that extra day and get better before finishing… er… unfinishing and then refinishing the editing. Capturing the Leading Block Quote Text In its simplest form, the handling of container block elements and their tokens is a relatively easy concept. Every line of a Markdown document enclosed within a container block element is usually prefaced with one or more characters that denote the presence of that container element. For Block Quotes these are character sequences with the > character and an optional whitespace, and for Lists these are whitespaces. To make the processing of those contained lines easier, at the start of the project I took some up-front time to ensure that the container block element processing is independent of the leaf block element processing. As such, I was able to work on that leaf block processing independently of container block processing. When I was doing the initial research for container block support , I started with List tokens. I quickly wrote a proof of concept for the line/column number check and was happy to find that I had already placed the required information in the token. However, after using the same process with Block Quote tokens, it was evident that I had missed the mark with them. Unlike the List tokens, none of the required information was placed within the Block Quote tokens. At that time, after having gone down a rabbit hole for a bit , I wisely decided to wait until later to implement that. Well, that time was now and I knew it was going to take some work to add it. Addressing the Issue Like the work I did for List Block tokens, the Block Quote stack token needed a change to support the matching_markdown_token field. This field is pivotal to inform the processor of the latest markdown token that is containing other tokens. But to properly use this field, it would take a bit of disassembly and reassembly. Before this change, the code to add the necessary tokens to the two relevant stacks were: parser_state . token_stack . append ( BlockQuoteStackToken ()) ... container_level_tokens . append ( BlockQuoteMarkdownToken ( extracted_whitespace , adjusted_position_marker ) ) Two different tokens and two different stacks, with nothing between them. With this new functionality in place, that code needed some slight changes: new_markdown_token = BlockQuoteMarkdownToken ( extracted_whitespace , adjusted_position_marker ) container_level_tokens . append ( new_markdown_token ) parser_state . token_stack . append ( BlockQuoteStackToken ( new_markdown_token ) ) Instead of two distinct tokens, there was now a stack token that included a reference to the Markdown token that it represented. With that field properly initialized, the add_leading_spaces function was then added to make use of that new field. It is a simple function that adds any extracted text (usually whitespace) to its field, separating any new text from the existing text using a newline character. The function itself is very boring. The interesting part about the function would be in locating where that function needed to be called from and using it properly. The First One Is Almost Always Easy The first location was obvious: within the __handle_block_quote_section function of the BlockQuoteProcessor class. This is where the bulk of the processing of Bulk Quote elements and their effects go through. In there is an easy to find block of code that records the number of characters removed and resets the string to exclude those characters: line_to_parse = line_to_parse [ start_index :] removed_chars_at_start = start_index That block of code was slightly modified to record that removed text and place it into its own variable: removed_text = ( line_to_parse [ position_marker . index_number : start_index ] ) LOGGER . debug ( ... ) line_to_parse = line_to_parse [ start_index :] removed_chars_at_start = start_index With the removed text in hand, the code just needed to know which token to associate the removed text with. As the code processes Block Quote elements, it was reasonable to assume that the most relevant Block Quote stack token is the last one on the stack. That stack token was easily found with a simple for loop: found_stack_token = None for stack_index in range ( len ( parser_state . token_stack ) - 1 , - 1 , - 1 ): ... if parser_state . token_stack [ stack_index ] . is_block_quote : found_stack_token = parser_state . token_stack [ stack_index ] break With the extracted text and the ‘top' stack token, the only thing that was left to do was: found_stack_token . matching_markdown_token . add_leading_spaces ( removed_text ) Almost right away, I was off to a good start on extracting the leading spaces for the Block Quote token and storing them. Locating the Next Issue After those changes were in place, I ran the scenario tests again, seeing almost every test that has a Block Quote token fail. My heart dropped. But after a second, I realized that I wanted that to happen. As I wanted to make sure each Block Quote token was verified, I added the leading space data to the end of the string for the BlockQuoteMarkdownToken class, separated from the rest with the : character. If things worked properly, that meant every Block Quote token would, at the very least, now include an extra : character. Every serialized Block Quote token was now \"wrong\", so every Block Quote scenario test should fail. Good! Working through each case was decently fast, with a solid methodical process in place: look at the results, find the next failing test, and manually determine what change needed to be made to the Block Quote token. After making that change to the test data, I would then re-run that specific scenario test, and check for token differences. It was by using this process that I found the next issue: missing leading whitespace. In some cases, the leading text that was extracted was preceded by one of more whitespaces. Those whitespaces were stuffed into the extracted_whitespace variable and then ignored after that. The resolution to that issue was simple. Instead of only adding the leading space to the removed_text variable, the contents of that extracted_whitespace needed to be added to the variable, as such: removed_text = ( extracted_whitespace + line_to_parse [ position_marker . index_number : start_index ] ) Running the tests again, a new block of tests started passing. And Next… Blank Lines As more and more of the Block Quote scenario tests were changed and started passing, there were a handful of tests that were failing that I left for later. When I got to the end of those tests, I went back and started to look at those failing tests one-by-one. The first group of failures that I examined were ones in which there was a Blank Line element contained within a Block Quote. A good example of this is example 222 : > foo > > bar In these cases, the parsing was correct, but the newly added add_leading_spaces function was not being called. The outcome was that the number of newlines contained within the owning Block Quote token did not match the number of lines within the Block Quote element itself. To ensure that those two values lined up, the add_leading_spaces function was called with an empty string, thereby evening up those values. Running the scenario tests again, all the scenario tests explicitly for Block Quotes were passing. I then ran the scenario tests again, checking to make sure that the scenario tests with Block Quotes and Blank Lines passed. While the new changes were now passing, there were still a handful of tests to work on. And Finally, HTML Blocks Having run the scenario tests again, the only tests that were not passing were scenario tests that included Block Quotes and HTML blocks. Doing some research into the issue, I quickly found that it looked like the same issue with the Blank Line elements, just with HTML blocks. This issue was uncovered through one of my additional tests, \"cov_2\" used to increase coverage on the various forms of HTML start and end tags: </hrx > </x-table> Like the previous issue with Blank Line tokens, the number of new lines were not lining up between the two sources. Once again, calling the add_leading_spaces function with an empty string in these cases solved the issue. Closing Out the Token Changes After those changes has been completed, it was obvious that all the scenario tests were passing. Just to be sure, I manually went through each of the scenarios and verified the newly changed tokens. As I have said before, maybe I am paranoid, but if I was depending on those results, I figured an extra pass or two would not hurt. It just felt good to get these changes completed. I had a high degree of confidence that I was able to find most of these issues, but an even higher degree of confidence that the remaining work would flush out anything that I missed. It was with those positive thoughts in my head that I started working on the consistency checks. Validating the Line Numbers and Column Numbers With that confidence, I started working on the consistency checks for line numbers and column numbers. The first step in doing this was removing a couple of lines of code that prevented the consistency checks from firing if one of the tokens was a Block Quote token. It was useful when I did not have anything in place, but now it would just get in the way. After that change, tests predictably started failing, but I was ready for them. From the work that I had just completed, I figured that I would have to follow a similar path in implementing the consistency check. Therefore, the first thing I did to help with the consistency check is to reset the leading_text_index field of the BlockQuoteMarkdownToken instance to 0 when it is encountered. This made sure that the tracking of the leading spaces would always start with the first entry. With that done, the code needed to take advantage of that was easy to write. When a token exists on a new line, it's indent must be determined by tracking the size of the removed text and adding that measurement to the column number. Based on the previous work, this becomes trivially easy: split_leading_spaces = top_block . leading_spaces . split ( \" \\n \" ) init_ws += len ( split_leading_spaces [ top_block . leading_text_index ]) Basically, grab the top Block Quote token, split it into lines, and use the leading_text_index value of that topmost token to grab the right leading space. From there, the new work mirrored the work that I did in preparing the tokens. When a Blank Line token is encountered, that index is incremented. For each line of text within an HTML block, that index is incremented. And in addition, that leading_text_index field needed to be tweaked at the end of HTML blocks, Atx Heading blocks, and Paragraph blocks, which was something I figured might come up. Just some simple cases where a bit of extra finessing was needed. To be clear, I was not 100% percent sure that this would happen, but I was hoping that it would. To me it made sense that if I needed to add code to track the leading spaces that were removed, any consistency check would need to follow that same path to verify the information. And as to the extra tweak for the end tokens, I was confident that it was a solid change, and was not worried about that deviation from the previous work. Writing the Block Quote Markdown Transformations Given the work I had just completed to get to this point, it was obvious to me that I was going to have to do a lot of fiddling with spaces to get the transformations correct. To make that job a bit easier, I decided for the moment to eliminate any Markdown documents which contained both Block Quote elements and List elements. I would get back to these quickly, but at that moment, it was just too much. The processing of the start and end of the Block Quote elements were simple, largely mimicking the work I did for the List elements. When a Block Quote token was processed, it created a copy of itself and added that copy to the top of the container_token_stack list. From there, the leading space were retrieved from their storage in the token and added to the sequence to be emitted. The end of the Block Quote element was even easier, returning an empty string after removing the top token off the container_token_stack list. The work I had previously done on setting up the leading spaces was really paying off. With the work to recognize and process the actual tokens taken care of, the main task ahead of me was to add and populate the __perform_container_post_processing_block_quote function. Like how List Block tokens were handled in the __perform_container_post_processing_lists function, this new function was used to handle the newline processing for text enclosed within Block Quote elements. After having completed all this other work, that work was relatively simple to do. With all the individual token processing already performed, this function just needed to focus on detecting newline characters. For each of these characters encountered, the top Block Quote token would be used to determine the current leading_text_index to start with. With each newline encountered, the current split value would be used, incrementing the leading_text_index value afterwards. This pretty much worked flawlessly out of the box. As I was cleaning up An interesting issue came up as I was generating these transformations. For some reason, I had missed some cases involving Block Quote elements that contained Fenced Code Block elements. With the Markdown transformer now rehydrating the Markdown, it was obvious that things were missing. It did not take me long to figure out that the Fenced Code Blocks were missing their Block Quote leading characters. This was found for example 98 : > ``` > aaa bbb When the transformer tried to rehydrate the document, it rehydrated that Markdown text without the leading > sequence before the text aaa . Having just gone through that research for other elements, I was quick to spot it and realize where the issue was. A couple of quick changes, and that change was passing! Along the Way And to hammer home the point that this consistency checking is good, it found some issues along the way. The first one was an interesting issue where the starting whitespace for one transformation was missing. And it was such a weird case. It was an Atx Heading element that contained a Shortcut Link element as its only text, separated from the Atx Heading character ( # ) by the mandatory single space. That is really important. Two space it was okay with, but the one space, nope! Due to the way that it was parsed, that spaces were missing and not accounted for. The fix to this was to add that starting whitespace as a new token containing that removed text, but with a twist. That twist was to use the create_replace_with_nothing_marker function to add that text as itself for the Markdown transformer, but keep it as removed for the HTML transformer. With both transformers appeased, it was on to the next issue. The second issue that I uncovered was that, in rare cases, the leading spaces for certain lines were missing. After doing some digging, it only appeared to be lines that did not have any block quote prefix characters removed from the line. So after adding a new __adjust_paragraph_for_block_quotes function to the LeafBlockProcessor and wiring it into the parse_paragraph function, the debug confirmed that it was only those few cases where that was an issue. And Of Course, Some Cleanup It would not be a proper week of coding if I did not do some cleanup. In this case, the cleanup was simple: relocating the line/column number logic to its own module. One of the things that made the Markdown transformations easy to deal with is that those transformations are in their own TransformToMarkdown class. That worked well. The line/column number checks were in with some other validation code in the utils.py module. That worked… well… okay? When I started with the consistency checks, they were new, and keeping all that code in the utils.py module made sense. But as the amount of code in the module grew, I never took the time to to some refactoring. As such, the module developed two responsibilities. The first was to be the location for all the assert_* functions and the write_temporary_configuration functions. Those are all the functions that are called directly from the tests, mostly in the final Assert stage of the test. The second was to house the logic for the line/column number consistency checks. It just seemed logical, now that that part of the code was stable and well-tested, to take that second responsibility and put it into its own module. I created the verify_line_and_column_numbers.py module and started moving the functions into it, with the verify_line_and_column_numbers function being the main entry point for the module. It just seemed cleaner and more compact. One module, one responsibility. What Was My Experience So Far? The dominant part of my experience at the current moment is one of humility and patience. Yes, it is Tuesday night. And after a little over 3 hours of extra work, I have finally recovered the article to where it was at this time last night. I do feel an impulse to be hard on myself about this delay, but I also am working hard to remember to be patient with myself. This is one time where I am missing a deadline, and it probably will not be the last. The humility that I am trying to practice is in understanding that I cannot do everything all the time. I know that sounds like it should be an obvious thing to know, but I think we all forget it from time to time. I was so focused on making sure that I met my deadline, that I neglected to have a conversation with myself on whether that was the right choice. But it was not a large rabbit hole, just a small one that I was able to easily recover from. Focusing more on the actual work that I accomplished, I was buoyed. I was long worried about how hard it would be to implement the consistency checks for Block Quote tokens. Having completed that work, I now am trying to figure out why I was worried. Trying to figure it out now, I would guess I would focus on what it took to complete that work with lists. That work was very nitpicky because it contained a lot of \"empty\" whitespace, if I am remembering it clearly. From the effort that it took to deal with that, I can see how I might have thought it would have taken the same effort for Block Quote tokens. But it did not. The work I had done in that area on List tokens forced me to get things set up to make List token processing easier, which I believe Block Quotes benefited from. Regardless, I was glad that I could close the books on the Block Quote tokens and their consistency checks. What is Next? With all the block token consistency checks now taken care of, there was a bit of clean up to do with determining the height of each block token. While I initially thought that it would be easy, it did not turn out that way.","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/08/25/markdown-linter-adding-consistency-to-block-quotes/","loc":"https://jackdewinter.github.io/2020/08/25/markdown-linter-adding-consistency-to-block-quotes/"},{"title":"Markdown Linter - Adding Lists to the Markdown Transformer","text":"Summary In my last article , I increased the coverage provided by the token to Markdown transformer by adding support for the link related tokens. In this article, I take another large step towards completing the consistency checks by adding support for list related tokens. Introduction Having implemented the link related tokens, I was now down to one large group: the container related tokens. Even with the confidence I gained from the work that I performed with link related tokens, I felt that \"container related tokens\" was too large of a group for me to be comfortable working with. Given that the container related tokens group contained only 2 types of tokens, it only seemed natural to focus on one of those two tokens: the list tokens. More specifically, I was going to start with the Unordered List tokens. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 04 Aug 2020 and 08 Aug 2020 . Starting with Unordered Lists Taking a wander through the scenario tests with unordered lists in their examples, two things were clear to me. The first thing is that with few exceptions, they were all working well from the point of view of existing consistency checks. Those few examples were 1 case with nested links and 4 cases which mixed up block quotes with list. Those examples are example 528 , example 237 , example 238 , example 270 , and example 271 . The second thing that was clear was that because of the first point, I had a high degree of confidence that I was going to be addressing issues that almost exclusively focused on whitespace. With the HTML output already being verified, I was certain that properly transforming Unordered List tokens back into list Markdown would mostly boil down to whitespaces. Thinking About the Design Before I leapt into the coding, I sat back and thought long and hard about the approach I was going to take with the transformation of these tokens. When I started sketching out what my approach would be, I started to understand that there would be two issues I would have to deal with. They were the transformation of the tokens themselves, and the whitespace they would inject before other tokens. The first part was the easy part: see an unordered list token, use the elements in the token to figure out its transformed form, and emit that transformed form. Done. Managing the whitespace was going to be a lot more difficult. The thing that helped me was that I knew I already had experience with handling that initial whitespace from writing the main parser. What helped me immeasurably in the parser was to keep the processing of the two container elements, lists and block quotes, separate from the processing of the leaf tokens. By only passing \"container element free\" text to the leaf token processor, that processor was kept simple. To keep the container handling for the Markdown transformer simple, I decided that employing the same approach was pivotal. But even with the decision to keep that processing separate, I figured that it would only get me part of the way there. To complete the management of the whitespaces, I would need to be able to calculate the whitespace used before each line contained within a list block. The more I looked at the problems to be solved, the more I was sure that most of my time was going to be managing that initial whitespace. It was not going to be fun getting the transformations done for the Unordered List tokens, but I had a feeling that it would be really satisfying when it was done! And… Go! I began this block of work with the moving of the if statement that avoided processing any scenario test that included block quotes or lists starts. Before that move, it was buried in a large collection of if statements, and I wanted to make sure I called it out until it was fixed. Making it even better, when I moved it, I broke that single statement into three explicit statements. As I knew I was going to be enabling each one in the next week or so, being explicit about that work just seemed like the right thing to do. But even though the move was mostly for aesthetics, I confess that it was also to remind me that I only had those tokens left to go. Once that was completed, I did the easy work first and added the rehydrate_unordered_list_start function and the rehydrate_unordered_list_start_end function to the main processing loop. After running the scenario tests again, I was reminded by the test output that the rehydrate_next_list_item function would have to be added to deal with the Next List Item tokens in the stream. Another quick run of the tests and a lot of comparison failures, but no missing token failures. A good step in the right direction! First Step: List Indentation With the actual token handlers dealt with, it was time to focus on the effects that those tokens had on the leaf blocks. Following my usual pattern, instead of immediately creating a new function to specifically handle the lists, I kept that code inline with the existing transform method the Markdown transformer. While I recognize that it looks messy and sloppy and the outset, it helps me think more clearly without worrying about that I need to pass where. Therefore, following my usual pattern, I first added a simple post-processing step that took the contents of the variable continue_seq and applied them to start of the output of specific tokens. The continue_seq variable was initialized with an empty string, but I altered the rehydrate_unordered_list_start function to reset this variable to the amount of indent specified by the list. With the change in place, the end of the loop processing was simple: new_data = new_data . replace ( \" \\n \" , \" \\n \" + continue_seq ) This gained some traction in getting the scenario tests passing, but that processing needed to be made a bit more complicated. The first complication that needed to be addressed was that both list starts, and list ends modified the continue_seq variable, but needed to avoid applying it to the line on which they element resided. This was because the processing of the Unordered List start token already had the required indenting taking care of, so the postprocessing would just add extra \"garbage\" whitespace. To remedy this, I added the skip_merge variable to allow the Unordered List token handlers to inform the main loop to skip any post-processing. The second complication was the handling the list terminations using the rehydrate_unordered_list_start_end function. In some of the easy cases, what was there was fine, but as soon a list was nested in another list, that processing fell apart. What was missing was a recalculation of the indent level once the prior list ends. That was quickly addressed by doing a recalculation of the contents for the continue_seq variable for the new list token at the top of the stack. With those easy fixes, and with the main replacement call in the main loop, a lot of the scenario tests were passing, while keeping the processing simple. That simplicity would soon change. Indented Code Blocks As I went through the test failures, there were a few failures that stood out as an odd grouping: indented code blocks and lists. Doing some more research, I found out that due to a bug in my code, the indented code blocks were not being checked properly. It only involved one list item scenario test, but nonetheless, it still needed to be fixed. In that newly found bug, the problem was that the indented code blocks were always adding their indented padding at the beginning of the lines. This was not usually a problem, but with any examples that contained blank lines within the indented code block, it was an issue. A good example of this is example 81 : chunk1 chunk2 { space }{ space } { space } { space } chunk3 When the parser tokenizes this example, the Blank Line tokens that are generated already include any whitespace that is present on that line. Taking care of their own whitespace data, when the Markdown transformer interprets those Blank Line tokens, it needs to accept those Blank Line elements as they are. Modifications were needed to enforce this behavior. The combine function of the TextMarkdownToken class containing the indented blank line was changed to insert a NOOP character and then a newline character. As text used in an indented code block was the only paragraph-like encapsulating token that inserted a blank line into the composed text, had confidence this was isolated. With those NOOP characters in place, the Markdown transformer needed some modifications to understand how to deal with this. Before proceeding with the normal insertion of any whitespace in the continue_seq variable, a quick check was made to see if the new_data variable contained a NOOP character. If so, the string in the new_data variable was split and processed. For each element in the split list, the element was checked to see if it started with a NOOP character. If it did it simply removed the NOOP character by setting the replacement_data variable to the contents of the new_data variable after that first NOOP character. If it did not find it, it set the replacement_data variable to the contents of the continue_seq variable plus the contents of the new_data variable. Once that was done, the value was put back in into the array at the same index. Then, when the processing was done, it reconstituted the new_data variable by joining the elements of the list back together using the \\n character as a joining character. While I was not looking for that, I was glad I found it. A bit embarrassed that I did not find it right away, but I did find it! Handling Lazy Continuations Lines With most of the scenario tests now passing, my focus was centered on a set of tests that dealt with lists and lazy handling. While this took me a bit to get my head around, it basically says: If a string of lines Ls constitute a list item with contents Bs, then the result of deleting some or all the indentation from one or more lines in which the next non-whitespace character after the indentation is paragraph continuation text is a list item with the same contents and attributes. The unindented lines are called lazy continuation lines. Huh? Let me translate: If you have a list already started, and you encounter lines that should be in the list except for the fact that they are not indented properly, they are lazy continuation lines, and are included. The best way to explain is with an example: 1 first list item next line of first list item next next line of first list item In that example, all three of those lines are considered part of the list item, even though the line is indented less than the indent of 2 specified by the initial list item element. But that presented a bit of a problem. When parsing the Markdown document, the indent level was being tracked, and any additional whitespace was added to the contained token. But as I was writing the Markdown transformer, I noticed that I had missed the case where the amount of indent was less than the current list's indent level. This was not an issue with the HTML transformer, as that transformer does not rely on any of the extracted whitespace. However, the Markdown transformer does. To fix this issue, I needed to first make a change in the parse_paragraph function of the LeafBlockProcessor class. In that function, I reconstituted the actual indent of the given line and compared it against the indent level from the dominant unordered list item token. If that actual indent level was less than the dominant indent level, I adjusted the actual whitespace by prefacing that whitespace with…well… blech characters. Yes, blech characters. Blech, according to Webster's means \"used to express disgust\". While I knew I had to track those indenting characters somehow, I really was not happy with it. Disgust may be a bit too intense of an emotion, but when I figured out what I had to do, that was the most printable word that I uttered. Using the above example, the tokenized text looks like: - first list item{newline} {blech}{blech}next line of first list item{newline} {blech}next next line of first list item{newline} In this way, the indent was properly represented in the token, and the Markdown transformer had enough information to rehydrate the data afterwards. With those changes locked into the tokens, the Markdown transformer was then able to be changed to understand those tokens. That processing was simple. If a line of the text output started with a blech character, those blech characters were replaced with a space character. If no blech characters were there, the normal replacement of the newline character would occur. And I could have changed the name of the character from \"blech character\", but after a while, it just kind of stuck with me. New List Item Tokens It was about this time when I decided to tackle the New List Item tokens. While I had been working around them somewhat, it was starting to get to the point where they were unavoidable. At first, I did not think these tokens would be an issue, but I forgot about one little part of the New List Item tokens: they reset the indent for the list. A good example of this is example 290 : - a - b - c - d - e - f - g In this case, the first line starts a list and each line after that starts a new list item. As the new list items are gradually increasing and then decreasing the indent, the 3 middle lines ( c , d , and e ) are interpreted as a new list item element, rather than a sublist. If it was not for the new list item elements resetting that indent, those 3 lines are indented to the point where their indent would make them eligible to start a new sublist. But to properly track this indent change, it required some interesting thinking. If I tracked the indent change in the actual token, it would mean changing that token. To me, that was a non-starter. Instead, I added a separate stack for container tokens in the Markdown transformer and added copies of the container tokens to this stack. As I added copies of the tokens to the stack, I was free to change the indent value located within the Unordered List token without worrying about side effects. With those changes in place, the Markdown transformer was able to reset the indent level and make sure it was being properly tracked. This meant that the indents were able to be properly reset to the correct value once a List Item end token was received for a sublist. Taking a bit of a deep breath and a pause, I noticed that I was close to finishing off the Unordered List Item tokens. That gave me a bit of a jump in my step to clean things up! Taking Care of Stragglers With all the major and minor cases of lists dealt with, I started going through the other scenario tests, fixing up the token lists after verifying that the new tokens were correct. Everything else was easily resolved at this point, except for some lists in a couple of cases. Those cases were interesting in that there was actually too much whitespace, not too little. And in this case, it was a newline character, not a space. The Fenced Code Block element and the SetExt Heading element are unique in that they have a line-based sequence that delimits the end of the element. Usually this is not a problem, but in the case of interacting with lists, the transformer was inserting a newline after the element itself, and then another newline to make the end of that line. While this duplication did not occur all the time, it took a bit to figure the exact sequence that triggered this. After doing some research, it was weird to me, but it only occurred if: it was one of these two elements the new block of data ends with a newline character the next token to be processed is a New List Item token While the sequence of thing that had to occur was weird, the solution was easy: block_should_end_with_newline = False if next_token . token_name == \"end-fcode-block\" : block_should_end_with_newline = True delayed_continue = \"\" elif next_token . token_name == \"end-setext\" : block_should_end_with_newline = True ... block_ends_with_newline = \\ block_should_end_with_newline and new_data . endswith ( \" \\n \" ) ... if ( block_ends_with_newline and next_one and next_one . token_name == MarkdownToken . token_new_list_item ): new_data = new_data [ 0 : - len ( continue_seq )] Basically, if we hit that situation, just remove the excess character. I was hoping to refactor it into something more elegant, but it worked at the time and I wanted to get on to the handling of Ordered List Item tokens. Second Verse… I fondly remember being a kid at a summer camp and hearing the words \"Second verse, same the first, a little bit louder, and a little bit worse!\". Working on the ordered list tokens made me think of that saying almost immediately. Except for the fact that it was not a little bit worse, it was a lot easier. There were two main reasons for that. The first reason is that looking at the samples as a group, there are objectively fewer examples with ordered lists than unordered lists. In the list items section of the GFM specification, there are 20 of each, but in the lists section of the specification, there are 20 examples of unordered lists and 7 examples of ordered lists. The second reason was that most of the work was either completed when working on the unordered list token transformations, or it was used in a copy-and-paste manner. However, knowing that lists are one of the two container elements in Markdown, I took some extra time and reverified all the scenario tests, both ordered lists and unordered lists. I was able to find a couple of small things that were quickly fixed, but other than that, everything looked fine! Cleanup As with a lot of my free project time recently, I used some extra time that I had to focus on a few cleanup items for the project. While none of them was important on its own, I just felt that the project would be cleaner with them done. The first one was an easy one, going through the HTML transformer and the Markdown transformer, and ensuring that all the token handlers were private. There really was not any pressing need to do this, but it was just cleaner. The only code that was using those handlers was in the same class, so it was just tidier that way. Next was the creation of the __create_link_token function to help tidy up the __handle_link_types function. The __handle_link_types function was already messy enough handling the processing of the link information, the creating of the normal or image link was just complicating things. While I still want to go back and clean functions like that up, for the time, moving the creation code to __create_link_token was a good step. Finally, there was the case of the justification function calls throughout the code. Not to sound like a purist, but I felt that they were messy. I often had to remind myself of what the three operands were: string to perform on, the justification amount, and the character to use for the justification. The actual use of the function was correct, it just felt like its usage was not clear. So instead of having code like this around the code base: some_value = \"\" . rjust ( repeat_count , character_to_repeat ) I replaced it with: some_value = ParserHelper . repeat_string ( character_to_repeat , repeat_count ) While the code for this operation was a one-line function, now located in the ParserHelper class, I felt it now made sense and was in an easy to find place. def repeat_string ( string_to_repeat , repeat_count ): \"\"\" Repeat the given character the specified number of times. \"\"\" return \"\" . rjust ( repeat_count , string_to_repeat ) \"Fixing\" Example 528 I do not want to spoil the surprise too much, but the fact that I have a section called \"Fixing\" Example 528 probably gives it away. I fixed it. But the story is more interesting than that. In the last article, I talked about example 528 and how I was having problems getting it to parse properly . Even having done thorough research on the example and the algorithm, I came up with nothing. To me, it looked like the parsing was being performed according to the GFM specification's algorithm , but the parsing was just off. After yet another attempt to figure this example out and get it working, I posted my research and a request for help to the CommonMark discussion forums. . Keeping my head down and focused on writing that week's article, I did not notice that I had received a reply the very next day. As a matter of fact, while I did notice that I had a response to my post, it was not until Friday night that it clicked that it was a response to \"THAT\" post. After getting the cleanup documented in the previous section taken care of, I reserved some dedicated time to walk through the reply. Kudos First off, I would like to extend kudos to the replier, John MacFarlane, one of the maintainers of the Markdown GFM specification. While he would have been within his right to tell me to RTFM 2 , he took some time to walk me through the algorithm as it applied to that example, even providing me with some debug output from his program for that example. His response was just a classy response with just the right amount of information. Side by Side Comparisons Armed with that new information, I turned on the debug output and ran through the output from my implementation of the algorithm again. Slowly, with my own written notes as an additional guide, I began to walk through the output. Found closer at 8 . Check. Found matching opener at 4 . Check. Deactivating opener at 3 . Check. Found closer at 15 . Check. Popping inactive opener at 3 . Ch…er… what? \"Popping\"? Going back to the algorithm and the text that John provided, it hit me. The popping that he was referring to was this part of the algorithm: If we do find one, but it's not active, we remove the inactive delimiter from the stack, and return a literal text node ]. For some reason, when I read that before, I thought it was referring to removing the token from consideration, not actually removing it from the stack. It probably also confused things in that I did not maintain a separate stack for the link resolution. Instead, I added an instance of the SpecialTextMarkdownToken token to the inline block list whenever I hit a link sequence. In either case, I was not doing that. To compound the issue, I did not stop at that inactive token, I just kept on looking for the next active SpecialTextMarkdownToken token, finding the image start token. Ah… everything was now falling into place in my mind. Fixing the Issue The fix was very anticlimactic. I created the new __revert_token_to_normal_text_token function, which removed the SpecialTextMarkdownToken token and replaced it with a normal TextMarkdownToken token. In addition, I changed the algorithm to make sure that when this happened, it stopped processing for that link end sequence, as per the algorithm. With the start character sequence now being effectively invisible to the algorithm, the rest of the parsing went fine, with the correct results coming out. Well, almost. A small fix was needed to the __consume_text_for_image_alt_text function to make it properly emit the correct text if an instance of a SpecialTextMarkdownToken token was encountered. With the big fix and the little fix both completed, the scenario test for Example 528 was fully enabled and fully passing. Finally! Reminder to Self: Be Humble Having taken a quite a few attempts at implementing the algorithm and making sure it passed all test cases, I hit a wall. A seemingly rock-solid wall. That was fine. During any project, be it software or hardware, things happen. When it got to that point, I gave myself some time, I knuckled down 3 , and I did some solid research on the problem. Keeping good notes, I was then able to share those notes with peers in the community, along with a sincere and humble request for help. I do not always get a good response to requests for help. However, I have noticed that doing good research and presenting that research with humility increases the chance of getting a positive response. At no point did I rant and say, \"it's broken\" or \"my way works, what is wrong with yours\". Instead I said, \"Is there something wrong with the algorithm?\" and \"Based on my implementation of that algorithm\". By acknowledging that it could be an issue with my implementation, I feel that I opened the doors for someone to help, rather than slamming them shut with negative talk. And as I mentioned in the Kudos section above, mostly in what I believe was a humble approach to asking for help, I got a real good response. What Was My Experience So Far? Wow… that work was intense. For one, I was right, it was a lot of addressing issues with whitespace and running scenario tests repeatedly. But it was more than that for me. I knew I had the leaf blocks taken care of, but I was really concerned about how difficult the implementation of the container transformations would be. If I did it right and kept to my design, I was confident that I could keep the complexity down, but I still figured it would be complex. I guess that led to me second guessing every line of code and getting in the way of myself a bit. I did prevail, but that concern or fear of damaging existing tests was somewhat paralyzing at times. And while the logical half of my brain was telling me that I had plenty of tests to reinforce my completed work, the emotional half was another story. That is where that fear was coming from, my emotional side. Only when I took a moment to take another breath and focus on the tests was I able to banish that concern for a while. And it also helped me to do a bit of self-analysis on why I was concerned. After a lot of thinking, I came to a simple conclusion. The closer I get closer to getting a complete project, the more I am concerned that I have not architected and designed it properly. If I have done that, small changes can be accomplished with few or no unintended side effects. If not, encountering side effects should be frequent. Seeing as I know I have identified some areas of the code base that I want to refactor, I questioned whether the current state was good enough. Knowing that, it helped me figure it out for myself. I do believe that I have confidence with my architecture and design, and at the same time, I believe that I can improve it. It might seem like a dichotomy, but I am starting to think that both can be correct at the same time. But knowing that was the issue that was causing me concern helps me combat it. I am a lot less worried about it, but it is a work in progress, much like the project. With that information in hand, I felt better. Cautious about the next steps in getting the checks closer to the finish line, but better! And let's not forget about finally closing the issue with Example 528. That was good! What is Next? With the Markdown transformer almost completed, the only tokens left that need a transformation are the Block Quote tokens. In addition, as the line/column number consistency checks do not currently deal with Block Quote tokens either, I will need to add both checks in the next block of work. The only example 249, but ordered list. ↩ Read The F&$#ing Manual… or I guess RTFS in this case. ↩ According to Webster's : \"pitch in, dig in\". ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/08/17/markdown-linter-adding-lists-to-the-markdown-transformer/","loc":"https://jackdewinter.github.io/2020/08/17/markdown-linter-adding-lists-to-the-markdown-transformer/"},{"title":"Python Projects - Using PipEnv","text":"Summary With almost a year of Python development under my belt, I wanted to start talking about the Python elements that I feel have made my development of the PyMarkdown project successful. When thinking about where to start, the first thing that came to mind was my use of PipEnv to maintain the project environment. Introduction While I am relatively new to Python, I am an old hat at trying to maintain a cohesive way to keep my projects usable yet portable. From my early days in C and Pascal to my current days in Go and Java, I have always tried to have a cohesive story around how to package my project's source code to be used by multiple developers. In the end, I found that the main goals were always the same: how do I keep a project usable by all interested developers, while keeping the maintenance of that project to a minimum? From my point of view, PipEnv meets those goals for Python by solving a lot of the common issues that project maintainers and developers have. This in turn makes two of the harder parts of developing programs mostly disappear: dependency management and portability. Using the simple command pip install --user pipenv to install into any Python system, PipEnv quickly became a useful tool in my toolbox and has stayed there. During this article, I will talk about why PipEnv keeps that position in my toolbox. Quick Note While I realize some projects may still be on Python version 2, this article is targeted for people developing on Python 3 and above. More precisely, these examples were all tested with Python 3.7. What Is PipEnv? While the full answer can be seen at the top of the PipEnv home page , my summary of that answer is as follows: PipEnv is a Python module that cleanly manages your Python project and its dependencies, ensuring that the project can be easily rebuilt on other systems. While that might seem like an easy thing to accomplish, PipEnv or tools like it are usually only employed after a journey through other, less efficient solutions. To understand those solutions and the problems that they present, let's start at the beginning with dependency management. Starting with Dependencies In Python, as with most modern languages, a lot of the heavy lifting is done in libraries. For Python, these libraries must be installed directly into the current instance of Python for those libraries to be visible to the Python programs. The most common way of installing and managing those libraries locally is to use the pip tool. For example, to add the latest version of the colorama library to the local Python installation, the following command is used: pip install colorama But that command has a problem in that it installs the specified library into the global instance of Python. Even addressing that problem using a user-specific variation of that command: pip install --user colorama that caching still takes effect for all Python programs that are run as that user. That is a bit better, but not optimal. The big issue is that it is hard to replicate which libraries are installed for any given project without maintaining a per-project script with many lines in it, one for each library. With multiple projects sharing the same global cache of libraries, a clear way to enforce the libraries and versions needed for a specific project is needed. Enter the requirements.txt file. Explicitly Specifying Requirements as Code The first step that many people take on their Python project journey is to create a requirements.txt file to hold all the library requirements for their project. The file has a format of: colorama=0.4.3 Pygments==2.4.2 with each library being listed on its own line, optionally followed by the version of the library to install. To apply these library requirements to a given Python environment, the command: pip install --user -r requirements.txt is used. This usage of pip is used as a shortcut for having to call pip install once for every library needed in the project. By this point in the project process, most developers understand that nailing down a specific version of the library being used is critical to the project's success. By specifying an exact version of the library to be referenced, the pip tool guarantees that it will always reference that specific version of the library, locking down the behavior of the library and making it predictable. While the tool usage is simple enough, there are problems with using the pip tool in this way. The first problem is that to ensure that I have the right libraries for my program, every time I run that program, I need to re-run that pip install --user -r requirements.txt command before I run my program. If I do not run that command, I risk the chance that another program has changed the libraries on my program, either causing the program to fail or rendering its output questionable. Even when I created a script to run the above command and my program together, I felt that the combination often feels slow, bothersome, and inelegant. The second problem is that of \"phantom\" dependencies. For argument's sake, let's assume that I am maintaining 2 projects and their dependencies. Project A has a dependency on Library 1 and Project B has a dependency on Library 2, with Library 1 and Library 2 being separate libraries. Furthermore, let's assume that both projects use a requirements.txt file and the above pipenv install method to manage their project dependencies. Because of the way these files are applied, if Project B is used after Project A, it retains access to Library 1 that was installed by Project A. After all, with nothing to remove Project A's dependencies, they stay in the user's global cache of libraries. This means Project B's dependencies are not clearly defined and may prove difficult to replicate on someone else's machine. Given those problems, how can the project move away from using global library caches? Another Step Forward: Virtual Environments The next step forward is to use the tools accumulated so far and to add virtual environments into that group of tools. Installed using the following command: pip install --user virtualenv the user can then run the following command to create a virtual environment under the current directory: virtualenv venv This command may take some time to complete, but once it is done, there will be a local virtualenv directory that contains a completely distinct version of the Python installation that it was created from. Even with that environment in place, there is some work to do before executing the project's Python program. To use the virtual environment, the execution of an activator script is required to set the shell's environment to point at the correct environment. Located in either the virtualenv\\Scripts directory (on Windows systems) or the virtualenv\\bin directory (on Posix systems) are a group of scripts with the name activate or matching the pattern activate.* . Executing the correct script activates the virtual environment for a given shell, isolating any changes to the Python environment into that virtual environment. The usage of this tool seems beneficial so far, so what are the issues with this tool? The big issue for me is that you must remember to deactivate the environment before leaving the project folder. If you do not deactivate the environment before leaving the project's directories, you can be doing something else in another directory and forget the directory that you anchored that environment to . Without noticing it, a simple pip install command will then alter the requirements of that environment, and not of the environment in the current directory. This is a realistic scenario. In my early days of Python development, I did this numerous times! And each time, it took a while to figure out what I had done and how to reverse it. A smaller issue with these environments is that they are specific to the local system but are anchored within the project. This means that when dealing with version control systems such as Git, the project needs to explicitly ignore the files in the project's virtualenv directory to prevent local environment files from being committed. However, even with that directory ignored, the project requires extra scripts as part of its project code that specify how developers need to create the required virtual environment. Given those issues, how do we take the best parts of pip and virtualenv and merge them together? Enter PipEnv Installed using the following command: pip install --user pipenv PipEnv combines the best aspects of the previous concepts while getting rid of a fair number of the problems. To illustrate this, I created a sample project from scratch to say, \"Hello world!\" Setting up the project was easy. I created a new directory, changed my current directory to that directory, and entered the command: pipenv --three It took a while for the command to complete, but upon its completion I had a directory that contained the file Pipfile and a \"hidden\" 1 virtual environment. To see the location of that virtual environment, I entered the command: pipenv --venv and PipEnv returned the path to the virtual environment. To be specific, it created a virtual environment that was not in my project directory, but in my local system's user space. This meant that the virtual environment directory did not show up in any scans of the project directory, meaning that I did not have to ignore that directory by any version control systems. That was definitely a win! From there, I decided I wanted to add a splash of color to the program, brightening up a normally dull Hello World program. While a simple Hello World program would look like this: print ( \"Hello World!\" ) I decided to go with using the colorama library to add that color. Installing the colorama library to the current project was easy, using the following command: pipenv install colorama That command looked for the latest version of the colorama library, installed it in the virtual environment, updated the Pipfile and generated a new Pipfile.lock file. Once that was completed, I created the file main.py with the following contents: from colorama import init , Fore , Back , Style init () print ( \"Hello \" + Fore . BLACK + Back . GREEN + \"World!\" + Style . RESET_ALL ) and saved that file. After a quick check for spelling and grammar mistakes, I executed that program with the command: pipenv run python main.py and I was greeted with this response: To be clear, with 3 commands and 3 lines of Python code, I was able to create a simple program that referenced a library to colorize the output for my simple program and to write a single line of text to the console. An even bigger win for me was that I knew that if I needed this project to be portable, I could easily bundle up the source in the directory and recreate the project elsewhere. Having used PipEnv for months, this was not a theory for me, this was something that I have done in practice multiple times. But unless you have performed that action, it may be hard to appreciate. So let's prove it! Proving That the Project Really Is Portable To prove that the project is portable, I created a separate directory and copied the contents of my sample directory into that directory. While that is not exactly what happens when I clone a project from Git, I believe it is a close enough estimate for the purpose of this example. To be specific, there were only 3 files in the source directory for my sample project, and all of them were copied over: main.py , Pipfile , and Pipfile.lock . To properly setup the project, I entered the directory and executed the command: pipenv sync After a while, control returned to my shell, with the pipenv output clearly detailing that it created a new virtual environment and downloaded the needed libraries. From there, entering the command: pipenv run python main.py yielded the same output as the example project in the original directory. 2 While the typical project will be more complicated than this small project, the underlying principles are the same. If a project uses PipEnv to manage library dependencies, the Pipfile and Pipfile.lock files become part of the project's source and allows the project's dependencies to be replicated in another directory or on another system. Satisfied with the portability test passing, and not wanting to be a bad consumer of system resources, I then used the following command to remove the virtual environment from my local machine: pipenv --rm By using the pipenv --venv command both before and after this command, I was able to verify that the directory containing the virtual environment was removed from my local system. Why Do I Like It? I like using PipEnv because it is simple to setup, simple to maintain, and easy to use. I am lazy in that I want to have the right tools at my fingertips to do the simple and easy stuff for me. Having to remember to activate a virtual environment when I go into a directory and deactivate it when I leave was a headache. There were many times I forgot to do just that, and it caused a bit of chaos. I feel that PipEnv keeps everything I need together and keeps it in the right place for me: the base of the project. If I need to run a Python script from the project, I use pipenv run python {script-name}.py , a format that makes sense to me. If I need to check the project's dependencies, the pipenv graph command is there, with an intuitive output format. If I need to recreate the project in a new directory, the pipenv sync command is there, and completes its job properly Basically, it is a tool that I find useful and that I think makes sense. Wrap Up While there are options out there on how to maintain Python projects, the one that I prefer is PipEnv. Newer options exist, such as Poetry , but for projects that I rely on, I want something that has been battle tested thoroughly. I feel that PipEnv is the best option that easily meets my requirements. The directory is not hidden according to the file system. However, from the project's point of view, it does not appear anywhere within the bounds of the project. ↩ You'll have to take my word that the output was the same. I am using the same picture for both, but that is only because the output was the same. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/08/14/python-projects-using-pipenv/","loc":"https://jackdewinter.github.io/2020/08/14/python-projects-using-pipenv/"},{"title":"Markdown Linter - Adding Links to the Markdown Transformer","text":"Summary In my last article , I increased the coverage provided by the token to Markdown transformer by adding support for all tokens except for container related tokens and link related tokens. In this article, I take a large step forward towards complete consistency checks by adding support for link related tokens. Introduction Starting with a large group of tokens to implement, I was now down to two smaller groups of tokens left to implement in the Markdown transformer: container related tokens and link related tokens. Before starting with the container related tokens, I wanted to make sure that all the leaf block tokens were complete, so the link related tokens were the solid choice for the next block of work. But even with confidence in that choice I was still concerned. Why was I concerned? Because outside of container related tokens and text tokens, I feel that link related tokens are the most used group of tokens. While a good argument can also be made that Atx Heading token is the most used token, I feel that the importance of links in any document somewhat overpowers the argument for the Atx Heading token, if only a bit. In my own writing, I believe headings are useful, but I feel that it is the links that I add to a document that really increase the quality of my documents to the next level. It is possible that others may not agree with my reasoning, but it was one of the sources of my concern. Another reason for my concern? Links are complex. Just on a normal link alone, I counted at least 12 different properties that I am going to have to represent in the token to allow me to properly rehydrate it. And then there are the Link Reference Definitions, the only multiline element in the base GFM document. I hoped that I already had dealt with most of the complexity of that token, but I knew that adding support for this group of tokens to the Markdown transformer was going to include some serious work. Regardless of how often the link related tokens are used or how difficult I thought they would be to be implemented, they needed to be implemented. And as the last group of tokens before the container tokens, the time was now to work on them. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 31 Jul 2020 and 31 Jul 2020 . Where to Start? Before I started making any changes, I knew I was going to take a significant amount of time to complete the work on links. I also knew that all that work was not going to just happen. To get it right, I needed to plan it out properly. I started that process by breaking down the Link tokens groups into 4 smaller groups: Inline Links, Reference Links, Link Reference Definitions, and Image Links. The Image Link tokens were the first one to get prioritized into the first position, as they had no dependencies and could set the foundations for the other groups. Next, I looked at the Image Link tokens. As Image Link tokens were normal Link tokens with a couple of differences, it made sense to me that they would go last. By going last, the link token foundations would be tested and stable before adding support for the Image tokens on top of that. That just left the Link Reference Definition tokens and Reference Link tokens. The ordering between Link Reference Definition tokens and Reference Link tokens was going to be a bit tricky, but it would be a process that I knew I could manage. To ensure that I could properly test the Link Reference Definition tokens, I needed to start with a rudimentary rehydration of the Shortcut Link token. Once that was done, I could complete the work for the Link Reference Definition tokens, hopefully not hitting any difficult Shortcut Link tokens cases or other link types along the way. At that point, I could switch back to the Shortcut Link token scenario tests before completing the other Link scenario tests. With research done, the relative ordering of the tasks was easy. Start with Inline Link tokens with their lack of dependencies. Then work on the pair of Link Reference Definition tokens and Reference Link tokens, using the Inline Link tokens as a foundation. Finally, work on Image tokens using all that other work as a solid foundation to make the changes required of the transformer. It was not a complicated plan, but it was a decent plan that I believed in. And with that plan in place, I started to work on Inline links! Inline Links In the same way that I start all my transformer additions, like the addition of emphasis support in the last article, I found a good, easy example and started with the scenario test for that example. The needs to pass that first scenario test, example 493 , were simple. The LinkStartMarkdownToken class already had all the necessary fields, so no changes were needed there. I then proceeded to add the rehydrate_inline_link_text_from_token function into the transformer, containing a simple transformation: link_text = ( \"[\" + link_token . text_from_blocks + \"](\" + link_token . link_uri + \" \\\" \" + link_token . link_title + \" \\\" )\" ) From there, each additional scenario test introduced another variation on what an acceptable Link token was. For example, the scenario test for Example 494 introduced a link title that was not present. That changed the transformation into: link_text = ( \"[\" + link_token . text_from_blocks + \"](\" + link_token . link_uri ) if link_token . link_title : link_text += \" \\\" \" + link_token . link_title + \" \\\" \" ) link_text += \"\\)\" And so on, and so on. For any readers of my articles, this is the same pattern I have been following since I started this project: get a simple case working, then keep on adding on little changes until the big change is complete. This set of changes was no different in that regard. Then Why Did I Think It Was Difficult? The daunting aspect of this group was its volume. To be able to recreate the inline link token properly, I needed to ensure that every part of the data was properly catalogued and stored in the token. Doing this exercise for the Inline Link token, I came up with the following diagram and mapping: [foo]( </my url> \" a title \" ) |---|||-|||-----|||---|||--------|||---|| | | | | | | | | | | | | A B C D E D F G H G I B A - ex_label and text_from_blocks extracted link label, in final and original states B - self.label_type the use of ( and ) denoting an inline link C - before_link_whitespace whitespace before the link starts D - did_use_angle_start True indicating that this URI was encapsulated E - self.link_uri and self.pre_link_uri extracted link URI, in final and original states F - before_title_whitespace whitespace before the title starts G - inline_title_bounding_character character used to bound the title H - link_title and pre_link_title extracted link title, in final and original states I - after_title_whitespace whitespace after the title is completed While this may seem overly thorough, I felt that without a complete map of the Inline Link, any Markdown transformation would be incomplete. Especially with so many moving parts, I was convinced that without a solid plan, I would miss a combination or a permutation of the test data. How Did Those Changes Go? The scenario tests were not really difficult to get working properly, it was just the sheer volume of them. Behind each of those scenario tests were a set of Markdown parser functions that needed to be properly exercised. While those functions had already been tested against their HTML output, this work was to add more information in each token, to ensure that the data in the tokens are complete. And that completeness came at a cost. One of those costs was an increase in the number of results returned by functions, such as by the __process_inline_link_body function. To accomplish the extraction requirements of these changes, this function went from returning a tuple containing 5 variables to returning a tuple containing 10 variables. And while that may seem like a simple refactor to complete, I am still debating with myself on how to handle that change. Do I create a new class that is only used internally in this one case, or do I throw it into an instance of the LinkStartMarkdownToken class that I can pass around more easily? While the token data is not complete, the LinkStartMarkdownToken class has all the necessary fields, with 2 fields to spare. Which to choose? As I said, still thinking on that one. Another function that I want to clean up is the rehydrate_inline_link_text_from_token function. At 57 lines, it is a bit larger than I usually like in my projects. But in this case, maybe it is warranted. This function does have a single responsibility, the rehydration of the token, and it sticks solidly to that purpose. With the 12 fields that it uses to rehydrate, the implementation difficulty is there. And that is only for the Inline Link tokens, not the other 3 types of link tokens. They will need implementations too. For me, the really tough part was that I needed to slog 1 through the sheer number of combinations presented in the examples. Using link labels as an example, the link label can contain normal text, backslashes, character entities and inline tokens. To make sure I had all these covered, I needed to make sure I had representative scenario tests for each of these groups of combinations. And then there are the link URIs and link titles. It just got to the point where I used a small spreadsheet to keep track of things, ticking off combinations as I went… just to be sure. Reference Links and Link Reference Definitions Implementing the transformations for these two tokens was different than the other tokens in that I had to develop the transformations in coordination with each other. For example, I started adding support for these tokens using the scenario test for example 161 , which has the following Markdown: [ foo ] : / url \"title\" [ foo ] As I mentioned in a previous section, to properly get this test passing, I needed to start by implementing a bare bones version of the Shortcut Reference Link . After all, if I could not reference the Link Reference Definition, how could I know if it was working properly? Starting with Simple Shortcut Links Thankfully, after looking at Link Reference Definition examples example 161 to example 188 , the only extra link requirement in each of these tests was a Shortcut Reference Link. Having already written the transformer to handle Inline Link tokens, adding code to deal with Shortcut Link tokens was almost trivial. A Shortcut Reference Link is merely an Inline Link without the inline specifier, so modifying the rehydrate_inline_link_text_from_token function to handle the extra link type was quick and efficient. Reusing code already in that function, I came up with these changes within 5 minutes: if link_token . label_type == \"shortcut\" : link_label = link_token . text_from_blocks . replace ( InlineHelper . backspace_character , \"\" ) . replace ( \" \\x08 \" , \"\" ) link_text = \"[\" + link_label + \"]\" elif link_token . label_type == \"inline\" : ... else : assert False Everything looked good from the shortcut link point of view. However, since most of the tests that have Shortcut Links also have Link Reference Definitions to provide the reference for those links, I needed to switch to get the Link Reference Definition tokens done. While I was not 100% comfortable with leaving that implementation untested, I understood that I would have to wait a while to complete the testing. To do that, on to Link Reference Definitions! Moving Over to Link Reference Definitions With a good stab at a transformation for Shortcut Links in place, I turned my attention to the Link Reference Definition transformation. When I started putting the code for this transformation together, I was greeted by good news. The first bit of good news is that since the support for Link Reference Definition tokens was added more recently than the other tokens , I had created that token with a better idea of what information I might need later. As such, all the fields that I required to properly represent a Link Reference Definition element were already in place. That allowed me to implement a near-complete version of the rehydrate_link_reference_definition function after only 3 tries and 10 minutes. That was very refreshing. The second bit of good news was that the previous work on this token had already dealt with all the token's complexity. As I had a lot of issues in implementing the parser's support for the Link Reference Definition element, I assumed that the rehydration of the token would also be a lot of work. It turned out that because of all that hard work, that near-complete version of the rehydrate_link_reference_definition function was very simple. I had even collected both pre-processed and post-processed versions of the link label, link destination URI, and link title! Now back to the other Link tokens. Back to Finishing Up Links With all the hard work done, finishing off the rest of the links was easier than I had previously anticipated. With Link Reference Definition support in place, the scenario tests that included both Link Reference Definition elements and Shortcut Links were executed and, with a few tweaks, passed. Like the effort required to support Shortcut Reference Link tokens, the support for Full Reference Link tokens and Collapsed Reference Link tokens was added quickly. Within a couple of hours, a good percentage of the scenario tests that involved any 4 of the Link tokens were completed and passing. The remaining tests were those scenario tests that gave me some real issues with determining the original text. A good example of this was the scenario test for example 540 : [foo [bar](/uri)][ref] [ref]: /uri which produces the HTML output: < p > [foo < a href = \"/uri\" > bar </ a > ] < a href = \"/uri\" > ref </ a ></ p > The first link that gets interpreted from that text is the Inline Link [bar](/uri) . When the ] character is encountered after that link text, due to the provided algorithm , it is kept as a ] character as there is a valid link between it and its matching opening [ character. Finally, the [ref] is a valid Shortcut Link, matching the reference set up by the Link Reference Definition. Getting the correct, original text to insert into the Link tokens was a bit of an effort. The __collect_text_from_blocks function took a bit of fiddling to make sure that the original text that was extracted matched the actual original text. As with other things in this project, it took a couple of tries to find something that worked and worked well, but that time was well worth it. A bit frustrating at time, but worth it. Having completed adding the support for all the non-image link scenario tests, it was time to add the image links into that mix. Images Looking at the GFM specification for images , it clearly states: The rules for this are the same as for link text, except that (a) an image description starts with ![ rather than [, and (b) an image description may contain links. Basically, as I stated before, the Image Link tokens get to use a lot of the work done for the other link tokens as a foundation, with just 2 changes. The first part of that difference was easy to deal with in the transformer: emit the sequence ![ instead of the sequence [ . Done. The second part of that difference was handling examples of links within image links and image links within links. While avoiding the scenario test for example 528 2 , there were plenty of other cases such as example 525 : [ ![moon ] ( moon . jpg ) ] ( / uri ) and example 583 : ! [ foo [bar ] ( / url ) ] ( / url2 ) that I needed to deal with. The actual parsing of those images and their transformation to HTML were already tested and working. It was just the extraction of the original text that gave me issues. However, having dealt with similar examples in the previous changes for the __collect_text_from_blocks function, I was able to finish up those cases quickly. The good part about getting to the end of this work took a bit to sink in. I had budgeted and entire week to complete these changes. But even after making sure the commit was clean, it was early on Saturday morning. It was Saturday morning and the link token group was completed. Well, almost completed. More on example 528 later. But it was good enough to mark this block of work done and complete. With some extra time left in my schedule, I decided to put it to good use. Code Coverage The first good use that I put that extra time to was improving code coverage. While there were only 3 cases where I needed to tighten up the code coverage, it was just something I wanted to make sure got done. It is true that I almost always argue that the scenario coverage metric is more important than the code coverage metric. But with the code coverage percentage in the high 99's, I just wanted to nail this down while the required changes would be small and manageable. Moving Special Character Support The second good use for my extra time was to move the special character support in the parser into the ParserHelper class. Along the way, adding proper support for the Markdown transformer was accomplished using a small set of special characters. These special characters allowed the normal processing of the tokens by the HTML transformer to generate the proper HTML output, while at the same time allowing the Markdown transformer to rehydrate the token into its original Markdown. With the use of the characters scattered around the project's code base, I felt it was useful to centralize the usage of those characters into the ParserHelper class. To complete that centralization, I also introduced a number of functions that either resolved the characters (for HTML) or removed the characters (for Markdown). 3 Those characters and their behaviors are: the \\b or backspace character, used primarily for the \\\\ or backslash character when resolved, removes the character and the character before it when removed, leaves the previous character in place the \\a or alert character, used to provide an \"X was replaced with Y\" notation when resolved, removes the alert characters and the X sequence, leaving the Y sequence when removed, removes the alert characters and the Y sequence, leaving the X sequence the \\x02 character, used to split whitespaces when resolved, is replaced with an empty string when removed as part of SetExt Heading whitespace processing, delineates leading whitespace that was removed from trailing whitespace that was removed the \\x03 or \"NOOP\" character, used with the alert character sequence as a Y sequence when resolved, replaces the entire sequence with an empty string when removed, same as above but used to indicate that the X sequence was a caused by a blank line token The centralization of these characters and their usage did help to clean up the code. In all cases, I created a single variable to represent the character, and enforced its use throughout the codebase, except in test output. For example, instead of using the \\b for the backspace character, I created a new static member variable of the ParserHelper class called __backspace_character . The use of these variables in the code made it clear that those characters were being used with purpose, and not because of convenience. But even after that work, I still had a bit of time left. What else could I do to help the project? Example 528 With my last remaining bits of extra time, I wanted to take another shot at the proper processing of example 528 . Having tackled the Link related group of tokens, I felt that I had a good grasp of the processing required. With a sense of purpose and confidence, I felt it was time to put that belief to the test. For some background, example 528 is similar to example 525 and example 583 referenced in the previous sections. However, while each of those examples deals with one type of link within the other type of link, example 528 can be considered the composition of both of those examples together. The example is as follows: ! [ [[foo ] ( uri1 ) ] ( uri2 ) ] ( uri3 ) producing the following HTML: < p >< img src = \"uri3\" alt = \"[foo](uri2)\" /></ p > To be clear, this is an inline image link that contains 2 possibly valid inline links within its link label. The final result was that the image's URI is uri3 as expected, but the alt parameter's text is set to [foo](uri2) , an interpretation of the text within the link label. And to make it even more interesting, the GFM specification also provides an algorithm for evaluating emphasis and links which has been tested. Yes, I have had the algorithm given to me, and I cannot make it work. I confess. I have been over the specification's algorithm and my own implementation of that algorithm, and I cannot make it work. Every couple of weeks, I have spent a couple of hours looking at the log output, the source code, and the algorithm description, and… nothing. Giving it another shot, I decided that instead of assuming I knew what was going on, I would try and test it as a new problem. And with any new problem, I tackle it by doing research on it, so that is what I set out to do. I turned on the logging for the scenario test associated with example 528 and started observing and scribbling. By the time I had gone through the algorithm 3 times, I was convinced that I was either missing something important or there was a problem with the well-tested algorithm. If I were a betting man, my money would be on the algorithm being correct, but I just could not figure out where the issue was! What I saw in each of my examinations, was that as the processing progressed, the string [foo](uri1) was parsed as a link. Following the algorithm's instructions, any link starts before that point needed to be marked as inactive, so the code marked the second [ character was marked as inactive. I also double checked the handling of the initial ![ sequence. As that sequence denotes an image token and not a link token, that initial ![ sequence was not marked as inactive. Then, when the second ] character was processed, the implementation skipped over the inactive [ character, hitting the ![ sequence for the image. With the start link characters exhausted, the rest of that string became plain text. But that is not the result that was specified in the GFM specification. It wanted < p >< img src = \"uri3\" alt = \"[foo](uri2)\" /></ p > and the code was generating: < p >< img src = \"uri2\" alt = \"foo\" /> ](uri3) </ p > At that point, I decided to seek help from a higher power: the CommonMark specification site. I posted a quick message to the forums, making sure I indicated that I was having a problem, clearly stating my findings from above and that they were based on my implementation. A couple of quick checks for spelling and grammar, and I posted a request for help. I did get a response to my request rather quickly, and I will address that in a future article. What Was My Experience So Far? The goal that I had set for myself for this chunk of work was to make sure that I added the link token group to the Markdown transformer's features. While it did take me most of the week to accomplish that, I do believe that I accomplished that with some time to spare. It felt good to be able to take some time and do some small tasks to clean up the code a bit. The weird thing about being able to see the end of the project's initial phase is that while I want a quality project, I also want to hurry up. I can see the items in the issue list being resolved and then removed, and I just want to get them all removed. I want to push myself harder to finish them quicker, even though I know that is the wrong thing to do. As with all side projects, my time and effort spent on the project is a balancing act between my personal responsibilities, my professional responsibilities, and the project's responsibilities. And yes, while it is a balancing act, those three groups of responsibilities are in the correct order. I need to make sure to take care of myself and my job first, using any extra bandwidth to work on the project. While I do want to push myself to hurry and finish the project, from a resource allocation point of view, it just would not work. And there is also the quality point of view. While I am good at my job, I am keenly aware that every project has four dials that can be adjusted: scope, cost, time, and quality. If I want the project to completed faster, changing the time dial, I need to adjust the other dials to compensate. This is a personal side project, so I cannot adjust the cost dial, leaving the quality and scope dials. Seeing as I do not want to change my requirements for either of those two dials, I know I need to keep the time dial where it is. Between the balancing act and the resource logic puzzle, I know I need to stay the course. While I was feeling the need to hurry up, I also knew that is not what has got me to this point in the project. What has got me here is good planning, solid effort, and not exhausting myself. If I upset that balance, I might lose my desire to finish the project, and that would be a shame. So, as always, I started to look at and plan the next chunk of work, making sure it was a good chunk of work that I could easily accomplish. After all, slow and steady went the tortoise… What is Next? Add all the normal tokens to the Markdown transformer? Check. Add the Link-related token group to the Markdown transformer? Check. That just left the Container-related token group. Add since I knew that Block Quotes were going to need a lot of work, taking care of the List-related tokens was a good first step. According to Webster's dictionary : \"to plod (one's way) perseveringly especially against difficulty\". ↩ More on example 528 later. ↩ While I am sure I can come up with a better name for the two sets of functions, I am not sure what those better names would be. Ideas? ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/08/10/markdown-linter-adding-links-to-the-markdown-transformer/","loc":"https://jackdewinter.github.io/2020/08/10/markdown-linter-adding-links-to-the-markdown-transformer/"},{"title":"Markdown Linter - Improving the Markdown Transformer","text":"Summary In my last article , I walked through the items that I chose to work on from the issues list, detailing my process for resolving each one. In this article, I detail how I continued the march forward to increase my consistency check confidence by further implementing the token to Markdown transformer. Introduction Having removed a good number of items from the issues list, I decided that it was time to get back to verifying the tokens using the Markdown transformer. While my initial attempt at implementing the transformer yielded 8 new items on my issues list, I was hopeful that this next batch of features for the transformer would uncover fewer errors. Do not get me wrong. If there are issues with any part of the project, I want to know about them so I can properly prioritize them. I was just hoping that the number of new items that I found would be lower this time. It is with that hopeful mindset that I started to work on implementing the transformations for the other Markdown features, excluding the container blocks and the link related blocks. While awkwardly stated, that statement outlined a block of work that I knew I would be comfortable working on and confident that I could complete. Looking ahead, I knew that link transformations were next on the list, with container blocks taking up the final position. I knew I still needed to tackle those token groups, but I was far more confident about handling the simpler cases first. By handling those simpler cases first, I hoped to build up my confidence to work on the links in the subsequent section. At least it would build up if I did not discover as many errors as with the last chunk of work! What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 19 Jul 2020 and 26 Jul 2020 . Did I Take the Week Off? Looking at the commits for this chunk of work, it may look like I did the HTML block on the Sunday, then took the week off, restarting the work on Saturday. For no other reason than dumb luck, I just hit a wall when trying to transform the SetExt Heading tokens. Everything else just flew right by! More on that in a bit. Adding HTML Support During the writing of that week's article, I just felt that I needed a bit of a break. Being a bit fidgety about the block of work that was coming up next, I decided that I wanted to start doing some work on a random token type. The HTML block token support was that work, the type of token to work on chosen randomly using the time-tested Eeny, Meeny method . I honestly did not think that adding that support would be so easy that I could complete it with only 25 lines of code changed, even less if I do not include blank lines. I was just looking to find something to give my mind a break from writing the article. Nothing more. However, I cannot lie about it. It was refreshing. Go in, add the transformation, run the tests, fix 1 or 2 problems that I found, and… done! It worked so well, I thought I would do it with the next token. And that token is the SetExt Heading Token Adding SetExt Heading Support While you can estimate and plan for hours, there are times where the effort required to do something seems almost random. In one of the teams that I worked on, we referred to this type of work as the spins on the \"Wheel of Effort\". One spin of the wheel and you may get 1 hour, and another spin for a similar item may get 1 day. It just depends. I know there were underlying conditions that were contributing to that calculation of effort. However, from my viewpoint, it just seemed like a spin of the wheel. And the wheel was about to give me a spin that I really did not like. The Initial Push From experience dealing with SetExt Heading issues, I had a feeling that adding the support for the SetExt Heading tokens was going to be more difficult than most other tokens. That experience also reminded me that most of the issues that I have had with SetExt Headings were not with the SetExt Heading tokens themselves, but the text block contained within. While the SetExt Heading token itself is created to replace an existing Paragraph token, the handling of the two text blocks was different enough that I was immediately cautious about those differences. The first change that I made for this feature was to write a simple implementation of the token handlers. It was during that implementation that I discovered that while the SetExt Heading tokens contained information about which heading character was used, the quantity of those characters in the Markdown document was absent. That was an easy fix to implement. A quick jump over to the SetextHeadingMarkdownToken class to add the heading_character_count argument and member variable. Then another quick jump back to the new token handlers to use that new member variable when rehydrating the end token. Done. Easy. Except it was not. A single set of tests were failing in each test run: the example 052 series of tests dealing with SetExt text that starts with whitespace. To simplify the test for a previous fix, I created the scenario test test_setext_headings_052a to keep the problem simple and easy to diagnose. The issues for both test_setext_headings_052 and test_setext_headings_052a were easily fixed, but I then noticed a glaring issue: there was no tests for SetExt Heading text that had both leading and trailing whitespace. To address that, I created test_setext_headings_052b to add some trailing whitespace while maintaining the same HTML output with test_setext_headings_052a . From there, the c , d and e variations were added to test various other patterns that I hoped were working properly. All those variations passed immediately, except for scenario test test_setext_headings_052b . That was failing and I was not sure why. The Problem Scenario test test_setext_headings_052b is a simpler version of example 52 with some changes thrown in. In this new scenario test, the data is as follows: a { space } b { space } c === where the literal text {space} are space characters at the end of each of those lines. For comparison, the Markdown for test test_setext_headings_052a is: a b c === and the HTML output for that test is: < h1 > a b c </ h1 > By comparison, the new scenario test test_setext_headings_052b , is a literal copy of that original scenario test test_setext_headings_052a with the addition of a single space character at the end of the first two lines. As the GFM specification states that leading and trailing spaces are removed in normal paragraphs, it makes sense that those additional space characters would be removed as excess whitespace. As such, I expected that the HTML output would be the same, and it was. But when the Markdown transformer rehydrated the text, that removed whitespace was inserted at the start of the line. The rehydrated Markdown for this initial stage was: a b c === Digging a bit deeper, it took a fair amount of additional logging and debugging before an answer started forming in my head. The last line was fine, mostly because it did not have any whitespace at the end of its line. In the case of the first and second lines, that trailing space was being placed at the start of the line, instead of the end of the line. The problem? The 2 spaces removed from the start of the line and the 1 space removed from the end of the line were being merged. The result? When the Markdown transformer went to rehydrate the token, all 3 of those removed spaces were placed at the start of the line. I needed some way to keep the two groups of spaces separate from each other. Another Rabbit Hole? Yup, that thought had crossed my mind. By the time that I had debugged the problem and understood it clearly, it was late Wednesday night and I had already spent 2 nights working on this issue. To ensure that I did not go \"missing\" again, I set a maximum duration of 2 days to solve this issue. If not solved in that time, it would go on the issues list to be dealt with later. Was this a bit overboard? Perhaps. But given my past history with chasing down issues like this , I figured it was better to plan ahead to prevent myself from getting in that predicament. Given how much I like puzzles, combined with a gut feeling about this issue, it just seemed the right thing to do at the time. Over the Next 2 Days Over the next 2 days, I tried 3 or 4 different approaches. But in each case, there was some manner of complication that prevented me from going forward with it. It was frustrating, but I was adamant that I was going to find a solution. Well, if it did not exceed my self-imposed deadline of Friday night. And that deadline just kept on getting closer and closer. The Final Breakthrough Early on Friday night, I was very frustrated that nothing I tried had worked. After resetting the code yet again, I decided I was going to do my best to keep things simple. Before, I had tried altering the signatures of the functions and passing variables around, and that did not get me anywhere. Turning that effort around, I figured that the simplest way to separate the two parts of the line was with a separator character. Previously, the tokens for scenario test test_setext_headings_052b were: expected_tokens = [ \"[setext(4,1):=:3: :(1,3)]\", \"[text:a\\nb\\nc:: \\n \\n ]\", \"[end-setext::]\", ] Using a simple separator character of \\x02 , I was able to change the serialized form of the text token to: expected_tokens = [ \"[setext(4,1):=:3: :(1,3)]\", \"[text:a\\nb\\nc:: \\n \\x02 \\n \\x02]\", \"[end-setext::]\", ] That one change was simple and pivotal, and it struck a chord in me right away. That separator character clearly separated the two different whitespace sequences from each other, with no complex variable passing to enforce it. But that was only part of it. Would it the Markdown transformer portion of this fix be as clean and easy? The Solution I needed a come up with a fix to compensate for that change in the token's text. Before this change, the token's text and the token's whitespace were reintegrated with each other using a simple algorithm: rejoined_token_text = [] split_token_text = main_text . split ( \" \\n \" ) split_parent_whitespace_text = next_token . end_whitespace . split ( \" \\n \" ) for iterator in enumerate ( split_token_text , start = 0 ): joined_text = split_parent_whitespace_text [ iterator [ 0 ]] + iterator [ 1 ] rejoined_token_text . append ( joined_text ) main_text = \" \\n \" . join ( rejoined_token_text ) Basically, split the text string and the whitespace string into arrays, splitting them on the newline character. Then take those two arrays and create a new array with the concatenation of the element from the whitespace array with the text from the text token element. When done with all the elements, create a new string by merging the contents of the array together, using a newline character to join the lines. After this change, that algorithm got a bit more complex, but not by much. rejoined_token_text = [] split_token_text = main_text . split ( \" \\n \" ) split_parent_whitespace_text = next_token . end_whitespace . split ( \" \\n \" ) for iterator in enumerate ( split_token_text , start = 0 ): ws_prefix_text = \"\" ws_suffix_text = \"\" if split_parent_whitespace_text [ iterator [ 0 ]]: split_setext_text = split_parent_whitespace_text [ iterator [ 0 ]] . split ( \" \\x02 \" ) if len ( split_setext_text ) == 1 : if iterator [ 0 ] == 0 : ws_suffix_text = split_setext_text [ 0 ] else : ws_prefix_text = split_setext_text [ 0 ] else : ws_prefix_text = split_setext_text [ 0 ] ws_suffix_text = split_setext_text [ 1 ] joined_text = ws_prefix_text + iterator [ 1 ] + ws_suffix_text rejoined_token_text . append ( joined_text ) main_text = \" \\n \" . join ( rejoined_token_text ) Instead of the very simple concatenation of the whitespace and the text, there was a bit more work to do. First, the whitespace needed to be split into two, based on the new separator character \\x02 . It was more difficult than the original algorithm, but not by much. Once I did a refactoring pass on the Markdown transformer, I was sure that I could clean that algorithm up a lot. But even without that refactoring, the changes in the algorithm were easy to understand. The root of the changes centered on the splitting on the whitespace in the newly inserted \\x02 character. Once that given line was split, there were 3 cases to handle. The easy case was the one where the \\x02 character was present, yielding an array with 2 entries. In that case, the ws_prefix_text variable was set to the first element and the ws_suffix_text variable was set to the second element. The second case was where there was only 1 element in the array and it was the very first entry in the array. In that case, the prefix whitespace had already been applied when the SetExt Heading token was dealt with, therefore the whitespace was assigned to the ws_suffix_text variable. Finally, in all other cases, that whitespace was good at the start of the line, hence it was assigned to the ws_prefix_text variable. Testing this algorithm endlessly in my head, I was pleased when I tested the algorithm with the actual code and it almost worked. The first iteration of this algorithm did not include the properly handling of the second case, as detailed above. As such, the other lines were formatted properly, but that first line was still a bit off. However, the good news is that it only took a little bit of debugging before I had that cause identified and fixed. And after 4 days, seeing all the tests pass for this change was a sweet sight for my eyes! Hindsight is Always Clearer Looking back at the solution, the simplicity and correctness of it is even more evident. Add an extra marker during the initial processing and interpret it during the Markdown transformer processing just makes sense. Looking back, the thing that made the most sense was that the simplest solution was the one that one. It was as complex as it needed to be, and not more complex. And after taking the time to add this support properly, I could only hope I would have enough time to finish adding the support for the other tokens that I had planned for. Adding Emphasis Support After the effort required to add the SetExt Heading transformation, I was hesitant to start working on another token. It was not a lack of confidence that was causing me to pause for a bit, it was the work! After taking 4 days to complete the SetExt Heading token support, I was exhausted. Also, by the time the code changes were committed, I only had about 24 hours to complete the rest of the work! After adding my initial attempt to perform a simple transformation of Emphasis tokens, I was very pleased when I found out that my work for this token was almost complete. The writing of that initial attempt used the information already present in the token and was able to satisfy most of the tests in dealing with emphasis. The only tests that had a problem were the tests that used the alternate emphasis character _ and its slightly altered emphasis rules. When I originally added the EmphasisMarkdownToken class, the HTML transformer did not need to know which character was used for emphasis, only the level of emphasis in the emphasis_length field. As such, any indication of the emphasis character used was absent from the token. To address that problem, I simply added the emphasis_character field to the EmphasisMarkdownToken class. This was followed up by some small changes in the __process_emphasis_pair function to ensure that the new field was being properly initialized in both the EmphasisMarkdownToken constructor and the emphasis block's end token constructor. The change in the end token was a bit trickier in that it does not have a specific place for the character, just a generic extra_end_data variable. Those changes modified the code from: EmphasisMarkdownToken ( emphasis_length ), ... MarkdownToken . token_inline_emphasis , \"\" , str ( emphasis_length ), to: EmphasisMarkdownToken ( emphasis_length , emphasis_character ), ... MarkdownToken . token_inline_emphasis , \"\" , \\ str ( emphasis_length ) + \":\" + emphasis_character , These changes made sure that both the start of the emphasis block and the end of the emphasis block had access to the emphasis character that had been used. To accommodate these changes, a trivial change was needed in the HTML transformer. Once that was done, I temporarily disabled the consistency checks and ran the full set of tests against those changes. Perhaps I am paranoid, but even with such a small change, I wanted to run the full battery of GFM specification tests to ensure that those changes were solid on their own before adding in more changes. But with all those changes in place and all tests passing, I then returned to the Markdown transformer. Due to the above work, the Markdown transfer was changed to be aware of the emphasis character used. Similar to the HTML transformer, the Markdown transformer only required a few lines to be changed to accommodate the new versions of the tokens. After that work was completed, the consistency checks were enabled, and I was glad to find out that all emphasis tests were passing on the first try. Compared to the last element, this element was a breeze! Adding Autolink and Raw HTML Support Seemingly on a roll, the Autolink token support was added in 15 minutes and the raw HTML token support was added in just over 10 minutes. In both cases, the tokens contained all the required information, allowing the transformation to be accomplished with very simple functions. The testing for these changes was likewise, quick, as the amount of code that was changed was small. As these changes took less than 30 minutes combined, there really is not anything more to add. Adding Code Span Support Trivial, Hard, Decent, Trivial. That was the effort that I required to add support for the tokens documented in the last 4 sections. Picking Code Spans by random out of the group of remaining tokens, I was not sure where the \"Wheel of Effort\" would land this time. It turned out that adding support for the code span tokens was a bit more difficult than adding support for the Autolink token and the Raw HTML token, but it was not much more difficult. The Initial Attempt To add the support for the Code Span tokens, I first needed to change the constructor for the InlineCodeSpanMarkdownToken class to keep track of three additional fields: extracted_start_backticks , leading_whitespace , and trailing_whitespace . While those 3 fields were easy add to the token and populate with the right information, their use in many of the scenario tests required small changes in each of those tests. Even those changes were annoying, once they were out of the way, the changes to the Markdown transformer to properly support the Code Span token were minimal. After running the all the tests with the above changes implemented, everything looked good except for a couple of tests. When I looked at those tests more closely, in each test the Markdown contained a code span with a newline character in the middle of it. While Code Span tokens keep everything within their boundaries exactly as they are, including backslashes and character reference sequences, newline characters are a separate matter. Addressing the Issue When a Code Span element has a newline character in it, that newline character gets replaced with a single space character. This introduced a bit of a problem for the Markdown transformer, as the HTML transformer was expecting a space character and the Markdown transformer was expecting a newline character. Luckily, I was able to repurpose some work I did a couple of weeks ago for character references in indented code blocks. As I wanted to ensure that I represented both states of the code span data in the tokens, I replaced the newline characters with the text \\a\\n\\a \\a instead of replacing it with a single space character. At that point, depending on which viewpoint I was taking, I was able to know both the before processing and after processing states of that character. Going forward with those changes, I went to look at any errors with the HTML transformer, but there were not any. It turned out that the work I did on 16 Jul 2020 included adding a call to the resolve_references_from_text function, then housed in the tranform_to_gfm.py module. To be honest, I was not aware that I did that, but it was nice that it was already in place. That just left the changes to the Markdown transformer to resolve any replacement markers in that data. And given the the literal text string \\a\\n\\a \\a , that was a simple task. Running the tests at each interval along the way, I was confident that I had made the right changes. But there is always that moment where I say to myself \"did I remember everything?\". Fighting past that though, I ran the all the tests and was happy to see that Code Spans were now being transformed properly. Adding Fenced Code Block Support Like the work done for the Code Span tokens, the Fenced Code Block tokens were missing some information. In this case, that information was tied to both the start token and the end token. For the start token, the information was being serialized properly, but I needed to add specific member variables for each of the arguments being passed in. This small change allowed me to use that information in the transformers in subsequent steps. For the end token, instead of trying to add a lot of generic information to the token, I opted to add a new member variable start_markdown_token to allow for a reference to the start token. In this way, I was able to easily reference the data of the start token without having to duplicate information in a generic way in the end token. When I started testing these changes, there were a couple of tests that were trickier than the others to get working. Debugging into those tests, I quickly found that the whitespace removed from the start of lines within the fenced code block was causing issues for the Markdown transformer. Having faced a similar problem with the Code Span tokens, I was quickly able to pivot and replace that removed whitespace with a similar replaced sequence and a noop character. Based on the work in dealing with Code Spans, if knew that if I needed to replace a single newline character with a space character, I would replace the newline with the sequence \\a\\n\\a \\a . As there are boundary cases where replacements can be nested, I did not want to use the sequence \\a \\a\\a to replace a removed space character. Instead, I used the \\x03 character to signal a character that did not mean anything, in effect a NOOP character . Thus, the replacement string became \\a \\a\\x03\\a . After a few small fixes, everything passed. Adding Atx Heading Support With the way the \"Wheel of Effort\" had been spinning for me during this chunk of work, I had no clue where the effort for this token would land on that metaphorical wheel. On one side, there were plenty of simple changes required to add support for many of the tokens. On the other side were the SetExt Heading tokens, which took days. It was with trepidation and caution that I started to work on the support for this token. And it was within a couple of minutes of starting that I figured out this work was destined for the the trivial category. Piggybacking on the work documented in the last section regarding the start_markdown_token field, the big change for the parser was to set this field in the end token for the heading. Seeing as the change was solely additive, I jumped to the Markdown transformer, and added the handlers for both the AtxHeadingMarkdownToken and its corresponding end token. The handlers were both simple, just regurgitating the token's member variables back into a Markdown form. Running the tests, I was pleased to see that most of the tests worked on the first run. The ones that did not caused me to look at the tests more closely. It was there that I realized that the only tests that were failing were tests with closing Atx Heading characters. Looking specifically at the code to handle those closing characters, I discovered that I had transposed two variables, producing weird results in the output. Fixing that error, the tests ran without any issues. What Was My Experience So Far? While I mentioned in the last article that I started to believe that I could see the end of the project, it was this chunk of work that brought it into sharper focus for me. In my mind, anyone can say that they can properly parse a Markdown document, but for me, anything short of being able to empirically prove that the project can parser the Markdown project properly is a failure. For me, it is not about guesswork, it is whether I can back up my claim with data. The comparisons between the HTML parser output and the HTML output from the GFM specification? They proved the project could get the HTML right for those specific cases. The comparisons between the line numbers and column numbers from the parser's tokens and the consistency check algorithms? They proved that the project could pinpoint the origin of any element in the original Markdown document. The consistency check using the Markdown transformer? It proved that the project could, with certainty, transform the Markdown document into an intermediate form and then back again, with no loss of data. From my point of view, each one of these checks was building on the work of the other, providing solid, empirical proof that the parser had properly tokenized the document. I understand that to some it may seem excessive. But to me, this was just dotting my Is and crossing my Ts. I wanted to make a good first impression with this parser, and I wanted to take steps to ensure that. In addition, that new level of confidence spoke to me. The scenario tests were effective. The consistency checks were working. And in my mind, the project was clearly past the half-way point on its journey to the finish line! It was all starting to come together. What is Next? As mentioned above, Link transformations were next on the list of Markdown transformations to add. Given the work documented in this article, I was sure that I would run into at least a couple of issues. It was just a question of how much effort it would take for me to push through it!","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/08/03/markdown-linter-improving-the-markdown-transformer/","loc":"https://jackdewinter.github.io/2020/08/03/markdown-linter-improving-the-markdown-transformer/"},{"title":"Markdown Linter - Addressing the Initial Markdown Transformer Issues","text":"Summary In my last article , I started to work on the token transformer, a transformer specifically used to verify the PyMarkdown tokens by rehydrating or reconstituting the original Markdown document from them. In this article, I talk about how I went through the items that I added to the issues list as a result of that work and resolved them. Introduction Normally, I use this space to explain some of the reasoning behind the work which formed the basis for the article. Without that bit of preamble, I feel that I am just dropping the reader off in an unfamiliar location without a compass, a map, or any indication of where they are. While that remains true for this article, the reasoning for this article is so simple that I feel that I do not need a lot of words to explain it. Last week, I added 7 new items to my issues list, with 1 other issue pending from the week before. Plain and simple, I just wanted those issues off the list before I continued with the project's Markdown transformer. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 16 Jul 2020 and 18 Jul 2020 . Things Continue to Be Busy While there are often weeks where I wish something interesting would happen, those types of weeks are not showing up in my life this summer. Instead, I am having the type of summer where I wish I had less to do, giving myself some extra time to relax and recharge. Thankfully, I have a spouse that is very supportive of my hobbies and writing, giving me time on the evenings and the weekends to make my project and this blog happen. If you check the Git repository for this project, the times when I have been able to eke out some time to work should be obvious. I am hoping, that as the summer continues, more \"free\" time comes my way. Nothing to Worry About Looking at the issues list and the items that I added during my last session, I did not believe there were any serious issues to address. Sure, there were some questions that I wanted to follow up on, but I did not feel that any of them indicated a serious issue. Similarly, there were some possible bugs that I had noticed, but I did not feel that any of these possible bugs were severe. In the worst case, I figured that each of these issues would each take a couple hours' worth of effort or less to solve. Nothing I could not handle. But adding those items as I was working on the consistency checks was not something that I liked doing. After all, if I was checking the consistency of my work and there were problems with those checks, did that mean the checks were faulty? It just made sense to me to address those questions right away, ensuring that my confidence in the consistency checks remained high. Besides, if my guesses were right about the severity of the items, I should be able to address them quickly. And with that mindset in place, I started working on removing those items from the issues list. Lone Link Reference Definitions Link Reference Definitions are interesting in that they break the mold created by the other Markdown tokens. When these elements are parsed from their original Markdown document into Markdown tokens, they do not add any information to the output HTML document. For the elements themselves to be useful, a separate link element is required that references them. The absence of such a distinct link element is the basis for example 176 and example 188 . Apparently, this issue was so important, that the specification includes it twice with the exact same example text for both examples: [ foo ] : / url These examples deal with the other unique part of the Link Reference Definition: it is the only multiline element in the base GFM specification. This means that the next line may need to be parsed to determine if the Link Reference Definition on the current line has ended. In this specific case, the link label ( [foo] ) and link destination ( /url ) have both been provided, but there is the possibility of the link title occurring on the next line. So, when the end of the document is reached, for whatever reason, the partial Link Reference Definition was being left partially open, resulting in the token was not being emitted. But why? Debugging and Resolving the Issue Adding some debug statements and doing some research with the scenario tests, the cause of this issue was quick to pop out at me. At first, I thought it was an issue to do with the __close_open_blocks function, as it was a likely candidate. But a quick change to example 176 to add a couple of blank lines after the Link Reference Definition put a quick end to that. That modified case worked without any problems. Doing a bit more digging, the problem seemed to be in the handling of the return value from the __close_open_blocks function. At that time, when the __close_open_blocks function was called from the main __parse_blocks_pass function to close the document, the return value containing any immediately closed tokens was discarded. That definitely did not seem helpful. To address this, instead of using the _ variable to capture and ignore the list returned from the __close_open_blocks function, I replaced it with the variable tokens_from_line to capture that value. Making sure that the list was not being extended with itself (which caused failures in almost every scenario test) the lines: if tokens_from_line and not self . tokenized_document : self . tokenized_document . extend ( tokens_from_line ) were added to extend the document list with the previously missing tokens. A quick run of all the scenario tests to verify that it was fixed, and this issue was removed from the issues list! Getting Rid of Extra Blank Line Tokens While example 559 and example 560 are the two main recorded issues, there were a total of 6 scenario tests that had this same issue. With a Link Reference Definition (or possible Link Reference Definition) being started at the end of the document, an extra Blank Line token was being inserted before the document was closed. Basically, given the source Markdown document for example 559 : [ foo ] : / url or a modification of example 188 : [ foo ] an extra Blank Line token appeared in the token array without fault. Debugging and Resolving the Issue The cause of this issue was a bit more subtle than the others, so it took me a bit longer to figure it out. Most of my initial time on this issue was spent trying to figure out why this issue happened with Link tokens and Link Reference Definition tokens, not just Link Reference Definition tokens. It was not until I took a look at the log output that I realized they were the same issue. While the text [foo] can be used as a shortcut link, that same text can also be used as a link label for a Link Reference Definition. As the Link Reference Definitions are processed in the initial phase and links are processed in the final phase, that text is considered part of a Link Reference Definition until it can be proven otherwise. In this case, that consideration was dropped quickly, as the link label is not followed by the : character. However, it still underwent the initial consideration before being dropped and requeued for further processing. With this new insight, I went through the logs for the __parse_blocks_pass function again. I quickly noticed that in these cases, there was a single blank line getting requeued. Thinking about it for a while, this did make sense. At the end of the document, when a partial Link Reference Definition is closed, an empty string ( \"\" ) is passed into the Link Reference Definition functions to make sure the Link Reference Definition is terminated properly. As a blank line in a normal document naturally closes the element, the passing of the empty string into those functions has proven to be a smart and efficient solution. However, following the flow of data in the log statements, it became obvious that these empty strings were surfacing to the __parse_blocks_pass function where that blank line was eventually stored as an entry in the lines_to_requeue variable. Once placed in that variable, it was requeued and the extra blank line appeared in the array of tokens. From this information, I figured out that there were two parts to solving this problem. The first was to add this code near the start of the closing code in the __parse_blocks_pass function: was_link_definition_started_before_close = False if self . stack [ - 1 ] . was_link_definition_started : was_link_definition_started_before_close = True As this problem only ever occurred with a Link Reference Definition in progress, it made sense to use that condition as a trigger for the fix. That change allowed me to add the code: if was_link_definition_started_before_close and not lines_to_requeue [ 0 ]: del lines_to_requeue [ 0 ] line_number -= 1 Basically, in the cases where an open Link Reference Definition is closed at the end of the document, if there is a blank line at the start of the lines to requeue, remove it. As a side effect of removing that line, it also becomes necessary to adjust the line number by 1 to ensure that the line number is pointing at the correct line. My initial thought was that this code was a kludge, but in the end, I grew to be okay with it. As the mainline is where the requeued lines are managed, it started to grow on me that this was the correct place to address that issue. Fixing it at any other point seemed to me like I was just going to pass a variable around through various functions just to handle this one extant case. This was indeed clearer. The Case of the Possibly Missing Link Text Sometimes, as with example 575 , I look at an example or its scenario test, and I get a feeling that it does not look quite right. If am in the middle of something else, which I usually am, I note it down in the issue list to be worked on later. When I find the time to start working on it, I almost always begin by looking around for other supporting cases or documentation to support any possible answers to the asked question. This issue was no different than the rest. To me: [ foo ] () [ foo ] : / url1 just looked weird. Should not it be the shortcut link that took precedence? The following example, example 576 : [ foo ] ( not a link ) [ foo ] : / url1 produced the following HTML: < p >< a href = \"/url1\" > foo </ a > (not a link) </ p > If that was the case for example 576's output, in the HTML output for example 575: < p >< a href = \"\" > foo </ a ></ p > where were the parentheses? Debugging and Resolving the Issue It took bit of looking around, but I finally found some good supporting documentation. In the case of example 576, the text not a link does fulfil the link destination requirements of an inline link with not , but then fails on the link title requirements because of missing enclosing characters, such as \" . In a subsequent processing pass through the tokens, the inline processors looks for a shortcut link, which it does find with [foo] , ignoring the text (not a link) . This leaves that text as normal text to be dealt with outside of the Link element. The case for example 575 is slightly different. Based on example 495 , this Markdown text: [ link ] () is a valid inline, producing the HTML output: < p >< a href = \"\" > link </ a ></ p > This almost exactly matches the desired output for example 576. In the previous case, the text between the parentheses, (not a link) was determined to be invalid, and hence was not processed as a valid inline link. However, in this case, it is perfectly valid for an inline link to have no text within its parentheses, as example 495 demonstrates. As that text was a valid part of an inline link element, the entire text was consumed, leaving nothing else for processing to deal with later. With nothing outside of the inline link left for processing, nothing gets added to the surrounding paragraph as \"leftover\" text. Based on that research, the parsing of the above Markdown into the above HTML is 100% correct and explainable. Did it initially look weird? Yes. But was it correct? Yes again. Question resolved and removed from the issues list. Where Was That Extra text Coming From? While the 8 scenario tests in which this issue was found were passing, they were only passing because I had modified the acceptance data in the tests. In each case, I simply added text at the appropriate place in the token data to match what was expected. The decision to do that never really sat well with me, so I was happy that it was time to fix this issue. For this issue, I started by looking at the easy (ok, easier) to digest example 524 . The Markdown for this example is: [ link * foo ** bar ** `#`* ]( / uri ) and its HTML output is: < p >< a href = \"/uri\" > link < em > foo < strong > bar </ strong > < code > # </ code ></ em ></ a ></ p > Double checking those two against each other, and against the GFM specification, everything looked fine. However, when I looked at the serialized token for the link: [link:inline:/uri:::::link *foo **bar** text*] I noticed that the word text was showing up in the output instead of the text # . At that time, as noted above, I simply added the token information as-is to the output, created an item in the issues list, and planned to deal with it later. It was not that later. I really wanted to figure out where that text was coming from. Debugging and Resolving the Issue When I looked at the logs, I arrived at the answer within the first 5 minutes of looking at the problem. It turned out that I forgot to add some functionality to the __collect_text_from_blocks function. This function is used to determine the representative text from a list of inline blocks. That text is then used to create a proper record of what the actual text for the link label was before any processing, as well as a normalized version being used for looking up link information. In the case of various inline tokens, instead of placing the original text for the inline element in the resultant string, the literal text text was added to that string. Fixing this was easy, as in most inline tokens use the token_text member variable to contain the correct text. After running the scenario tests for each of the failing cases, the __collect_text_from_blocks function was modified to properly handle replacement of the text for the various inline cases. I then manually went through all eight cases and verified that they were working properly, without the text text showing up in the token unless the string text was in the source Markdown. It took a while, but I got it working cleanly, and for all inline tokens. It just felt good to get this \"dirty\" issue off the books! Where Was That Extra not Coming From? After working on the last issue, I thought that this issue would take the same level of effort and research to resolve. In the tokenization for the link in example 576 : link:shortcut:/url1::not:::foo I wondered why the text not was in the token. Spotted when I was debugging the last issue, that not value just looked out of place. Debugging and Resolving the Issue Thankfully, this was an easy problem to find, debug, and fix. In the handle_link_types function for the link_helper.py module, the Markdown for the example: [ foo ] ( not a link ) [ foo ] : / url1 was parsed to see if it could be a shortcut link and failed. However, before it failed, the link label was set to foo and the link destination was set to not . In the code for handle_link_types , this meant that the inline_link variable was set to the link destination and the pre_inline_link variable was set to the original text for the link destination before processing not . After the failed processing, the pre_inline_link variable was never cleared but was passed into the constructor for the link token when it was created. With that research performed, it was easy to determine out that the statement: pre_inline_link = \"\" was all that was required to resolve this issue quickly and cleanly. What to Do With Backslashes in Code Spans? Sometimes the questions I pose to myself have simple answers. In this case, the question was: - backslash in code span ? what should it look like ? Debugging and Resolving the Issue Doing a quick check in the backslash section of the GFM specification, I did not see anything relevant. However, in the code spans section, example 348 provided me with the information that I needed. The preface to the example states simply: Note that backslash escapes do not work in code spans. All backslashes are treated literally: This is backed up with example 348, whose Markdown: `foo\\`bar` produces the following HTML: < p >< code > foo\\ </ code > bar` </ p > The answer to my question? Leave them alone. They exist as they are, no parsing or interpretation needed. As far as code spans go, they are just another normal character. Is It Worth It? As I quickly resolved this issue, I paused before moving on to the next issue. I started to wonder whether this practice of noting questions and issues in the issues list was worth it. In this case, this was a question whose answer I should have known. The fact that I did not know it right away was somewhat embarrassing. Thinking about it some more, I decided to really devote some serious time to understand what I felt about this question, and to really think thought through it. Doing that, it was only 5 minutes later when I noticed that I had changed how I was feeling from embarrassed to proud and confident. It took a bit of time, but I mentally reviewed the other times I have used the issues list to document questions and possible bugs. Overwhelmingly, the issues that I logged either helped identify an issue or helped me solidify my understanding of the project and the GFM specification. Should I have known that fact about code spans and backslashes? Maybe? While I have a good grasp of the specification, it is also a large specification with a decent number of exceptional cases along the way. But as I thought about that further, I realized that my sincerity about the quality for the PyMarkdown project was not absolute, but ever increasing. When I started the project, I had to look up every fact about every element, just to make sure I had it right. I knew that I was now at a level where I usually only had to look up the more esoteric and exceptional parts of the specification. If I was sincere about the quality of the project, I also needed to acknowledge the quality that my learning about the specification brought to the project. Given that altered point of view, I was confident that it was worth it. Both as an early problem indicator and as a learning tool, it was indeed worth it. And besides folks, I know I am not perfect. That does not mean I am going to stop trying and stop learning. And to me, that is what made the issue list worth it! Link Labels and Special Characters When I add a question to my issues list, it is always with the hope that I have forgotten something simple, and some quick research will resolve the issue. In this case, the question was: - backslash and char ent in link label? tests? Or, for those that do not read native \"Jack\": - what about the backslash character and the character entities in link labels? - are they covered by tests? Reading this now, I believe that this was an honest question to verify how backslashes and character entities are handled in link labels. Based on my current work solving these issues, I have a good idea on the answer, but at the time, I probably was not as sure. The answer? From previous issues, both backslashes characters and character entities are allowed in most areas, including link labels. But as always, it was time to back up my guess with research. And something told me it was not going to be the quick answer I was hoping for. Researching the Issue Starting my search at the beginning, I looked for a scenario test that contained a backslash in a link label. It was tricky, but eventually I found example 523 in the section on inline links: [ link \\[bar ] ( / uri ) Checking the HTML output, it is easy to see that the backslash effectively escapes the character following it, even in the link label: < p >< a href = \"/uri\" > link [bar </ a ></ p > Backslashes? Check. One part of the issue down, one to go. To start researching the issue of character entities in link labels, I decided to start with inline Unicode characters. Based on my usage of Markdown, I generally use character entities to represent characters that I cannot find on my keyboard, usually Unicode characters. As such, starting with inline Unicode characters seemed like a good first step before tackling character entities. This decision was validated by my ease in finding not 1, but 2 examples in the GFM specification, example 175 : [ ΑΓΩ ]: / φου [ αγω ] and example 548 : [ẞ] [SS]: /url But try as I might, I could not find an example of a character entity reference within a link label. As I looked, I did find the text above example 327 which backed up my previous research stating: Entity and numeric character references are recognized in any context besides code spans or code blocks, including URLs, link titles, and fenced code block info strings: So, while the GFM specification did not explicitly state that character entities in link labels were rejected, there were no actual examples of the positive case either. To me, that was not a showstopper… it was just an invitation to be creative! Testing the Issue The first thing I needed to do was to come up with some good examples to test against. To do this, I started by creating the scenario test test_character_references_328a as a variation of scenario test test_character_references_328 . The sole difference between these two scenario tests was the specification of the link label as [f&ouml;&ouml;] instead of [foo] . In a similar fashion, I copied the test test_reference_links_558 into the new test test_reference_links_558a , altering the link label from [bar\\\\\\\\] to [bar&#x5C;] . If I understand the specification properly, both \\\\\\\\ and &#x5C; should produce the same \\ in the HTML output, so to me it was a good test case. In addition, I added the test function test_reference_links_558b with the link label of [bar&beta;] to make sure named entities were addressed. Finally, before I started debugging and changing code, I created the following Markdown document from bits and pieces of those new tests: [ f & ouml ; & ouml ;]( / f & ouml ; & ouml ; \"f&ouml;&ouml;\" ) [ bar \\\\ ]( / url ) [ bar &# x5C ;]( / url ) Verifying its content, I submitted it to my favorite test harness, BabelMark 2 , getting the following results for reference parser, commonmark.js 0.29.0 : < p >< a href = \"/f%C3%B6%C3%B6\" title = \"föö\" > föö </ a > < a href = \"/url\" > bar\\ </ a > < a href = \"/url\" > bar\\ </ a ></ p > Taking the specific HTML lines generated for each scenario test, I inserted that text into the owning scenario test, and verified that the proper HTML was being output. With that taken care of, it was on to cleaning up the consistency check! Debugging and Resolving the Issue After putting in that hard work to get the scenario tests set up, the resolution to this issue was anticlimactic. In the inline_helper.py module, the handle_character_reference was already being called, but its return value was being ignored. That was easily addressed by replacing the line: current_string_unresolved = InlineHelper . append_text ( current_string_unresolved , new_string_unresolved ) with the line: current_string_unresolved += new_string_unresolved The other change that happened at the same time was that the new_string_unresolved variable was added directly to the current_string_unresolved variable instead of using the append_text function. As the append_text function is used to add text to a string while respecting any needed encoding, it was adding a step of extra encoding to the text where it was not needed. With those two changes in place, a couple of extra changes were needed to the handle_character_reference function to make sure it was returning the correct original string, instead of a mangled version. Having made those changes, and a couple of test runs later, the problem was fixed and verified. While the research and setup of the new tests took a while, the actual resolution of the issue took less than 20 minutes to complete. With everything setup, the logs were very helpful by showing me where the extra transformation of the text was taking place, leading to a quick resolution. Yeah verbose log files! Now on to the last issue for this week! Indented Code Blocks and Disappearing Leading Spaces I knew that the spaces were not disappearing, they just were not being recorded properly. But the phrase \"disappearing leading spaces\" sounded cool, so that is what I called it to myself. It was also the last thing that I felt I needed to fix before getting back to implementing the Markdown transformer. Having been discovered in the last article , this was the issue with the Markdown transformer check and example 81 where leading spaces were not always being record for later rehydration. Doing a bit of research, I deduced that when a blank line is encountered in an indented code block, there are three possibilities: too much whitespace, too little whitespace, and just the right amount of whitespace. The scenario test for example 82 was passing without any problems: chunk1 { space }{ space }{ space }{ space }{ space }{ space } chunk2 which took care of the \"too many\" case. A quick change of this scenario test 1 to present 4 spaces on the blank line instead of 6 spaces resulted in the expected behavior. That took care of the \"just the right amount\" case. Left was the \"too little\" case and the issues demonstrated by example 81. Debugging and Resolving the Issue Originally, my time on this issue was spent looking at the logs with a specific focus on how the tokens were created. I had theorized that most of the issue was going to be concerned with how I extracted and stored the whitespace in the text tokens… and I was wrong. Looking at the logs, the tokens looked fine when they exited the initial processing phase, containing the right number of space characters and the right number of newline characters. Debugging further, after the coalesce_text_blocks function was invoked on those tokens, the leading whitespace was missing. Digging deeper, the first thing that I noticed is that the variable remove_leading_spaces was being populated with data from the text token's extra_data field, not the extracted_whitespace field. Fixing that issue got me part of the way there. The next part was to alter the TextMarkdownToken code to allow the combine function to return any whitespace that it removed. This change was pivotal in making sure that any removed whitespace was stored somewhere. Finally, in the case where the coalescing takes place within an indented code block, that removed whitespace is added to the active Indented Code Block token. Unlike the previous issue, this one took a bit of effort to arrive at the right solution. Looking back in hindsight, the path to fix it seems as clear as glass, but it took getting past a number of failed solutions to get there. Each one of the steps outlined in the last paragraph had to be coded, debugged, and tested before going forward. And in most cases, those steps were not the first solution, but the third or fourth. But still, it was worth it. Another item off the issues list, and the project's quality was raised just a bit higher than before! What Was My Experience So Far? For any readers that have followed the progression of this project from a simple idea to its current form, you may have noticed that my confidence and happiness spikes a bit after a round of refactoring. There is just something about cleaning up the code and resolving ambiguity that makes me smile and makes me feel better about the project. While I can quantitatively show that the number of possible issues and tasks are declining, I believe that I am more affected by the qualitative changes in the project. Removing a single large task from the issues list? Meh. Removing 6-7 small tasks or questions from that same list. Yeah! But the interesting part of this block of work was not the work itself, but the writing of the article about the work. During the writing of this article, it suddenly dawned on me. For the first time during this project's lifetime, I believed that I could see the end of the project. There was just this moment where I remember looking at the issues list, and the list looked smaller. That was followed by a moment of clarity that the consistency checks were making a significant impact on my perception of the project. Combined, these two perceptions linked together to form a new level of confidence in my mind. That new level of confidence spoke to me. The scenario tests were effective. The consistency checks were working. And in my mind, the project was clearly past the half-way point on its journey to the finish line! What is Next? With the current round of questions and issues addressed, it was time to get back to adding to the consistency checker. While I knew I had a busy week of personal stuff and professional stuff coming up, I also knew that I did not want to lose any momentum on the project. And making progress on the Markdown transformer was just the right kind of thing to help boost my spirits in a long week! As part of writing this article, I added scenario test test_indented_code_blocks_082a which is the result of the number of spaces on the blank line being reduced from 6 to 4. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/07/27/markdown-linter-addressing-the-initial-markdown-transformer-issues/","loc":"https://jackdewinter.github.io/2020/07/27/markdown-linter-addressing-the-initial-markdown-transformer-issues/"},{"title":"Markdown Linter - Transforming Back to Markdown","text":"Summary In my last article , I talked about how I believe that the benefit of adding consistency checks to the project outweighed the costs of developing those tests and maintaining them. In this article, I talk about how I decided to double down on the consistency checks by adding a token transformer that transforms the tokenized document back into its original Markdown. Introduction Since implementing the consistency checks for the line numbers and column numbers in the tokens produced by PyMarkdown's parser, I have found enough errors to remove any questions in my mind regarding their usefulness. From my point of view, adding those consistency checks is not a \"pull the fire alarm\" concern, but more of a \"let's put some extra water on the campfire and wait to be sure\" concern. These checks are an important tool in a collection of tools that I use with each build to help me ensure that my desired level of quality for the project is maintained. But while I have made great progress on the automated validation of those line numbers and column numbers, validating the content of those tokens was a different story. Each test already includes a comparison of the output text to the reference implementation's output, but I felt that it was only testing the scenario's output, not the input. After all, there were times when I introduced a small change to the structure of the token and token itself changed, but the HTML did not change one bit. While I knew I had 100% coverage for the token's output, I did not feel that I had the right amount of coverage for the tokens themselves. The only way to really test this out? Use the tokens themselves to generate the Markdown that created them. If the tokens contained all the needed information, the regenerated input text should match the actual input text. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commit of 16 Jul 2020 . A Small Note on the Commit We all have weeks that are busier than others, and the week of 12 Jul 2020 was one of those weeks for me. All the substantial work on the commit was completed and tested on 12 Jul 2020 before I started writing the last article. However, it was not until that Thursday that I was able to complete the usual cleanup and refactoring that I require before I submit a normal commit. While this does not affect the work that was done, the timing of the actual work was important to me, for reasons described in the next section. Beware of Rabbit Holes While I feel that I both wanted and needed to add these new checks, I also knew that I needed to be cautious. It was less than a month ago when I lost my way trying to add tab support to the consistency checks, and I was still smarting from that experience. Instead of hitting a difficult problem and taking a step back to reevaluate the situation, I just kept on going, not realizing how much time had passed on that one problem. When I did stop and look up at where I was, it was almost immediately evident that I got lost in the problem… again. As this was another major task for consistency checks, I was keenly aware that I was going to need to take better precautions this time around. If I did not, I was likely to fall into the same pattern and get lost again. As such, I was determined to come up with a firm set of rules that I would follow for this task. After some thought on those rules, the rules that I came up with are as follows: no tab support no need to go down that path again so soon! no container block support get the basics down, then complicate things! primary support for the text token, the paragraph token, and the blank line token these are the primary building blocks for the document no other leaf block support except for the thematic break and the indented code block tokens the indented code block token modifies the output from the text tokens, thus making sure that changing that output the thematic break token provides another external touchpoint that non-text related tokens are possible no inline support except for the hard break token one inline token would prove that the others would be possible proper support for the character sequences and the backslash escape character In addition, I decided to help mitigate the risk of going down a rabbit hole for this new feature by timeboxing the work on the task to approximately 36 hours clock time. While I did do a bit of research before that Friday night, the time I allocated for this task was from Friday after work until I started writing my article on Sunday morning. As I have never been late with an article, despite coming close a couple of times, I knew that it would be a good stopping point that I would not ignore easily. Starting Down the Path While I have plans to simplify it later, as I did with my work on validating the base scenarios , the first iteration of this code was going to be somewhat messy while I figured things out. But the base bones of the transformer started out very cleanly with the following code: def transform ( self , actual_tokens ): \"\"\" Transform the incoming token stream back into Markdown. \"\"\" transformed_data = \"\" for next_token in actual_tokens : # do stuff return transformed_data Not very glamorous, but a good starting point. As with anything that transforms something list related, I needed to perform some action on each token, that action being represented by the comment do stuff . Just a good and solid place to start. Baby Steps - Setting up for Token Discovery The next step was a simple one: discover all the tokens I would need to eventually transform. As I took this same approach with the TransformToGfm class, and that approach was successful, I decided to adopt the same process with this new class. I started by adding this code in place of the # do stuff comment: if False : pass else : assert False , \"next_token>>\" + str ( next_token ) Once that was done, I then modified it to take care of the end tokens: if False : pass elif next_token . token_name . startswith ( EndMarkdownToken . type_name_prefix ): adjusted_token_name = next_token . token_name [ len ( EndMarkdownToken . type_name_prefix ) : ] if False : pass else : assert False , \"end_next_token>>\" + str ( adjusted_token_name ) else : assert False , \"next_token>>\" + str ( next_token ) Once again, this code is not glamorous, but it is setting up a good solid framework for later. The purpose of this code is to make sure that when I start dealing with actual tokens, I get a clear indication of whether an if statement and a handler function exist for that token. If not, the appropriate assert fails and lets me know which token is not being handled properly. In this way, any encountered token must have a matching if statement and handler, or the transformation fails quickly. Setting Up the Test With that in place, I started with a very simple test function, verify_markdown_roundtrip . This function started with the code: def verify_markdown_roundtrip ( source_markdown , actual_tokens ): if \" \\t \" in source_markdown : return transformer = TransformToMarkdown () original_markdown = transformer . transform ( actual_tokens ) print ( \" \\n -=-=- \\n Expected \\n -=-=- \\n \" + source_markdown + \" \\n -=-=- \\n Actual \\n -=-=- \\n \" + original_markdown + \" \\n -=-=- \\n \" ) assert source_markdown == original_markdown , \"Strings are not equal.\" While I added better error reporting over the course of this work, it started with a simple test and simple error reporting. The first two lines of this function check for a tab character and, if present, exit quickly before any real processing is done, as tab handling is out of scope. With that check accomplished, the next 2 lines create an instance of the transformer and invoke the transform function on the list of tokens. Finally, after printing some debug information, the source_markdown variable is compared to the original_markdown variable containing the regenerated Markdown. If the two strings match, the validation passes, and control is passed back to the caller for more validation. If not, the assert fails, and the test is halted. The invoking of this function was easily added at the top of the assert_token_consistency function, which conveniently was already in place and being called by each of the scenario tests. As such, the extra consistency checking was added to the consistency checks with only a single line change to invoke the verify_markdown_roundtrip . def assert_token_consistency ( source_markdown , actual_tokens ): \"\"\" Compare the markdown document against the tokens that are expected. \"\"\" verify_markdown_roundtrip ( source_markdown , actual_tokens ) split_lines = source_markdown . split ( \" \\n \" ) ... After running the tests a couple of times, it was obvious that some work needed to be done to add if statements and handlers. And as it is the most central part of most Markdown documents, it made sense to start with the paragraph token. Starting to Discover Tokens Once that foundational work was done, I started running the tests and dealing with the asserts that fired. Each time I encountered an assert failure, I added an if statement to the normal token or end token block as shown here with the paragraph token: if next_token . token_name == MarkdownToken . token_paragraph : pass elif next_token . token_name . startswith ( EndMarkdownToken . type_name_prefix ): adjusted_token_name = next_token . token_name [ len ( EndMarkdownToken . type_name_prefix ) : ] if adjusted_token_name == MarkdownToken . token_paragraph : pass else : assert False , \"end_next_token>>\" + str ( adjusted_token_name ) else : assert False , \"next_token>>\" + str ( next_token ) Once I was no longer getting failures from one of the two asserts, I was faced with another issue. There were tokens that I recognized with an if statement, but any handler for that token was out of scope for the time being. To deal with this, I made a small modification to the transform function to allow me to skip those tokens that were not yet supported by setting the avoid_processing variable to True . def transform ( self , actual_tokens ): \"\"\" Transform the incoming token stream back into Markdown. \"\"\" transformed_data = \"\" avoid_processing = False for next_token in actual_tokens : if next_token . token_name == MarkdownToken . token_paragraph : pass elif ( next_token . token_name == MarkdownToken . token_thematic_break or ... ): avoid_processing = True break elif next_token . token_name . startswith ( EndMarkdownToken . type_name_prefix ): adjusted_token_name = next_token . token_name [ len ( EndMarkdownToken . type_name_prefix ) : ] if adjusted_token_name == MarkdownToken . token_paragraph : ... else : assert False , \"end_next_token>>\" + str ( adjusted_token_name ) else : assert False , \"next_token>>\" + str ( next_token ) return transformed_data , avoid_processing Basically, the avoid_processing flag was set to True for any token that was recognized by the function but did not have a handler implemented. Then, with a small change to the verify_markdown_roundtrip function, that function could then be instructed to avoid comparing the two markdown variables. def verify_markdown_roundtrip ( source_markdown , actual_tokens ): if \" \\t \" in source_markdown : return transformer = TransformToMarkdown () original_markdown , avoid_processing = transformer . transform ( actual_tokens ) if avoid_processing : print ( \"Processing of token avoided.\" ) else : print ( \" \\n -=-=- \\n Expected \\n -=-=- \\n \" + source_markdown + \" \\n -=-=- \\n Actual \\n -=-=- \\n \" + original_markdown + \" \\n -=-=- \\n \" ) assert source_markdown == original_markdown , \"Strings are not equal.\" While this sometimes felt like cheating to me, it was a solid plan. If any token in the token list was not supported, it clearly stated it was avoiding processing. If that statement was not present and the debug output was present, I was sure that the comparison was made cleanly. Why Was This a Clear Stop-gap Solution? I believe it is a very clean solution. As I knew from the start that I was going to be implementing this check in stages, returning a boolean value from the transform function allows the transformer to specify if it has any trust in the results. But unlike my emotion-based trust in the code base for the project, this trust was binary: it was True if I encountered any tokens that I had not yet accounted for, otherwise it was False . Basically, if there was no code to handle the token, the function returned True to indicated that it was confident that the transformed_data value was incorrect. Given the situation, and wanting to handle the tokens in stages, I believe this is the cleanest solution that I could come up with. No hidden parts, a very small area of code to determine if the check would be skipped, and almost no code to trip out when handling for all the tokens was completed. Leaving the Foundational Work Behind This foundational work put me in a good position to start transforming the tokens back into their original Markdown. While I was sure that this was not going to be easy, I was sure that I had taken the right foundational steps to make this effort as easy as it could be. And if I was slightly wrong and needed a couple more things added to the foundational code, I was sure that I could easily add them. Terminology As I start to talk about actual work to reconstruct the original Markdown text from the parsed tokens, I found out that I needed a simple name to describe the process to myself. I prefer to have functions named descriptively after the action they are coded for, preferably with at least one verb describing the action. Repeating the last half of the first sentence in each function name did not seem to be a sustainable solution, especially not for the naming of Python variables. I needed something more compact. After a certain amount of thinking, the process that I feel the comes closest to what this transformation is accomplishing is rehydration. Possibly taking influence from my Java experience, the word serialize means, according to Wikipedia : translating data structures or object state into a format that can be stored […] and reconstructed later in the same or another computer environment Since the word is overused a lot, I looked up synonyms for serialize and hydrate was one of the words that was in the list. In my mind, I was \"just adding water\" to the data to get the original Markdown text back, so the new word hydrate fit pretty wel. Therefore, I will use the word hydrate in the rest of the article and in the transformer code to signal that the transformer is reconstituting the Markdown. Starting with The Paragraph Scenarios In terms of picking a good place to start, I feel that the paragraph tokens were the best place to start. As paragraphs are usually the foundation of any Markdown document, I was confident that cleaning up all the scenario tests in the test_markdown_paragraph_blocks.py module would be a good initial case. Being simple in their nature, that set of tests would cover the following types of tokens: paragraph token (start and end) - all tests text token - all tests blank line tokens - 3 of 7 tests hard line break token - 2 of 7 tests indented code block token (start and end) - 1 of 7 tests It was a small set of tokens, easily constrained, and built on afterwards. Paragraph Tokens, Text Tokens, and Blank line Tokens In this group of tests, the simple tests were the easiest to verify, but the most important to get right. With a grand total of 7 tests, 5 complete tests were simply around the handling of these 3 basic tokens. But it was early in the coding of their handlers when I recognized that I needed to implement a simple stack to process these tokens properly. Simple Token Stack The reason for the token stack was simple. While I was just dealing with paragraph tokens around the text token for the first 6 tests, the last test would require that I handle a different leaf token around the text token: the indented code block token. Instead of doing the work twice, once to just save the paragraph token somewhere and second to implement a proper token stack, I decided to skip right to the stack implementation. This stack was created to be simple in its nature. The current block would remain at the top of the stack, to be removed when it went out of scope with the end block token. The initial test was to make sure that the text tokens for the examples can extract information from the encompassing paragraph token as needed. This is important because any whitespace at the start or end of each paragraph-text line is removed for the HTML presentation but stored for other uses in the paragraph token. Therefore, the following functions were added to handle the task of keeping the stack synchronized with the paragraph tokens: def rehydrate_paragraph ( self , next_token ): self . block_stack . append ( next_token ) return \"\" def rehydrate_paragraph_end ( self , next_token ): top_stack_token = self . block_stack [ - 1 ] del self . block_stack [ - 1 ] return \"\" Back to the Text Token Getting back to the actual hydration cases, the rehydration of the basic text block is simple to explain but takes a lot of code to accomplish. The general algorithm at this stage was as follows: def rehydrate_text ( self , next_token ): leading_whitespace = next_token . extracted_whitespace if self . block_stack : # Get whitespace from last token on the stack and split it on new lines # Get the text from the current token and split it on new lines # Properly recombine the whitespace with the text return leading_whitespace + combined_text For basic paragraphs, because of the GFM specification, any leading or trailing whitespace on a line is removed from the text before that text transformed into HTML. However, as I thought there was a rule about excess space at the start and end of a line in a paragraph, I made sure to append that text to the owning paragraph token. In addition, when the paragraph itself starts but before the text token takes over, there is a potential for leading whitespace that must also be considered. So, in addition to the above code to rehydrate the text token, the following changes were needed to handle the start and end paragraph tokens properly. def rehydrate_paragraph ( self , next_token ): self . block_stack . append ( next_token ) extracted_whitespace = next_token . extracted_whitespace if \" \\n \" in extracted_whitespace : line_end_index = extracted_whitespace . index ( \" \\n \" ) extracted_whitespace = extracted_whitespace [ 0 : line_end_index ] return extracted_whitespace def rehydrate_paragraph_end ( self , next_token ): top_stack_token = self . block_stack [ - 1 ] del self . block_stack [ - 1 ] return top_stack_token . final_whitespace + \" \\n \" With that done, the text within the paragraph and around the paragraph was being rehydrated properly. At that point, I raised my glass of water and toasted the project as the first 2 scenarios were now checking their content and passing. Yes! From there, it was a short journey to add a 3 more tests to that roster by adding the handling of the blank line token, as such: def rehydrate_blank_line ( self , next_token ): return next_token . extracted_whitespace + \" \\n \" While it was not a running start, this was the first time the entire content of those 5 scenario tests was validated! It was enough to make me go for number 6! Hard Line Breaks From there, the scenario test for example 196 , was the next test to be enabled, adding support for hard line breaks. Interestingly, when I wrote the algorithm for coalescing the text tokens where possible, the new line character for the hard break was already setup to be added to the following text token. This leaves the hard line token primarily as a \"marker\" token, with some additional information on the extra whitespace from the end of the line. As such, rehydrating the hard break properly was accomplished by adding the following text. def rehydrate_hard_break ( self , next_token ): return next_token . line_end And that made 6 tests that were now fully enabled! But knowing that the last test in that group dealt with indented code blocks, I decided to take a bit of a break before proceeding with that token. I needed some extra confidence. Handling Backslash Characters The interesting part about the parsing of this Markdown character is that once it is dealt with, the original backslash character disappears, having done its job. While that was fine for generating HTML, rehydrating the original text from a tokenized version of a string that originally contained a backslash was a problem. If it disappears, how does the code know it was there in the first place? To solve this issue, I had to resolve to a bit of trickery. I needed to determine a way to make the backslash character visible in the token without it being visible in the HTML output. But anything obvious would show up in the HTML output, so I had to take a preprocessing step on the data and remove whatever it was that I would add to denote the backslash. Thinking Inside of the Outside Box Trying a couple of solutions out, the one that held the most promise for me was to use (or misuse) the backspace character. In Python, I can easily add the sequence \\b to a string to denote the backspace character. When use this format to write out the text for the token containing a backslash, I would now add \\\\\\b in place of the backslash to allow it to be placed in the token. To show an example of this, consider the Markdown text a\\\\*b\\\\* , used to create HTML output of a*b* without the asterisk character getting misinterpreted as emphasis. Before this change, the text in the token would have been a*b* , without the inline processor splitting the emphasis sequences into their own tokens for interpretation. After this change, the text in the token is a\\\\\\b*b\\\\\\b* , keeping the backslashes in the data, but with the backspace character following them. But now that I had a special character in there, I would need to preprocess those strings. Dealing with the Complications How does the preprocessing work? In the case of the HTML transformer, the preprocessing uses the new resolve_backspaces_from_text function to scan the incoming string for any backspace characters. When a backspace characters are encountered, it is removed along with the character proceeding that backspace character. In this manner, the HTML output is identical to how it was before this change. Using the above example of a\\\\\\b*b\\\\\\b* , this preprocessing would render that string as a*b* , removing each of the backspace characters and the backslash characters before them. In the case of the new Markdown transformer, a similar algorithm is used that simply replaces any backspace characters with the empty string. Because the end effect is to restore the data to the way it was before, removing the backspace character by itself leaves the data in its original form. Once again using the above example of a\\\\\\b*b\\\\\\b* , when the backspace characters are removed from the string, the string is changed into a\\\\*b\\\\* . While it took me a while to arrive at this preprocessing solution, it worked flawlessly without any modifications. It was just a simple way to handle the situation. And because it is a simple way, it is also simple to read and understand when dealing with the data for the scenarios. After all, when I type in code or a document, I use the backspace key to erase the last character I typed. This just extends that same paradigm a small amount, but to good use. The Fallout As this change affects everywhere that a backspace character can be used, there were some sweeping changes needed in multiple locations to deal with the now escaped backslash characters. The InlineHelper module's handle_inline_backslash function was changed to take an optional parameter add_text_signature to determine whether the new \\\\\\b sequence was added when a backslash was seen in the Markdown. That was the easy part. In any of the places where that function was called, I traced back and figured out if there was a valid escape for adding the new text signature. In a handful of cases, the original string was already present, so no new transformations were required, passing in a False for the add_text_signature . But the more prevalent case was for the calls that passed in True . And it did not end there. This process needed to be repeated with each function that called each of those functions, and so on. In the end, it was worth it. It was a clean way to deal with having the backslash in the token if needed and removing the backslash when it was not needed. Indented Code Block Tokens For the most part, the indented code blocks were simple. Like how the text tokens were handled for paragraphs, the trick was to make sure that the right whitespace was added to the text tokens. def reconstitute_indented_text ( self , main_text , prefix_text , leading_whitespace ): split_main_text = main_text . split ( \" \\n \" ) recombined_text = \"\" for next_split in split_main_text : if next_split : recombined_text += prefix_text + leading_whitespace + next_split + \" \\n \" leading_whitespace = \"\" else : recombined_text += next_split + \" \\n \" return recombined_text The nice thing about the new reconstitute_indented_text function was that it was simple to start with, as shown above. Take the text from the text token, and put it back together, keeping in mind the extra whitespaces at the start of each line. In short order, the single scenario test in the test_markdown_paragraph_blocks.py module dealing with indented code block tokens was passing, and most of the indented code block scenario tests were also passing. It was then down to 2 scenario tests to get enabled. Handling Character References Character references on their own are a vital part of Markdown. When you want to be specific about a character to use, there is no substitute for using the ampersand character and the semi-colon character and specifying the specifics of the character you want between the two. But as with backslashes, these character references represented an issue. Like the backslash disappearing after it is used, the character references also disappear once used. But in this case, the mechanics were slightly different. If the resultant token and HTML contains the copyright character © , there are three paths to getting that Unicode character into the document. The first is simply to use a Unicode aware editor that allows the typing of the © character itself. If that fails, the next best approach is to use a named character entity, adding &copy; to the document. Finally, the numeric character reference of &#xA9 or &%169; can also be used to insert that character into the token. The problem is, if the token contains the character © , which of the 4 forms were used to place it there? Similar to the way I used the backspace character with the backslash character, in this case I used the alert character ( \\a ) as a way to specify that a series of characters have been replaced with another series of characters. Using the previous example of the copyright character, if the character was specified by using the actual Unicode character itself, no alert characters were needed as nothing changed. But in the cases where the character entities were used, to indicate \"I saw this entity, and I replaced it with this text\". For example, if the Markdown contained the text &copy; 2020 , the text would be replaced with \\a&copy;\\a©\\a 2020 . While it takes a bit of getting used to, reading this in the samples quickly became easy to read. For the HTML output, all 3 occurrences of the alert character were searched for, and the text between the second and third alert was output, and the rest ignored. In the case of rehydrating the Markdown text, the text between the first and the second alert was output, and the rest of that text was ignored. The fallout of this change was of a similar scope to that of the fallout for the backspace character changes. There were a few places where this change had to be turned off, but due to sheer luck, most of those places were the same for the backspace character and for the alert character. While it took a small amount of time to get things right, once again it was a clean solution. Indented Code Blocks and Blank Lines All the other scenarios down, and one to go! And… I hit a wall. But unlike some of the other walls that I hit, this one was a good one. When I looked at this remaining case, the scenario test for example 81 , I knew that there was going to be a cost to getting this test to pass its consistency check. And while I could go ahead and work on it, I made the decision that the work to get this one case passing was out of the present cope of work that I agreed to do. The scenario? chunk1 chunk2 { space }{ space } { space } { space } chunk3 \"\" \" (To make the spaces visible on the blank lines, I replaced them in the above Markdown sample with the text {space} .) Double checking the code for the indented code blocks, if the blank lines contained at least 4 or more space characters, the tokenization proceeded properly, and the rehydration of the text was fine. But in the case where the blank lines did not contain enough spaces, that was another issue. While it is not specifically spelled out in the GFM specification, example 81 makes it clear that blank lines do not end indented code blocks, regardless of whether they start with 4 space characters. But looking at the tokens, the only way that I could think of to address this issue was to put any extracted spaces in the indented code block token itself. This would allow them to be used later, if needed, by transformers such as the Markdown transformer. But thinking about it clearly, I felt that this work was beyond the scope of the current rules for this task. I figured that I had a choice between finishing up the task with thematic break token support completed or getting mired down with this one scenario and not properly completing the task. While I was not initially happy about the idea, I noted the item down in the issues list, disabled the consistency checks for the test, and continued. Thematic To wrap up this block of work, I just needed to complete the handling of the thematic break token. As this is a simple token, I expected a simple implementation to rehydrate it, and I was not disappointed. The text that it took to complete the rehydration of the thematic breaks was as follows: def rehydrate_thematic_break ( self , next_token ): return next_token . extracted_whitespace + next_token . rest_of_line + \" \\n \" Simple, short, and sweet. No surprises. A nice way to end. Along the way… In total, I added 6 items to the issue list because of things I noticed during this work. While I was sure that 4-5 were actual issues, I was okay with the remaining issues being good questions for me to answer later. It just felt good to be able to write a new item down and clear it from my mind. It helped me stay on track and keep my focus. And that is a good thing! What Was My Experience So Far? To be honest, I believe I was so worried about going down another rabbit hole that it got in the way of my creativity a bit. Not so much that I could not get the work done, but it was there. And thinking back to that time, I am not sure if that was a good thing or a bad thing. On the bad side, it caused me to question myself a lot more. With each decision I made, I reviewed it against the goals that I specified at the start of this article. Did it pass? If no, then try again. If yes, what was the scope? If depth of the scope was too unexpected or too large, then try again. If yes, start working on it. At various points within the task, stop to reevaluate those same questions and make sure I was staying within the scope. It definitely was annoying at first. On the good side, it was probably what I needed. And I hate to say it, it probably was a good annoying. I do not need to continue to have this internal conversation for smaller tasks. But for this big task, that frequent dialogue helped me focus on keeping the task on track. If I noticed something that was not in scope, I just added it to the issues list and moved on. If I had a question about whether something was written properly, I just added it to the issues list and moved on. It is not that I do not care about these issues, it is that I can more about completing the task and not getting lost on something that is off task. There will be time later to deal with those. All in all, I believe this chunk of work went well. Sure, I pushed my 36 hour time limit to the max, resulting in my article getting written later than I am usually comfortable with. I also pushed my definition of \"complete\" to the max, as I noted in the section on A Small Note on the Commit . All the work was completed before I started that week's article, even if I took me another 3-4 hours to clean it up enough before committing it to the repository. After all an agreed upon rule is a rule, and I kept to each of them. Even if I strained them a bit. I was happy with how I handled myself with this task. I did not get too bogged down that I got nothing done, and I did not go down any rabbit holes. It was a good week! What is Next? Having encountered a number of bugs and possible logs that were logged in the issue's list, it just made sense to tackle those right away. Especially for something that is meant to track the consistency of the tokens, it does not look good to have bugs against it.","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/07/20/markdown-linter-transforming-back-to-markdown/","loc":"https://jackdewinter.github.io/2020/07/20/markdown-linter-transforming-back-to-markdown/"},{"title":"Markdown Linter - Improving Consistency","text":"Summary In my last article , I talked about how I \"weeded\" the project's issues list to build up my confidence that the PyMarkdown project was going in the right direction. In this article, I talk about how I used that confidence to get back to work on the consistency checker for the tokenizer that is the backbone of the PyMarkdown linter. Introduction At the end of the last article , I talked about how confidence is more emotional than logical. From that point of view, resolving items off my issues list was more to fan my passion for the PyMarkdown project that any logical reason. And while there are logical components to my desire to instill quality to the project at every stage, it is mostly an emotional desire to make it better. It is true that part of that desire is due to my experience. I know the large cost of fixing a bug once a product is released, and I want to avoid as many of those issues as possible as soon as possible. But another part of that desire is because that I know that once I put the project out there, it will be considered a reflection of my abilities. Due to both reasons and others, I just want the project to have a healthy level of quality that I have confidence that I can maintain. It was because of that desire to maintain or increase quality on the project that I started to think about whether the cost of my efforts was worth the benefits that I was seeing. Was it worth it? What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 28 Jun 2020 and 05 Jul 2020 . Is the Cost Worth The Benefit? In the end, the tokens that are generated by the project's parser are just a series of letters and numbers that must be verified for each scenario. For the parser's scenarios alone, there are 728 scenario tests that are used to verify that parser's output whenever a change is made. The validation of those same tests is made easier by the test foundations that are already in place: checking the produced tokens against a static list for that scenario, and checking the output from transforming those tokens against the HTML snippet for that scenario. The HTML part of that verification is easy: it either matches the reference HTML output or it does not. The tokens are a lot more difficult to verify. Is the actual content correct? Are the line number and column number correct? Was the correct amount of whitespace stored in the token to allow us to generate the HTML properly? While I try my hardest to ensure that the token information is correct, I do make mistakes. And that is where I see the benefit of consistency checks. As I documented in my last article , there are enough parser rules to remember that I regularly add items to the project's issues list to check later. But I have also noticed that even the act of answering those questions and whether those rules were applied properly can increase my confidence in the project. And if those answers can increase my confidence by themselves, working on completing those checks should have an even larger impact. From where I was, even though the cost of adding the consistency checks was starting to grow, I felt that my increased confidence in the project was an acceptable benefit given that cost. As such, it was forward with the consistency checks! Lists and Blank Lines Sometimes it is the large issues that bother us the most. But often, I find it is the smaller items that really grind away at my mind. If I have not started a task, I try to make sure that I have lined up as many of the requirements for that task as possible up before I start on the task. This preparation helps me to focus on the task and to work on that task without too many interruptions. But, as is normal in my line of work, I have had to adapt to make sure that I can switch away from a task before it is completed. I am usually okay with this, as I often I find a good stopping point and document why I stopped and what I was thinking, hoping that it helps me pick it up later. But if I am close to finishing a task and I know there are only a \"couple\" of things left to go on that task, it is hard for me to resist getting it over that finish line. Just so very tough. Identifying the Issue This was the case with lists and blank lines, the subject of an item being researched and further documented in the last article This issue was not an overly big issue per se, but it was the issue that was gnawing at me. Specifically, I looked at one of the two list examples that start with a blank line, example 257 : - { space }{ space }{ space } and the text before example 257, which clearly states: When the list item starts with a blank line, the number of spaces following the list marker doesn't change the required indentation: Based on that information, a start unordered list token occupies column 1, and the number of spaces following the list marker is 1, for a total of 2. As such, the blank line, being contained within the list, starts at column number 2, with the extra whitespaces added to the end of the line. So why was I seeing the token [BLANK(1,5):] instead of the expected token [BLANK(1,2): ] ? Working the Problem Doing some research, the answer was easy. The blank line token was not wired up to capture the following spaces, something that was quickly fixed. But even with that fixed, when the whitespace went to the end of the line, the count of the following spaces was simply zeroed out within the list handler. To fix this, I had to take some time and create a new algorithm from scratch, paying extra attention to ensure that trailing spaces were put in the right location. I knew that the block quote tokens needed a rewrite to track the skipped block quote characters and their trailing space characters, so I left that part alone. But I did update the issue to note that the list case had been fixed. Improving Consistency With the blank token issue fixed, I then was able to add needed logic to the consistency checker. Previously, the content of the __validate_same_line function was pretty sparse: assert last_token . token_class == MarkdownTokenClass . CONTAINER_BLOCK assert current_position . index_number > last_position . index_number It was time to change that! Keeping in mind that I had only addressed the above issue for blank line tokens within a list token, the first thing I did was to exclude the block tokens from the new code. Then, remembering some previous work that I did, I knew there were going to be three different asserts that I would need to do: one for blank lines, one for indented code blocks, and one for everything else. if current_token . token_name == MarkdownToken . token_blank_line : assert current_position . index_number == last_token . indent_level elif current_token . token_name == MarkdownToken . token_indented_code_block : assert ( current_position . index_number - len ( current_token . extracted_whitespace ) == last_token . indent_level + 1 ) else : assert current_position . index_number == last_token . indent_level + 1 For blank lines tokens on the same line as a list token, the blank line tokens start right after the list token, so the index_number is the same as the list's indent_level . Because the indented code block token includes whitespace in its column number, there is a bit of correction to do to take that into account. Other than that, the remaining leaf block tokens verify that they start at the indent_level specified by the list. After testing and manual verification, that change to the check was completed and verified. I examined around ten related scenario tests, and double checked the results from the new modifications. But even after that extra validation, I was left with a feeling that something was left undone. I was not sure what it was, but it was something to think about as I worked. Making Position Markers Immutable Honestly, the title that I was thinking of for this section was \"Making Position Markers as Immutable as Python Will Allow\", but I thought that was too long. But let's get that out of the way upfront. Python does not have a concept of an immutable complex object. Like most languages, the base objects (such as strings, integers, and floats) are immutable, meaning they cannot be changed once instantiated. To me, this makes perfect sense. If you create one of these base objects, it always retains its properties. If you add two of these objects together, you create a third object to contain the results. Basic computer science 101. Python complex objects are not so nice with respect to immutability. Sure, there are tricks to prevent overwriting of well-known objects within a class, but there is no built-in support for creating an immutable complex object in Python. 1 Conveniently, I did not need the PositionMarker class to be 100% immutable, just immutable enough that I do not add code that overwrites the member variables from that class by accident. Benefits of Immutable Objects Why is this important to me? Partly cleanliness and partly experience. Immutable objects are just cleaner, as you can guarantee that their values will never change. If you instantiate an immutable object as the first object in your program, it will still have the same value that it was instantiated with when the program ends. From experience, immutability provides a level of safety that is desirable. This sense of safety is because immutable objects are useful where there is a concern about random side effects introduced by child objects and functions. In the case of the parser, I know that before some previous refactorings, I was passing the variables text_to_parse and start_index all over the place. Even after that refactoring, I remained concerned that the parser code could accidentally alter the existing PositionMarker objects without a good way for me to detect it or prevent it. By changing the PositionMarker class into a mostly immutable object, those concerns could be largely eliminated. Any concerns of side effects would be mitigated by the simple fact that once the value for the member variable was specified upon creation, it cannot be changed. Before this change was put in place, I knew where there were cases where I was not 100% sure that I understood where the values in the object came from. If I did things right, after this change I would probably have a couple of extra instantiations in the code, but I would know that the values were solid. Making the Change Far from one of the bulletproof approaches talked about on Stack Overflow , I just needed something simple to prevent accidental overwrites. For my lightweight approach, I chose to change the PositionMarker class to expose its member variables exclusively through properties. Before this change, the line_number member variable was defined in the constructor with: self . line_number = line_number When I needed to access the line number, I would simple take the instance name of the PositionMarker class, say position_marker , and append .line_number to it: position_marker.line_number . Clean, simple, and neat, but also mutable. Just as: x = position_marker . line_number assigns the value of the position_marker instance's line_number member variable to the local variable x , the following: position_marker . line_number = x assigns the value of the local variable x to the line_number member variable. To make the line_number member variable mostly immutable, I simply changed its instantiation to: self . __line_number = line_number and added a property getter named after the original variable name, but without adding a property setter: @property def line_number ( self ): \"\"\" Gets the line number. \"\"\" return self . __line_number By doing this for each member variable, two things were accomplished. The first thing was that by prefacing the member variable's name with __ , I declared it as a private variable. Variables declared as such are only able to be changed from within the class that declared them. 2 Then, by adding a property that has the name of the variable without the prefix ( line_number vs. __line_number ), I provided a simple read-only access to the object's properties. It was simple, and it did not require any renaming of read-only references to the member variables. Cleanup The cleanup that occurred as part of that change was mostly contained within the ContainerBlockProcessor class. That made sense to me, as that is where most of the manipulation of the position occurred after the block quote tokens and list tokens were processed. In that class, there were a handful of cases where I was directly manipulating the PositionMarker member variables, which now required new PositionMarker objects. But since those new objects were all created within the ContainerBlockProcessor class, where I expected them to be created, it was okay. As a nice benefit, once that cleanup was completed, there were a few functions where the passing in of a new PositionMarker class was no longer needed, resulting in some bonus dead code deletion. What did I gain by doing this? Mostly confidence. Before that change, I was concerned that I would introduce some random side effect in one of the processors. Since the class became mostly immutable and was only changed in the ContainerBlockProcessor class, I gained the confidence that a leaf block cannot change any PositionMarker object. Furthermore, I can easily scan to see where that class in instantiated, knowing in an instant where any changes are occurring, and therefore why they occurred. Preventing Consistency Checks for Tabs With all those changes in place, I was able to go back and add extra logic to prevent the consistency checks from executing if they encountered tab characters. Until I was able to work on proper tab character support, I felt that this was a good way to keep the consistency checks active without having to disable them in scenario tests that dealt with tabs. From a quality point of view, that felt like a short-term solution to me, not something I wanted to use long term. The change side of this was implemented very quickly. Within the utils.py module, I added calls to the __calc_initial_whitespace function. If the second returned value (often called had_tab ) was True , the whitespace contains a tab character. Due to the simple nature of the consistency check function, when this happened, I simply returned from the function at that point. With those changes made, I started to work on the testing and was surprised by the result. I was only able to uncomment one scenario test, test_tabs_004 . Digging into the cases a bit, I came to an interesting realization. I was missing the most obvious use of tabs: in the prefixes of list content. Extracting List Item Whitespace During the early design stages of the parser, I made a simple design choice that I believe has saved me hours of work. I designed the container block handlers to remove any processed text before passing the remaining text on for further processing. That design allowed the leaf block processing to be kept simple. The leaf block processor only sees the text that it needs to see to do its processing, and nothing more. Basically, the design choice was to make sure that each processor, container block and leaf block, had a solid definition of their responsibilities, and to keep to those responsibilities. Following that design, when the list block processing was added, the indent_level was used to figure out where the encompassed leaf block tokens would start. Before the ListBlockProcessor passed the line to the LeafBlockProcessor , it removed exactly indent_level characters from the start of the line, per that design. As those characters were part of the list's structural indent, it was the list's responsibility to deal with those characters, not the LeafBlockProcessor . This approach worked fine until I needed to understand the composition of that extracted whitespace to validate the token. At that time, if the whitespace was composed of space characters, everything remained fine. One character of indent equals one space character, and everything balances out. But when one of those characters is a tab character, that calculation was thrown out of whack. While properly handling tab characters was out of scope, being able to detect them to properly disable the consistency check was not. As far as I could tell, all the remaining scenario cases that I wanted to uncomment contained a tab character in the list part of the line. If done properly, it would keep the responsibility of when to enable and disable the consistency check in the purview of the check itself. To me, that was the desired goal. Correctly Extracting the Whitespace To be able to detect the tab characters properly, a bit of juggling had to be done. As the best way to know which container is active was still the token stack, I modified all three list stack tokens to allow them to keep the instance of the list markdown token that they started. This allowed me to add the add_leading_space function to the two main list markdown tokens, effectively adding the whitespace to the list markdown token that owns that whitespace. Once that foundation was setup, I proceeded to modify the __adjust_line_for_list_in_process function from the ListBlockProcessor to properly account for the leading spaces in the lines. Properly Enabling Consistency Checks Having completed that previous task, I knew that I was almost ready to go back and clean up the work started in the previous section on Preventing Consistency Checks for Tabs . The only thing that I needed to do is to make sure that if the leading spaces contained the tab character, that the leading spaces would be set to a single tab character. While this assignment was a large simplification of how to handle the tab character, the proper implementation could wait. I just needed it to work well enough for me to detect it. And it did work well enough. By adding a couple of extra tab checks in the right places, I was able to uncomment the remaining calls to the assert_token_consistency function within the test_markdown_tabs.py module. Between the previous tab checks and the extra tabs checks, the consistency checks were now able to properly exclude any Markdown with a tab character from the checks. For me, this was just the right way to do it. While I needed to comment out those function calls at that time, it always felt like I was cheating. It was not the test's responsibility to know when to disable the consistency check, it was the consistency check that needed to make that decision. By moving the determination of how to handle tabs into the consistency checks, I felt that I more properly constrained the problem to the responsible object. It did not feel like cheating any more. It just felt right. But now that the consistency of the line/column numbers was in a better place, how would I verify that the whitespace that I extracted was the correct whitespace? If I wanted people to write solid linting rules, I want to give them solid data to base those rules on. This All Leads Up to This Given my stated desires to check the consistency of the tokens, I could only see one way to be sure of the content of the tokens. While in previous weeks it was a small voice, that small voice was now starting to speak with its outdoor voice. To check the content, I would need to write a Markdown transformer. I had been thinking about working on this for a while, and even added an item to the issues list to keep track of it: ## Features - Correctness - can we generate Markdown from tokens ? do we have enough info ? To me, it felt like I was trying to avoid going down this route, afraid that writing a Markdown transformer would result in me going down another rabbit hole. Given my recent experience, I believe it was an acceptable concern that I needed to address if I decided to write the Markdown transformer. But from a quality point of view, I felt that writing the transformer was inevitable. Outside of manual validation, the only way that I could validate that that tokens were accurately representing the Markdown document was to regenerate the original document from the tokens. As much as I tried to convince myself otherwise, I just could not see another path that would provide me with the same level of quality and confidence I that I believe the project deserves. What Was My Experience So Far? While the actual work that was documented in this article varied from the work documented in the last article, the goals driving both were the same: quality and confidence. For me, it is always important to make sure that project responsibilities are defined and adhered to. Some of those responsibilities are people or role focused, and some of those responsibilities are assigned to objects within the project. In both cases, having those clear boundaries helps to figure out where things go and what should be taking care of any issues. Having completed this block of work, I felt good. For me, commenting out or disabling a test or a part of a test without a good reason just feels wrong. Like \"nails down a chalkboard\" wrong. By properly assigning the responsibility for disabling the consistency check to the consistency check itself, I was restoring the control of that decision to the object that was responsible for it. I had restored balance to one small corner of the universe! But it was hard for me not to think about doing a consistency check for the token content. But, like before, it boiled down to a cost-benefit analysis. Did I think that cost of a deep check like that would justify the benefit? The more I thought about it, the more it just made sense . Once again, this was not a logical decision, but an emotional one. And as such, I did feel that the benefit justified the cost. I could not give a solid reason why, just a solid feeling that it would. What is Next? Once I acknowledged to myself the need for proper verification of the token's content, the only true viable path was to write a Markdown transformer. While I knew that I had a good foundation for checking the line/column numbers of the tokens, I could not see any other alternative that would verify the tokens at a level that was acceptable to me. But how could I start work on that massive task, and not lose myself like I did with adding tab support? In all fairness to Python, most popular software languages do not natively support immutable complex objects without some sort of trick, pattern, or additional plugins. ↩ As with everything, there are caveats. If you understand the system that Python uses to mangle the names as part of making a variable private, you can technically get around this. However, that effort is considered out of scope for this article. Since all the member variables are base types, any consideration of modifying complex types exposed in this way by such an object are also considered out of scope. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/07/13/markdown-linter-improving-consistency/","loc":"https://jackdewinter.github.io/2020/07/13/markdown-linter-improving-consistency/"},{"title":"Markdown Linter - Weeding the Project's Issue List","text":"Summary In my last article , I talked about how I was pulling myself out of the rabbit hole that I dug myself into, by finding a small task, and completing it with panache. 1 In this article, I talk about how I continued to get my confidence back by resolving items on the issues list while increasing the quality of the project. Introduction At the end of the last article , I talked about how I was starting to get out of the negative headspace that I found myself in, making progress with the project at the same time. The project's progress was a good thing, but it was my emotions towards the project that I was more concerned with. I have seen people stop working on their passion projects for varied reasons, and I just did not want a momentary lapse of confidence on my part to be the reason that I stopped working on this project. When I was pulling myself up out of my rabbit hole, I came to the realization that part of the reason that my confidence took a bit of a hit, were the contents of the issues list. While I am pretty sure that not every item on the list is an actual issue, until I debug and verify each one, each item on that list is a potential bug. And each one of those potential bugs represented a bit of uncertainty that lowered my confidence. Given that realization, taking some time to go through and \"weed\" the project's issue list seemed like a good idea! What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 16 Jun 2020 and 26 Jun 2020 . Starting Out Easy The first three tasks on my list were simple things to resolve, little things that I kept on meaning to do when I had some time. Completing them to get me warmed up for the issues list was a good, solid plan. The first item on my list was some simple refactoring of the assert_token_consistency function in the utils.py module. I had added some decent functionality to it in the last couple of weeks, and it was time to make sure that the module was returned to its normal, properly organized state. The needed changes were simple. Start with a bit of extracting code into functions, then add some function and variable renaming, and finally test the changes to make sure I did not break anything. While nothing changed in the source code for the project, it felt good knowing that the test code was just that much cleaner. The next item was also a simple task: add reporting of leading whitespace to the next list item token ( li ). When I was adding the code to handle leading whitespace for the list start tokens, somehow, I forgot the new list item token in that work. I noticed this when I went to verify the next list item tokens in the consistency checks, and there was no indication of the leading whitespace, other than the indent_level variable. For the sake of those consistency checks, this needed to be addressed. Adding this support into the consistency checks with the new list item tokens modifications in place was almost trivial. The do not do new list item token until fixed check was removed and replaced with a simple calculation of the init_ws variable, in keeping with the handling of the list start tokens. To complete those changes, I also added some code in the __maintain_block_stack function to guarantee that new list item tokens were properly added and removed from the stack. Finally, I decided to use PyCharm and its enhanced code analysis tools to take a look at the project, fixing things where possible. I have talked before about how PyCharm, while not a good development environment for me, is a product I definitely like to use as a tool on a current project. For me, the most useful of these tools is a comprehensive look at what arguments and variables are used, and whether they are needed. In addition, PyCharm has a project dictionary that allows me to search for typos in comments and variable names while maintaining a custom dictionary that allows me to remove often used terms and abbreviations. Combined, I feel that using PyCharm as a tool just adds an extra level of quality and cleanliness to the project. While none of these tasks were big issues to tackle, they were small tasks that were easily resolved and crossed off my mental \"when I get time\" list. And one less item on that list is one less thing to worry about! Verifying Issues Scenario 86a: Indented Code Blocks There are times when I look at an issue and I know exactly why I put that issue onto the issues list. This was not one of those times. The notes I have for this one clearly state that scenario 86a is a modification of scenario 86 , but with 9 leading spaces before the indented code block. Those same notes are even clear that the reason I added this new scenario was because I was concerned that scenario 86 only tested a case where the length of the extracted whitespace equaled the length of the remaining whitespace. What I did not know was why I thought this was an issue. I did some due diligence here and found nothing. I temporarily added some extra debug code around the indented code block code but did not find anything useful. After making a mental note to myself to write better notes on why I added items to the issues list, I removed it from the issues list and moved on. Scenario 87: More Fun with Indented Code Blocks When I was doing the initial work to add the line/column number to the tokens back in May, I wrote an issue to myself as a question: 087 - shouldn't it be inside of the indented code block? Unlike the issue with scenario 86a, it was easy to see why I wrote that question. If you look at the example by itself, the blank line before the reported start of the indented code block is also indented by 4 space characters. However, when I looked at the example within the context of the GFM specification for example 87 , the line before the example reads: Blank lines preceding or following an indented code block are not included in it: An honest question, an honest and researched answer, and an issue that was quickly resolved. Next! Scenarios 235, 236, 252, and 255: Indented Code Blocks and Lists In each of these four cases, every scenario had to do with something that may look like an indented code block being started from within a list. Like scenario 87 in the last section, I could see how these scenario tests raised questions. For each one of these examples, from a quick glance it is hard to tell if the number of spaces is correct from the example. In the case of scenario 235 , my first inclination was that I had coded something wrong. The - character is followed by 4 spaces, so one should be in an indented code block. Right? Almost. The actual start sequence for the list is not - , but -{space} . As such, the list starts with the - character at column number 2, the space character at column number 3, followed by 3 space characters for a total indent to column number 6. The blank line then ends the list on line number 2, and line 3 with 5 leading spaces is picked up as an indented code block. The scenario test was correct. Yes! The general math for scenario 236 is the same, but because the text two is indented 6 spaces instead of scenario 237's 5 spaces, it counts as a continuation of the original list. Scenario 252 and scenario 255 are just variations of this, with and without the indented code blocks confusing the issue. In each case, the scenario tests were correct. But I felt good that I had questioned whether they were correct, and it was solidly answered in the positive. However, even though I did not find an immediate issue, I did find a future issue. In each of these cases, the indent_level associated with the list is assumed to be comprised of space characters. This assumption is fine for now, as the current consistency checks explicitly ignore checking tests that contain tabs. But when the tab character support is enabled in the consistency checks, extra calculations will need to be added to ensure the column numbers remain accurate. This was not something I needed to deal with now, but it would be an issue later. Blank Lines and HTML Blocks I initially thought that this one was an open-and-shut case; the issue being noted down as: blanks lines, if starts with 2 ws, is it (x,1) or (x,3)? The obvious answer is that it is always 1, as I indicated in the commit message for removing the issue: Answered question: blank lines always start at 1, as do HTML blocks. To verify this, I did a a quick scan of the test code for the text html-block and the text BLANK , looking for the string values of their respective tokens in the scenario test output. As expected, each of those tokens contained a column number of 1. Until they didn't. The HTML block token and the blank line token always start at the start of the line, and hence have a column number of 1. But when those leaf block tokens are created within a container block, the start of the line is where the container block says it is. Therefore, for blank lines created within a list block, their column number becomes the indent level for the container block. The good news here is that I had an issue with the commit message, not the source code. If I could go back and correct the commit message 2 to make it more correct, I would have changed it to: Answered question: blank lines always start at 1, as do HTML blocks, except when enclosed by a container block. After a bit of double checking, those scenarios and their tokens were also verified. Another quick issue to resolve and get off the list! Scenarios 197, 257, and 262: Blank Lines and Lists This issue was logged in order to explore whether or not there were issues with lists that started with a blank line, as in scenario 257 and scenario 262 . To start with a baseline for blank lines, the example for scenario 197 is as follows: \\ a \\ a aaa \\ a \\ a # aaa \\ a \\ a ( Aside: Due to previous issues with me missing spaces in the examples, I had previously replaced the spaces in this example with the character sequence \\a , making the space character more visible. Then, before passing this string to the test code, the string is processed this string by invoking .replace(\"\\a\", \" \") on the resultant string, transforming it into an accurate representation of the example. This greatly reduced the number of times that I missed trailing whitespace to zero!) In the scenario test for scenario 197, the tokens for the 1st, 4th, and 8th lines, includes the leading whitespace while maintaining a column number of 1. For example, the token for the blank line on line 4 is: [BLANK(4,1): ] Therefore, I compared that behavior to the blank line's behavior inside of a simple list block, such as with this Markdown for scenario 257: - \\ a \\ a \\ a foo and the differences were clear: [BLANK(1,5):] There are two differences between this token and the uncontained token above. The first is that unlike scenario 197, the leading whitespace that was removed is not stored in the token. The second is that the column number is 5, when it should be 2. Based on my experience with blank lines in the last section, it was easy to see that the column number should be 2, given the unordered list start character - and the mandatory space that followed it. Scenario 262 was a far easier issue to deal with. Its Markdown is: * Simple. An unordered list start token, by itself in a document. Thanks to the work I did on scenario 257, this one was easily verified. If scenario 257 should produce a blank line token that contains 3 leading space characters, then scenario 262 should contain a blank line token with no leading space characters. As this was just researching the issue, I resolved the existing issue and added more specific issues to be properly addressed later. One issue to do with recording whitespace for blank lines in a container, and the other issue for correcting the column number of that same blank line in a container. Scenarios Extra001 and Extra002: Checking for Correctness Looking at how blank lines are defined in the GFM specification, from a purely transform-to-HTML point of view, it is obvious that a document that only contains whitespace will produce an empty HTML document. This is mainly due to the stipulations: Blank lines between block-level elements are ignored and Blank lines at the beginning and end of the document are also ignored. But for those blank lines to be ignored, it stands to reason that from a tokenization point of view, there must be a blank line token to ignore. And as the linter operates on tokens, the scenario tests test_extra_001 and test_extra_002 were added to make sure the right blank tokens are produced. After the previous work in the above sections with blank lines, verifying these scenario tests was quick and painless. In reverse order, the text for scenario test test_extra_002 was a simple document with 3 spaces, hence it needed to produce a single blank line token with 3 spaces. With that test solidly in place, it logically follows that remove those 3 spaces for scenario test test_extra_001 would produce a blank line token with no spaces, which is what the test expects. While this may have seemed like a trivial test, it is often the trivial cases and edge cases that trip up projects. With everyone on the project worried how a big complex example will be resolved, sometimes it is those little examples that slip through the crack out into the wild. Honestly, even though they are trivial, I just felt better knowing that these trivial cases were double-checked and covered. Scenarios 559 and 560: Link Reference Definitions? I almost felt embarrassed when I read this one, as the answer was right in the scenarios themselves. The function comments for 559 (and with one small modification, 560) are as follows: def test_reference_links_559 (): \"\"\" Test case 559: (part 1) A link label must contain at least one non-whitespace character: \"\"\" Using scenario 559 as a benchmark, its Markdown is as follows: [] []: / uri and the Markdown for scenario 560 is almost the same, except for added whitespace: [ ] [ ]: / uri As it is hard to argue with the GFM specification's definition of a link label , I resolved this and moved on. In both cases, there just was not any whitespace in the link label. But I still did due diligence: verified the example, checked the tokens, and after a slight face-palm , I resolved the issue and moved on. Changing the MarkdownToken's Constructor? Starting to ramp down on the project work, I hoped that this was a simple issue to look at and resolve. I had logged a simple question in the issue list: for all of the tokens that used position_marker, do we need =None any more? This was an interesting question in that, except for the MarkdownToken class itself, none of the child classes have position_marker as an optional argument! From that point of view, it would be quick to resolve it. But I did not feel that I was doing a complete job, so I decided to run with that idea a bit and find out where it led me to. Doing a quick search over the MarkdownToken class and its children, the breakdown of how the MarkdownToken constructor was called from the child classes were as follows: line_number and column_number arguments used: 3 position_marker argument used: 10 none of the above arguments used: 11 For the \"none of the above\" case, most of those child classes were for inline tokens that do not have line/column support yet. But if the trend of the current statistics continues, this change may be something to revisit in the future. Knowing more about this issue, I was good resolving this issue now, possibly exploring this again in the future when line/column numbers are added to inline elements. Renaming the SetExt Token's Whitespace Variable I was finally at the end of my planned issues list, and I was glad that I was ending on another simple one. When I was writing my first pass of the consistency checker , I noted that I had to special case the SetExt heading tokens, as the member variable that is consistently named extracted_whitespace for other tokens was mysteriously named remaining_line for this token. After performing due diligence to find out where the variable remaining_line was referenced and what was depending on it, there was no reason to keep this difference around. It just made more sense to change the name to the more consistent extracted_whitespace . In addition to a simple search-and-replace, this change allowed me to reduce the complexity of the __calc_initial_whitespace function. Another small change, but another small step to a cleaner code base! Why Was This Work Important to Me? From my personal and professional experience, the longer you let an issue sit unexplored, the more uncertainty exists with respect to the team's confidence in a project. At the time when I resolved those issues, I was at a more emotional place than normal, where those uncertainties were weighing more heavily on my confidence. While it took some time to work through them, it just felt right doing the proper due diligence on each issue and resolving it. And the results of resolving these issues were very positive. With the exception of adding the proper encapsulation of leading whitespace for new list item tokens, no other source code was measurably changed. 3 With respect to the test code, there were a couple of net-neutral changes for code quality, but the other changes only added extra tests cases, not changing existing tests. Basically, while the work done in this time frame did not change the project's code base significantly, it did increase my confidence in the project by eliminating a few of the existing questions. What Was My Experience So Far? As far as recoveries go, I was feeling better after this stint of work. By going through the issues list, resolving 9 of those issues, and doing some code cleanup, my confidence was back on track to be where it was before. In any project, people encounter circumstances that force them to evaluate if they have made the right project decisions up to that point. Depending on how hard those circumstances hit them and how hard they hit them, people will decide to continue with the project, abandon it, or take a wait-and-see approach. While I was shaken for a bit, I was now back firmly in the continue with the project camp. According to Webster , confidence is: a feeling or consciousness of one's powers or of reliance on one's circumstances Confidence is an emotion, not logic. It is not a light switch and it is not something that listens to reason. Confidence it fickle, hence the expression: One bad apple spoils the barrel. I could have resolved 29 issues, but if I found just one issue that looked like a showstopper, those circumstances could be completely different. Who knows? Whether it was something substantial or something more lightweight, I knew that I needed to do some work to try and influence my confidence in a positive direction. In this case, resolving several items off of the issues list was needed, and it paid off. It was not a guarantee, but a gamble that paid off. Someone once told me: Marriage is made up with a whole bunch of days. You have good days, you have bad days, and you have so-so days. The sign of a good marriage is that you have more good days that the other two combined. A similar concept applies to working on passion projects, like me and my PyMarkdown project. It was just a matter of finding the right thing to do on the project that would reignite my confidence, and therefore passion, for the project. What is Next? Having done some decent cleanup, I decided it was time to get back to work on the consistency checker. While I realized it was not going to be able to be 100% complete until I started handling tab characters, I wanted to make a good effort towards getting it more complete. And that started with proper accounting of whitespace. Per Webster's \"dash or flamboyance in style and action\". ↩ Yes, I know you can change a commit message , but the price is usually too high to pay for anything other than your very last commit. ↩ For the sake of this sentence, I define a measurable change as a change that changes the requirements for input or the actual output of the project, with the exception of adding or modifying log messages. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/07/06/markdown-linter-weeding-the-projects-issue-list/","loc":"https://jackdewinter.github.io/2020/07/06/markdown-linter-weeding-the-projects-issue-list/"},{"title":"Markdown Linter - Rabbit Hole 3 - Trying To Dig Myself Out","text":"Summary In my last article , I talked about how I quickly found myself descending into a rabbit hole while addressing an issue with my PyMarkdown project. From experience, that behavior is a pattern that I personally must work hard at to avoid. In this article I talk about mentally digging myself out of that hole while extending the consistency checks for the PyMarkdown linter to tokens beyond the initial token. Introduction At the end of the last article , I talked about how I started chasing a feature down multiple rabbit holes, noticing that behavior a good 4-5 days after I felt that I should have noticed it. As part of who I am, I find certain types of puzzles very hard to put down and almost addictive in nature. During the development of the PyMarkdown project, I have had my share of these puzzles, so I have had to be very vigilant that I take a measured approach when solving each issue in an attempt to keep that behavior in check. As this was the first time during this project that I have travelled down this route so completely, I was slightly proud of myself. Usually, for a project of this complexity and duration, I would have expected to descend to those depths an extra 2-3 more times, if not more. From experience, I find that it is during those times that I lose my perspective on a project, and either go for perfection, give up the project that I am working on, or both. I am not sure, but my current belief is that by taking a look at the bigger picture, and looking at it frequently, I have mostly mitigated my desire for perfection by moderating it with other grounding concepts like feasibility. Having noticed that I dug myself into that hole, it was now time to try and figure out what to do. Stay where I was? Go forward? Go backward? It really was my choice on how I handled that situation. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commit of 14 Jun 2020 . Mental and Emotional Stuff Is Important Too I am not one for using excuses when something does not work out, but I am also not one to keep on knocking someone down for failing. Keep in mind the saying I have adopted : Stuff happens, pick yourself up, dust yourself off, and figure out what to do next. For me and my family, there are two parts of that statement are important to realize: the trying part and the figure out/learning part. In all honesty, I have every reason to not write this blog or work on this project. I could spend more time interacting with my family, fiddling around with home automation (how this blog started), watching movies (the more b-movie or \"hard\" sci-fi the better), reading books (see movies), or any number of other things. At the time that I am writing this article, it is just shy of 4 months that I have been working from home, due to COVID-19 . There is the stress related to thinking about the 500,000 fatalities and 10 million infected people worldwide. Let's face it. The mere thoughts of that scale of death and disease are somewhat crippling to me on good days. Add to that the normal stress of making sure I am being a good worker, colleague, husband, and father. Pick any 1 or 2 of those reason, and most people would understand my reasons if I said \"I started writing a blog, but I stopped when…\" But I continue writing this blog and continue developing the project. I have good days and I have bad days, and I also have just, well, days. But I keep on trying. I know I am not going to get the blog right each time, but I try, and I improve. I know I am not going to get the PyMarkdown project right on the first try, and I am probably at my tenth or eleventh try if I add all the small tries together. But I keep on trying. And that is where the second half of the saying comes into play: figure out what to do next. There is a great quote that may or may not be attributed to Albert Einstein : Insanity is doing the same thing over and over again and expecting different results. Figuring out what to do next for me is learning from what just happened, trying to avoid the same mistakes in the future. Does it always work? Nope. Sometimes it takes a couple of times for me to figure things out. Sometimes I do not figure them out and have to ask for help on how to deal with those mistakes or how to prevent them. And in some rare cases, I just need to let them be. But that is also learning: learning what I can do, what I cannot do, and trying to understand the effort required for each. How Does This Relate to My Current Situation? I went down a rabbit hole and lost 4-5 days of productive work, only to realize that when I was completely obsessed with the solution. While I did struggle to find some positive spin on the situation, it was a rough break to deal with. Mentally, it took me about a week to get over the \"oh no, I went there again\" thoughts in my head. Emotionally, I was hard on myself because I had \"let myself\" not pay attention and lost my way. Then add the weight of the elements in my environment that were going on around the world at that time. It took me a bit to work through that quagmire. But I do not believe on giving up on something without a good reason. I have learned a lot about Python, writing, and myself by working on the PyMarkdown project and writing about it in this blog. The things that I have learned are in all sorts of areas, not just the technical areas that I expected. And personally, I have committed to this path: to learning and helping others learn. Basically, I have too many good reasons to keep going, and not many reasons to give up. And yes, it was hard to push myself forward for the first couple of weeks, through all the emotional and mental debris in the way. I did a lot of falling in that time, deciding each time to pick myself up. There were times when the words flew out of my fingers, and times where I could not type the next line in either the project or the blog. But I had confidence that I would work through it, and that I would not give up. This was something I knew I could do, I just had to push through some bad stuff to get there. Where to Start? I knew that I had to push forward with something positive, but what to do? After a bit of thinking, hoping to get myself back into the writing mood, I decided that I would make some enhancements to the work already done. In the last article, I described how I added code to validate the initial token in the token stream. This was done by only adding logic into the function for the cases where the last_token variable was None for any block tokens. The plan for this enhancement was to keep the focus on block tokens, but to focus on the other block tokens in the arrays. That is, focus on the cases where the the last_token variable was not None . Keeping the Scope Small After being burnt by the events documented in the previous article, I wanted to be extra careful by setting myself up for success. If I started with a small scope and found that I was able to get it working quickly, I could always do more work on a subsequent enhancement. Knowing how I work; I knew that it would be far more difficult for me to reduce the work's scope once I had started it. Especially when my confidence needed a boost, I needed a solid, yet achievable, goal that I could complete with confidence. The question was where to start? I had a feeling that dealing with leaf blocks which started on the same line as container blocks were going to be a headache, so I removed them from the scope. I knew that I had issues with tab characters from the last enhancement, so I removed them as well. Was there anything else I could remove? Looking over the scenario tests and their data while scribbling some notes for myself, I quickly came to two conclusions. The first was that verifying the consistency of list-related objects would be relatively easy, as the token contained all the necessary data. Because of the presence of the indent_level variable and extracted whitespace in those tokens, I was able to quickly figure out how to check their consistency. After a couple of mental dry runs with my solution and the real test data, I had confidence I could complete those checks. The second conclusion was that handling the block quote tokens were going to be a completely different issue. Those tokens did not have a concept like an indent_level that could be generally applied, nor any memory of the whitespace that they extracted from the line. Supporting consistency checking with block quotes was going to take some decent changes to the block quote tokens, something that was going to take a lot of time and planning to properly execute. With these considerations in mind, I decided that this enhancement would not address any token arrays with block quote tokens or tab characters. To keep things more manageable, I would restrict any checking to leaf block tokens on new lines. With this in place, it was good to go! But I needed to add one thing first: a consistency token stack. Adding the Consistency Token Stack The necessity for adding this stack to the consistency check was not a surprise to me. To allow the parser to maintain a proper understanding of what the scope of the active container blocks was, I added a stack to the parser explicitly for those container blocks. With the task of validating the consistency of the tokens and without access to that internal stack, I knew that I needed to replicate some of the functionality of that stack. Before any of the serious checking of the tokens took place, I needed a simple stack that would track which container tokens were currently active. The first pass at this was really simple, with the pseudocode being as follows: for each token: if the token is a container block: add the token to the stack else if the token ends a container block: remove the last item from the stack do the rest of the processing This pseudocode is part of my mental algorithm repository that I lovingly refer to as Parser 101. Except in the case of very simple parsers, almost every other parser I have written has had to maintain some manner of stack to track context. While the consistency checker is not a full-fledged parser, it is operating on the tokens generated by a legitimate full-fledged parser. As such, it made sense that some of my parser experience would assist me with this enhancement. The first pass at implementing this algorithm was a simple translation of the above pseudocode fragment into Python code. The if the token is a container block translation was simple, due to a previous refactoring: if current_token . token_class == MarkdownTokenClass . CONTAINER_BLOCK : and the add the token to the stack part of the translation was just as simple: container_block_stack . append ( current_token ) The if the token ends a container block part of the translation was a bit more tedious to implement. This was primarily due to end tokens not having any concept of the token that started that block, only the name. As such, after adding a comment and a note to improve this, the following code was written to figure out if the container block was ending: elif isinstance ( current_token , EndMarkdownToken ): token_name_without_prefix = current_token . token_name [ len ( EndMarkdownToken . type_name_prefix ) : ] if token_name_without_prefix in ( MarkdownToken . token_block_quote , MarkdownToken . token_unordered_list_start , MarkdownToken . token_ordered_list_start , MarkdownToken . token_new_list_item , ): Finally, the remove the last item from the stack was translated into: assert container_block_stack [ - 1 ] . token_name == token_name_without_prefix del container_block_stack [ - 1 ] After all that was added, I quickly ran the tests and… tests were failing all over the place. But why? Getting the Stack Code Working Looking at the stack code, something became obvious almost immediately. If my observation was correct, the processing of the EndMarkdownToken was never being invoked. A couple of quick debug statements verified that observation. Doing a bit more digging, I found out that I had set the class of the EndMarkdownToken to INLINE_BLOCK . This meant at the start of the consistency check loop, the code: if current_token . token_class == MarkdownTokenClass . INLINE_BLOCK : would prevent the EndMarkdownToken from ever getting through. A quick fix changed that statement to: if ( current_token . token_class == MarkdownTokenClass . INLINE_BLOCK and not isinstance ( current_token , EndMarkdownToken ) ): and the test all ran… er… not fine? With echoes of \"what now?\" ringing through my ears, and debugging some more, the answer took a while to find. Using extra debug statements, I was able to pull some additional information out of the tests. It became apparent that the second half of the function had many issues with those end tokens. However, addressing that issue was easy, by adding the following code: if isinstance ( current_token , EndMarkdownToken ): continue after the stack code and before the remaining code. A couple of extra test runs, both with and without debug statements and… everything was working with the new stack code. On to the next issue! Removing Block Quotes With the stack code in place and running cleanly, I needed to add the code to prevent checking for the consistency for block quotes. As previously decided, block quotes were outside the scope of the current enhancement, and as such the following code was added to check for block quotes: found_block_quote = False for i in container_block_stack : if i . token_name == MarkdownToken . token_block_quote : found_block_quote = True if found_block_quote : last_token = current_token continue While this code may seem flawed, it was written this way on purpose. I did not want to have to validate any tokens that were contained within a block quote block, but I did not want to remove the validation of the non-contained tokens either. The above block of code simply checks to see if there is a block quote on the container block stack, skipping over any token while that condition is true. This allows for any block quote contained tokens to be avoided, while still validating any other tokens present in that test sample. Ignoring Tabs for Now The other big thing to take care of was to ignore any tabs that occurred in the initial whitespace for any of the block tokens. This was accomplished by changing the return value of __calc_initial_whitespace to be a pair of values, the first being the same indent_level as before and the second was a new had_tab variable. By setting this variable, it was then possible to change this assert: assert current_position . index_number == 1 + init_ws to: if not had_tab : assert current_position . index_number == 1 + init_ws As I knew tabs were going to be a problem until they are properly and thoroughly addressed, this change circumvented the assert statement when a tab was encountered. Adding the Basic Enhancement With those changes in place and the tests passing, it felt that it was the right time to add the code for the planned enhancement. The basic part of this change was easy, adding an else statement that followed the same line number check, as such: if last_position . line_number == current_position . line_number : assert last_token . token_class == MarkdownTokenClass . CONTAINER_BLOCK assert current_position . index_number > last_position . index_number else : init_ws , had_tab = __calc_initial_whitespace ( current_token ) if not had_tab : assert current_position . index_number == 1 + init_ws This part of the algorithm makes clear sense. Leaf blocks always start on a new line, except for when they are started on the same line as a container block. As such, the token starts right after any leading whitespace. Since this enhancement focuses solely on leaf block tokens on a new line, and due to the previous work to ignore any tabs in the consistency checks, this code was kept simple. Testing this code out in my head, it was all sound except for when the token was in a container block. While block quote blocks were excluded, that still left list blocks. As such, when I ran the tests this time, I expected failures to occur with leaf blocks that are started on their own line but are contained within a list block. As I went through the test run failures, it was affirming to see that each of the failures that were now showing up were squarely within those parameters. Going Through the Failures As I went through the failures from the last set of code changes, I scribbled down notes about the failure patterns that I saw. The big pattern that I observed is, what I felt, was a very obvious one. When a leaf block was created within a list block, the reported column number was always one more than the indent_level variable for the list token. That pattern does make sense, as the list block contains the new leaf block, calculated from the indent level of the list. The first pass at modifying the check to take this into account was the following: top_block = container_block_stack [ - 1 ] if ( top_block . token_name == MarkdownToken . token_unordered_list_start or top_block . token_name == MarkdownToken . token_ordered_list_start or top_block . token_name == MarkdownToken . token_new_list_item ): init_ws += top_block . indent_level This code almost worked but reported errors with the first line. That was quickly addressed with a simple change to the code, checking to make sure that the container_block_stack variable containing the container stack is not empty: if ( container_block_stack ): top_block = container_block_stack [ - 1 ] if ( top_block . token_name == MarkdownToken . token_unordered_list_start or top_block . token_name == MarkdownToken . token_ordered_list_start or top_block . token_name == MarkdownToken . token_new_list_item ): init_ws += top_block . indent_level Dealing with Consistency and Lists At this point, the only tests that were failing were tests related to list blocks. Most of those tests were either contained in the test_markdown_list_blocks.py module or the test_markdown_lists.py module. Doing a bit more digging, the reason for a lot of the failures was visible within seconds of looking at the failing scenario tests. To make sure that lists and their inevitable sublists can be controlled properly, the indent_level variable contains the required indentation from the beginning of the line, not since the end of the last list block. To complement this, any whitespace that occurs before that list block is saved within the list block token. The effect of this is that both ordered and unordered list start tokens did not require any further modification, containing all the required information within their own token. To address this, the check for adjusting the calculation if the token is in a list was changed to: if ( container_block_stack and current_token . token_name != MarkdownToken . token_unordered_list_start and current_token . token_name != MarkdownToken . token_ordered_list_start ): Furthermore, on additional examination, blank lines were in a similar situation. Blank lines contain complete information on how they were derived, so no manipulation of the token's column number was required. Hence, another slight modification of the check resulted in the code: if ( container_block_stack and current_token . token_name != MarkdownToken . token_blank_line and current_token . token_name != MarkdownToken . token_unordered_list_start and current_token . token_name != MarkdownToken . token_ordered_list_start ): Almost There Running the tests again, the number of failures remaining was now a much smaller set of tests. Once again doing my due diligence, I discovered an interesting issue with the new list item token: I had left out an important field out of the token. Unlike the ordered and unordered list start tokens, the contents of the new list item token only had the indent_level field, with the extracted_whitespace field being added to try and solve the failures. But without the proper values for the extracted_whitespace field, it was not a proper solution to the problem. I even made modifications to the container block stack to properly deal with the new list tokens. But even with those two changes in place, it just was not enough to solve all the issues. Once I determined that it was not enough, I knew that I had to fix this properly at a later date and close out this enhancement. To accomplish that, I made a small change to the code as part of the main consistency check: elif current_token . token_name == MarkdownToken . token_new_list_item : # TODO later pass I did not feel that it was a great solution, but it was a decent stop-gap until I could address those tokens. The Inevitable Bug Fixes Along the way, there were some small issues that were uncovered in the parser code, fixed almost immediately due to their small nature. There was a small fix made to the list_in_process function to make sure that the line was adjusted properly in some edge cases. There was another fix to the __adjust_for_list_start function to make sure that it returned an indication of whether it processed the list start index or not. And finally, even though the tabs were not part of the enhancement, there was a bit of cleanup to do in the area of the dual-purpose code used to handle list indents and block quotes. None of these were big bugs, but little issues that the consistency checker was uncovering, and getting dealt with up front before they became a larger issue. And that is where I stopped. I knew that if I started trying to get more of the tab code to work, it would be another rabbit hole. Reminding myself of the big picture and the current goals for the enhancement, I backed off. And after a couple of minutes, I felt good about it. Tabs would wait for another enhancement, and then I would spend a good amount of time working on tabs. But for now, it could wait. Importance of Starting Small I needed this enhancement to be a win, and I think I achieved that by keeping my goals in mind. Rather than trying to do everything and having my effort be scattered, I kept my work focused within the boundaries of a simple goal, one that I had confidence that I would be able to achieve. With this progress, I felt more confident about myself and the project, and I knew I was on my way back to building a solid and stable foundation that I could build further on. While my previous experience guided me towards this approach as I needed a win, it is a solid approach regardless. One of my managers at a previous company used to say: It is better to under promise and over perform than the other way around. I prefer another quote that was relayed to me by a friend at my current company: Having a conversation with someone about resetting expectations is always troubling for me. I would rather my reports take small bites of work and get better estimates on those bites of work, than take huge chunks of works and get lost and need help finding their way. Those words have stayed with me for a long while, and I have tried to live up to the essence of that quote. I cannot tell anyone when the PyMarkdown linter is going to be ready. The scope is big enough that I cannot accurately judge that span of time. But I can let people know what my next 3 items on the issue list are, and approximately how long it will take to complete them. Those items are my small bites, something I can solidly analyze and realistically figure out a time frame for their completion. For me, I feel that it is better to complete 10 tasks that work together than to complete 1 task that will not work until it is completely done. There just is no comparison in my mind. And having just completed one of those smaller tasks, my confidence was returning. What Was My Experience So Far? After my experience with the rabbit hole called Tabs , I was worried that I was going to focus more on the negative aspects of the project, rather than making some good progress. Would I repeat the behavior that I documented in my last article, or would I correct that behavior and move the project forward? While the enhancements I made to the consistency checking were small, I was convinced that they were solid enhancements. That was exactly the type of win that I needed. This enhancement was not about technical feasibility, it was about getting myself out of a negative headspace. I believe it is true of most people that we are own worst enemies, as we are privy to those thoughts and emotions that we keep private. I know that I am my own worst enemy for those reasons. And while I did try hard with the last enhancement, I know I was harder on myself that others would have been. Not for going down the rabbit hole, but because I did not notice I had slipped down there and needed to get out of there before getting lost. I started this enhancement trying to get rid of the \"oh no, here we go again\" mindset. Based on where my confidence was at finishing this enhancement, I am proud to report a lot of positive work has been done. My confidence was by no means back to where it should be, but I was on my way there. I developed a plan, made certain limitations, and stuck by those limitations while delivering a well thought out enhancement. This was definitely a step in the right direction. What is Next? Having added a few issues to my issues log, mostly to verify certain scenarios I tested or wrote about, I knew that I had some technical debt building up. Along with the new list item token issue, I felt it was a good time to build up some more confidence by knocking some more of those issues out of the way.","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/06/29/markdown-linter-rabbit-hole-3-trying-to-dig-myself-out/","loc":"https://jackdewinter.github.io/2020/06/29/markdown-linter-rabbit-hole-3-trying-to-dig-myself-out/"},{"title":"Markdown Linter - Rabbit Hole 2 - Losing My Way","text":"Summary In my last article , I talked about starting to add consistency checks for the line/column numbers by adding simple line number checks. In this article, I document the work that I performed to add column checks to the project in a similar fashion to. While I would like to say that the required effort was simple and the work was quickly accomplished, as the title of the article implies, there were issues along the way that I could have handled better. Introduction At the end of the last article , I talked about how I felt that the right move at that time was to move from the basic consistency checking of line numbers to adding a similar level of checks for column numbers. My main argument for switching was that, from experience, it was easier to track the line numbers when I manually verified the line/column numbers than it was to track the column numbers. For line numbers, unless it was a list block or a block quote block, a new block always meant a new line, which was easy to keep track of. For those two container blocks, I had to check to see if a new block followed the container block, but the calculation was still relatively simple. The calculation of column numbers was more detailed, and therefore more difficult for me to accurately calculate on-the-fly. If not contained within a list block or a block quote block, I looked at the whitespace at the start of the token and adjusted the column based on the amount of whitespace. If it was within a container block, I looked at the amount of indent imparted by the container block and adjusted from there. And while these calculations may seem straightforward and easy, I initially double and triple checked these values before moving on. Even then, when I was checking the consistency of the line numbers, I know that I got a decent handful of column numbers wrong. In the end, after getting some proper perspective, I believe that the situation became an easy one to resolve. I had the easy cases for line numbers done, and that was good enough for a while. I needed to get the column numbers to a confidence level where I felt comfortable going back to the line numbers and finishing off those checks. Even though I was nervous about the mistakes that I would find, I buckled down and started to work. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits after 30 May 2020 up to 11 Jun 2020 . In Case You Did Not Notice Looking at the dates in the previous section, the duration between the start of the column number work and the end of the column number work was 12 days. To be clear, this was not all the column number work, just the easy parts of the column numbers. The batch of work documented in this article took a lot longer than my normal 5-day to 7-day work time. It was for reasons like this situation that I named the articles in this part of the series as \"rabbit hole\" articles. Using that term at work a lot, it was amusing to looking around for a good definition of what \"rabbit hole\" really means. After some fun research, I settled on this specific part of the article at dictionary.com's slang dictionary : But as Kathryn Schulz observed for The New Yorker in 2015, rabbit hole has further evolved in the information age: \"These days…when we say that we fell down the rabbit hole, we seldom mean that we wound up somewhere psychedelically strange. We mean that we got interested in something to the point of distraction—usually by accident, and usually to a degree that the subject in question might not seem to merit.\" That really does capture what happened here. But I do not want to spoil the story too much. Just continue reading and keep this in mind. Keeping It Simple to Start As with all good tales, the work started off with good intentions. As stated above, I simply wanted to replicate for column numbers, the extra confidence that I achieved with line numbers. To that extent, the first stipulation that I made was that, if possible, I was going to leave the container blocks until later. Container blocks were going to easily make things more confusing, so leaving them out would free my mind up to concentrate solely on the leaf blocks. While I was not sure what impact that rule would make, I hoped that it would reduce the large amount of work that I was anticipating. Taking Time to Come Up With A Plan With container block tokens (via the previous section) and inline tokens (as noted in previous articles) excluded, I was able to concentrate all my effort on the leaf blocks. While it was tempting to dive right into the work and to \"clean everything up\", I knew that I needed more of a plan than \"clean everything up\" or \"check all leaf blocks\". I needed to take a good survey of the existing tokens and their resultant HTML and determine what the intent of the token is and how it translates into HTML. How did I know to do that? When I was going through the different scenario tests while implementing the consistency checks for the line numbers, I noticed that there seemed to be 2 different classes of column numbers for leaf tokens: one class where the whitespace occurs before the token and one class where the whitespace seems to be part of the token. It took a bit of research and looking again at the tests, but I did verify that there were at least 2 classes of leaf tokens. Confused? Let me help! Organizing Classes of Blocks By Their Intent Fenced code blocks are blocks where any whitespace that occurs before the token is recorded in the token, placing it solidly in the before class. Looking at the Markdown for example 105 : ``` aaa ``` and the Markdown for example 106 : ``` aaa ``` in the GFM specification, both fenced code block Markdown fragments produce the exact same HTML output: < pre >< code > aaa </ code ></ pre > Note that any whitespace before the code block starts is removed and does not appear in the output. While I need to add that whitespace back in to validate the column numbers properly, it makes no difference to the HTML that is output. As all the whitespace occurs before the token itself starts, I decided to call this class of leaf blocks the before class. On the other hand, example 86 shows Markdown text for an indented code block: foo bar that produces the following HTML: < pre >< code > foo bar </ code ></ pre > In this case, the indented code block is more aware of the whitespace that occurs before the block starts, putting it firmly in the contains class. Specifically, this class of blocks is for any leaf block where the whitespace makes up even a small part of the block itself. This is obvious with this block as the whitespace starts at column 1 but continues in the produced HTML within the <pre><code> block. To prove that these leaf blocks are in the correct classes, I replicated the above examples, but deleted a single space character from the initial whitespace for each code block. That change did not make any difference to the fenced code block, but that change precipitated the removal of one space character from the HTML output for the indented code block. Due to that simple behavior, it was simple to place every leaf block type into one of the two classes, allowing the blocks to be mostly handled as a group of blocks instead of as individual blocks. Aside To keep things above board, there technically is a third class, but it is mostly hidden. In the case of blank lines and link reference definitions, the actual HTML output for both elements is that they are ignored. Therefore, technically the third class is for ignored tokens, but for the purposes of this classification, I assume they are in neither class. Classifying the Blocks With that newly acquired information in mind, I walked through similar examples and assigned each of the leaf tokens to one of these three classes: before, contains, or ignored. thematic breaks: before, see example 17 atx headings: before, see example 38 setext headings: before, see example 54 indented code block: contains, see example 86 fenced code block: before, see example 106 HTML block: contains, see example 120 link reference definition: ignored, see example 162 paragraphs: before, see example 192 blank lines: ignored, but treated as contains, see example 197 Keeping Things Consistent In looking at how whitespace was handled for each of the leaf block tokens, one thing did indeed stand out: I was not storing whitespace for each of the tokens in the token itself. While it was being passed down to the base class, there was not an explicit variable named extracted_whitespace for each token that stored this information. Some tokens had that variable, some did not. While I could calculate this information on a token by token basis, it seemed simpler to add the following line: self . extracted_whitespace = extracted_whitespace to each of the constructors that were not assigning the argument to a member variable. Specifically, the tokens for block quotes, list starts, indented code blocks, html blocks, and thematic breaks needed this change, and it was a very quick change to perform. The important thing about this change was that, going forward, each of the non-ignored tokens contained a variable that explicitly was there to contain any extracted whitespace. As this variable was now in most block tokens, I hoped that the group of tokens could now be processed largely as one group, instead of 11 distinct block tokens. Player, Enter Rabbit Hole Level 1 When I implemented the column numbers to begin with, I honestly thought there was only one class of tokens, and therefore I treated them all as if they were in the before class. To that extent, when I calculated each of the column numbers for the leaf tokens, I started with the number 1 at the first position in the line and incremented by 1 for each space character present in the data. Effectively, I used the formula: column_number = 1 + len ( token . extracted_whitespace ) But now that I knew there was also a contains class, it meant adjusting the formula slightly. Additionally, based on further observations, it looked like the HTML blocks and the indented code blocks were going to need slightly different formulas. HTML blocks were the easiest one to observe and determine the pattern for. Based on the Markdown from example 120 : < div > * hello * < foo >< a > it was obvious to me that the Markdown produces output that is exactly what is contained within the block, namely: < div > *hello* < foo >< a > Based on the transformation, an identity transformation to be exact, it made sense that the formula for HTML blocks is: column_number = 1 The reasoning for this simple calculation is based on the observation of the transformed HTML. As any text that is part of the Markdown HTML block is part of the resultant HTML, I interpreted that to mean that the block itself starts at the start of the line, hence a column number of 1. Following a similar line of investigation, I looked at a good example of an indented code block, the Markdown for example 86 : foo bar and its resultant HTML: < pre >< code > foo bar </ code ></ pre > Based on this transformation, after the first 4 spaces are removed from the start of each line, those lines are inserted into the HTML output, with a <pre> <code> around them. As the first 4 spaces are removed from each line, they are effectively part of the token, but in a weird way. In the above example, the first 4 spaces are swallowed while the remaining spaces are left intact. Based on this behavior, I interpreted that to mean that the block itself starts after the 4th space, even though there are more spaces before the non-space text starts. Given that, the formula is: column_number = 4 but with a special provision that any unused text is added to the start of the text token that follows the indented code block token. Now that I had the behavior down, it was on to implementing the code for the consistency check. Congratulations Player, Now Enter Rabbit Hole Level 2 Having a solid idea of the behavior that I expected from each contains token and the group of begins tokens, it was time to write the consistency check. To allow for an easy computation of what the initial positioning was, I created the __calc_initial_whitespace function, starting it off with the equivalent of: 1 def __calc_initial_whitespace ( calc_token ): if calc_token . token_name in ( `every token` ) indent_level = len ( calc_token . extracted_whitespace ) else : assert False By isolating all the various formula into one function, I was to be able to use this simple code to implement the check itself: init_ws = __calc_initial_whitespace ( current_token ) assert current_position . index_number == 1 + init_ws Fixing the Easy Failures After running the scenario tests with this new code in place, there were a lot of successes, but also failures that I needed to deal with. The first set of failures was with various tokens that did not have a properly implemented extracted_whitespace member variable: HTML blocks, blank lines, and SetExt headings. The HTML blocks were the easiest to deal with, as the formula from the previous section always returns a column number of 1. I added code to always return an index of 0 for the length of the initial whitespace, resulting in a column position of 1 once the above code was applied. From there, blank lines were equally easy. While there are good arguments for having blank lines denote the start or the end of the line, I picked the start of the line to keep things simple. This made the coding easy, as these tokens ended up returning 0, the same value as for HTML blocks. That left an oddball: SetExt headings. For some reason that is lost to me, SetExt headings use the remaining_line member variable instead of extracted_whitespace . While it felt weird, it was easy to do a quick check for that, returning the length of that variable when SetExt headings tokens were encountered. I also added an issue to my list to check this out in the future, as it is an unnecessary complication that does not add any benefit. Running that scenario tests again, most of the issues were dealt with easily, following the above rules, and adjusting the column numbers to the newly corrected values after additional manual verification. It was only a small handful of errors that showed up, mostly in the HTML block scenario tests and the indented code block scenario tests. As those two blocks were ones that I placed into the contains class, these failures were expected and quickly addressed, applying changes to the scenario tests to resolve those failures in short order. Paragraphs Were a Bit More Work The next group of failures were easily grouped together as scenario tests that deal explicitly with paragraphs. On closer examination, it seemed that the failures occurred only in a couple of specific cases. After a couple of observations, it became obvious that the failures were restricted to multiline paragraphs. In those multiline paragraphs, all the whitespace stripped from the start of each line in the paragraph is added to the token, not just the first line. To address that, I made some quick changes to the code for dealing with multiline paragraph tokens: elif calc_token . token_name == MarkdownToken . token_paragraph : if \" \\n \" in calc_token . extracted_whitespace : indent_level = calc_token . extracted_whitespace . index ( \" \\n \" ) else : indent_level = len ( calc_token . extracted_whitespace ) With the paragraph tokens dealt with, the column numbers now lined up nicely between my calculated numbers and the consistency check numbers. Other than tabs, the only outlying scenario tests were two tests for indented code blocks inside of list blocks. Another Rabbit Hole To be honest, during the writing of this article, getting a firm understanding of the status of these two scenario tests took a couple of passes. I had that understanding when I completed the tests, then forgot it before I wrote the article. When I started to write article, I figured it out again, then forgot it while focusing on other aspects of the article. Finally, when I found the correct answer again, before I forgot the explanation yet again, I quickly jotted down notes for this section. Yes, column numbers can be confusing. Keep good notes. Trust me on this. The two list block scenario tests that I had issues with were example 235 and example 252 . For example 235: - one two due to the extra indentation, the top paragraph starts at column number 7. However, as the lower paragraph does not have enough spaces to match that start column, it is instead interpreted as an indented code block. 2 As such, that indented code block starts at column 5, per established rule from above, and contains text that starts with the 1 extra space. As the column number in the test was 6, it was adjusted to the now correct value of 5, with manual verification and consistency check verification in place. From personal experience, it was easy to forget that the indented code block starts after the 4 space indent, making the calculation look wrong. For example 252 : 1 . indented code paragraph more code it was more fun with lists, but this time it was an unordered list that contained a valid indented code block, unlike the example above. In this case, the list start sequence does not have any whitespace before it and is a simple 1 digit list start. From a calculation point of view, that means start with 0, add 0 to that total for leading whitespace for the list start, 2 to that total for characters in the list start sequence, and then add 1 to that total for the mandatory list-to-content whitespace separator. That means that the earliest that any content can start in the list is at index 3, matching the list's indent_level member variable. As the next token is an indented code block, adding 4 to that total results in an index of 7 or a position/column of 8 where the content for the code block starts. As the column number in the test was 9, it was adjusted to the now correct value of 8, with manual verification and consistency check verification in place. As I mentioned in my introduction to this article, the calculation for column numbers were more detailed. It was not until I wrote down the formulas for the calculation, as outlined above, that I was able to confirm that I had made the right choice. And with that, all the scenario tests were passing except for tabs. Yeah, tabs again. Be Wary Player, Now Enter Rabbit Hole Level 3: Tabs As I looked at the code, dreading a lot of changes to the code to support tabs, there was both good news and bad news. The first part of the good news was that except for the tab scenarios, the rest of the code was solid. I had found a couple of issues so far, but otherwise column numbers looked good. The second part of the good news was that anything to fix with respect to tabs at the start of a line would be largely restricted to the indented code blocks. Until I started testing with container blocks, any blocks that started with any combination of tabs and spaces would always trigger the 4 whitespace minimum required to start an indented code block. The bad news? This was not going to be fun. As I documented in More Fun With Tabs , Markdown uses tab characters as tab stops. To remove the complexity of handling those tabs at the start of every block, I did the necessary computations to translate tab characters into the correct number of spaces, based on the current index in the Markdown document. From that point on, any leading whitespace was tab-free and easy to work with and manipulate for the transformation into HTML. But then I got to validating tokens for consistency with respect to column numbers. After clearing away all the other failures, only 13 failures remained, and all them dealt with tabs and specifically with tabs in indented code blocks. How bad could it be to reverse the change and pass the initial whitespace through? I started working the problem. Three days later, I determined that it would be very difficult. After 2 restarts, there was just too much code already in place to handle that relied on all leading whitespace being space characters. Replacing it would require more time to just get it to a good starting point, not even to the point of having the tests passing. Resetting for a third time, I decided to just focus on the initial case of having a tab in the starting whitespace that caused an indented code block. Instead of working to reset the tab in all the code, I focused on reconstructing the leading whitespace to determine what the proper handling of the tab should be. Another three days later, and the code was complete. When the initial whitespace is encountered, a check is made against the initial line to be parsed to see if that line contains a tab. If so, further progressing is done to extract the original whitespace that directly corresponds to the initial whitespace that caused the indented code block to be started. Once the original whitespace fragment was recovered, the extracted whitespace in the indented code block token was adjusted, placing the proper portions of the leading whitespace in the code block token and the following text block token. And it also provisionally supported lists and block quotes. Why Did I Need to Do This? Basically, before this work, the whitespace that was in the leaf block tokens did not have to be correct as it was largely ignored. As such, only the extracted whitespace stored in the enclosed text block had to be correct, as that whitespace is directly exposed when rendering the token into HTML. To make this more evident, consider the Markdown text for example 7 : -< tab >< tab > foo where <tab> is a tab character. As the specification explains, there is a bit of calculation trickery in this area. The list start character - must be followed by a space, so when the first tab character is encountered, 2 spaces must be emitted to replace the tab stop. When the next tab character is encountered, we are already at a tab stop, so that tab is replaced with 4 characters, for a total initial whitespace of 6 characters. Why was this important? Because I had to make sure the right whitespace was being placed in the right token, otherwise either the HTML validation would fail, or the new consistency checks would fail. Before this consistency check, if I placed the needed 2 characters in the HTML text, the test passed. Due to the check, I now had to also properly calculate the whitespace to put in the indented code block tokens itself. As I finished up work on the changes, I looked at the calendar… and noticed that instead of my usual one week of work, it was 12 days later. What I had budgeted 5 days for had taken 12 days. What went wrong? Looking back, I simply messed up. To me, there is nothing wrong with messing up with something like this, if I learn something and grow. So, it is just a matter of taking a good look at what happened, and being honest with myself. The big thing that happened is that I believe that when I saw that it was going to be a lot of additional work to support tabs, I should have stopped to re-evaluate what needed to be done. Instead, I kept on pressing forward instead of evaluating whether tabs were worth it at that time. As I personally do not use tabs, and many of the documents that I surveyed do not have tabs in them, I could have sidelined tabs for another day. In addition, once I started to work on implementing the proper tab support for the tokens, I ignored additional warning signs. Thinking back, I believe that I kept on thinking \"just around the next corner\" and \"one more line\", instead of being honest about the work. Essentially, I let my pride get in the way. Instead of being okay with leaving a partially implemented solution or resetting the work, I was convinced that it was just another 5 minutes away… even after 5 days. On top of that, I mostly ignored my rule about leaving container blocks out of the current block of work. To get to this point with the tabs, I had to write extra code that is specifically in place to deal with indented code blocks within container blocks that contain tabs in the leading whitespace. That is doubling down on dealing with container blocks, not leaving them for later. I need to think about things. How did I feel about all this? What Was My Experience So Far? As I have mentioned before : Stuff happens, pick yourself up, dust yourself off, and figure out what to do next. Yeah, I messed up. Stuff happened. Instead of doing what I normally do, I went down the rabbit hole and got lost in trying to get the column numbers right, not just \"good enough\". Even after I started implementing the tab support, I did not pay attention to my own warning signs that were telling me to reset and regroup. As for the dusting myself off, I realized that there was some good news out of all this work. Good news from all this? Really? The biggest part of the good news is this is the first time that this kind of thing has happened on this project. Yeah, I was hyperfocused on getting the work done, and did not pay attention to anything else. But unlike other projects where this happened multiple times throughout the same project, this was the first time for this project. One instance of this for a project that has lasted 8 months… not bad! This is not just me making lemons out of lemonade , I was genuinely happy. Granted, it took me a bit of self-reflection to get there, but I got there. And on the way there, I did notice things. One thing that I am confident about is that even though having to take this route with the tab characters is painful, it would have been more painful to have to deal with those tabs in multiple places. The current implementation for leading whitespace removes the tabs, only adding them back in for the few cases where they are needed. Another thing is that although I needed to address a couple of issues with 2 classes of leaf blocks, the calculations for the column numbers were mostly spot on. I still want to make sure remain consistent by having consistency checks, but I am more confident that I calculated the column numbers correctly. Sure, I got distracted. It happens to everyone, especially with projects that we are all passionate about. I sincerely wanted to do the right thing, and that is not bad, just counterproductive at this time. Right now, I need \"good enough\", not \"perfect\". While this was indeed a setback, it was a relatively small setback, one that I can easily recover from. Overall, I was a bit bruised from following tabs down the rabbit hole, but I was okay with it. Not proud of it, but also not blaming myself and flogging myself for it either. Just okay with it. And while I did go overboard, I did get the initial scope of work done. In the end, all is good. What is Next? After focusing a lot of time on went wrong, it took a bit for me to realize that the consistency checks were working as planned. But on further examination, with possible influence from my issues with hyperfocusing, I decided that I was not yet at the point where I could switch back to line numbers. As such, my next article will talk about how I continued this work verifying column numbers. The text every token is not meant to be taken literally. Instead of listing each of the 11 tokens, I just felt it was more compact to use a figurative value. ↩ The case where the second paragraph's column matches the indent of the list item is tested in example 236 . ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/06/22/markdown-linter-rabbit-hole-2-losing-my-way/","loc":"https://jackdewinter.github.io/2020/06/22/markdown-linter-rabbit-hole-2-losing-my-way/"},{"title":"Markdown Linter - Rabbit Hole 1 - Adding Consistency Checks","text":"Summary At the end of my last article , I talked about how I felt a lack of confidence in the work that I had just completed, primarily due to the large number of scenario tokens that I added line/column numbers to. This article documents the work to increase my confidence in those line/column numbers by starting to add consistency checks to all the scenario tests. Additionally, I talk about the bad line/column numbers that were found by the new consistency checks and how I addressed those issues. Introduction Why am I referring to this work as a rabbit hole ? It is because this kind of verification can be easy, but if things are not done right, the verification itself can be very messy and seemingly endless. As with many things in this project, whether or not to do this is a balancing act between the risk of doing (or not doing) it, the cost of doing it, and the impact if the risk comes to pass. The risk of not doing this type of verification is that I may have miscalculated one of the line number/column number pairs for one of the tokens in the 700+ examples that make up the scenario tests. The cost of doing this is a small amount of code for the verification part of the code, but it could also add tons of code to the actual parser to track additional elements. The impact is even more difficult to pin down. While part of the impact is measurable, namely the number of line number and column number errors found, the other part of the impact is not measurable: confidence. There is my confidence that I have those numbers right and there is the confidence of any users that I have those numbers right. And if I do not reach the minimum confidence level for any users of the project, I am certain that I will lose those users. It will be a balancing act that I will need to be aware of, and to monitor going forward, even after this work is finished. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits after 30 May 2020 up to 14 Jun 2020 . A Quick Aside Yes, one of these again! I have been trying to figure out a good way to talk about the line number and column numbers as a pair, trying out various combinations over the last article and during the writing of this article. The most descriptive and most compact wording that I believe captures the information is: line/column number. It both indicates that they are a pair and that the pair has two distinct parts. From this point forward, I am going to strive to consistently use this format. Starting with Some Refactoring I am always looking for ways to make the project better, and this was no exception. During the addition of the position_marker variables throughout the parsing code, I noticed there were four variables that were usually getting passed around together: token_stack , token_document , close_open_blocks_fn and handle_blank_line_fn . While these variables might have slightly different names in the functions that used them, their usage pattern was very consistent. In each case, they were defined once in the TokenizedMarkdown class, and passed without change down throughout the rest of the parser. If Python had a good concept of a read-only variable, I would have used that concept to decorate these variables. Another alternative was to make these variables static variables, but it did not feel right to me to make them into 4 different static variables. class ParserState : \"\"\" Class to provide for an encapsulation of the high level state of the parser. \"\"\" def __init__ ( self , token_stack , token_document , close_open_blocks_fn , handle_blank_line_fn ): self . token_stack = token_stack self . token_document = token_document self . close_open_blocks_fn = close_open_blocks_fn self . handle_blank_line_fn = handle_blank_line_fn As I did want to reduce the overhead of passing these around, I created a new class ParserState , initialized an instance of it in the __parse_blocks_pass function of the TokenizedMarkdown class, and added those four variables to that class as member variables. Then I worked my way through the parser, passing that object down further and further into the parser. Along the way, where possible, I removed PyLint too-many-arguments warnings and a couple of too-many-locals warnings as well. To me, it just left things a bit cleaner, and ready for the next step: starting to add the consistency checks. Why not static variables? Perhaps it is my object-oriented background or perhaps it is my test automation background, but both of those backgrounds note that static variables are bad. From the object-oriented point of view, static variables cause issues because you must be very aware of what is changing and observing those variables at every moment. From the test automation point of view, static variables are just hard to test. Because everything in the project that uses that static variable has access to it, it can be the result of a direct or indirect change from almost anywhere in that project. If it is only accessible from a static method, then there are problems with clearing out the value of that variable so it can be properly tested under all circumstances. Basically, most of my experience tells me to stay away from static variables if possible. Starting with the Plumbing With any sort of verification code, it is always a good idea to start with something simple, slowly building on top of that work. As this was just normal verification code, the first thing I did was to add this function to the utils.py module: def assert_token_consistency ( source_markdown , expected_tokens ): pass Just a simple function that could be called from each of the scenario tests, starting out with a skeleton function that does nothing. With that in place, I started the laborious task of going to each of the test_markdown_*.py files and changing the tests at the end of each scenario test from: # Assert assert_if_lists_different ( expected_tokens , actual_tokens ) assert_if_strings_different ( expected_gfm , actual_gfm ) to: # Assert assert_if_lists_different ( expected_tokens , actual_tokens ) assert_if_strings_different ( expected_gfm , actual_gfm ) assert_token_consistency ( source_markdown , actual_tokens ) This was a simple task, but by making sure it was completed, I was able to go ahead and add in any consistency checks between the Markdown and the tokens without wondering if I had missed any of the tests. And yes, I was paranoid and double checked all the scenario tests at least one more time to make sure I did not miss any of the tests. Why did I take such a small step? While other people's mileage may vary, I find that adding multiple consistency checks in a single step compounds the errors that can occur if you get that check wrong. Unless I can avoid it, I choose small changes to the verification that I can easily validate manually, getting those changes addressed solidly before moving on. The base calculus that I am betting on is that the time taken to complete multiple steps independently is less than the time taken to combine them together. Based on my experience, including implementing, testing, debugging, and maintenance as factors into that equation usually favors the independent steps. Verifying the Line Numbers Start At 1 I was sure that verifying column numbers was going to be messy, but I was confident that verifying line numbers would be more simplistic. When it comes down to it, the base part of verifying line numbers is making sure that each block token starts on its own new line, except for a block that is within a container block. In those cases, and only those cases, can the line number be the same. But even before I started down that path, the first verification that I wanted to do was to make sure that each array of tokens started with a block token on line 1. It was the simplest verification that I could think of, and from previously discovered issues, I knew there were at least a couple of reported issues in this area. If I coded it properly, not only would I validate that the verification was working, but I could also detect and fix an issue or two at the same time. Classifying the Tokens Up to this point in the project, I had little need to classify any of the Markdown tokens as anything other than Markdown tokens. When I addressed a block of tokens, those tokens were always as a single token or as a small group of 2-3 tokens. But, to verify the line numbers properly, I was going to need to group the tokens on a larger scale. To accomplish that, I was going to have to add classification to the tokens. In re-reading the verification steps outlined at the start of the previous section, I realized at the minimum I was going to need to add the concept of each token belonging to a specific class of token. In this context, I was defining the class of the token as the grouping assigned to the Markdown element in the GFM specification. Using that as a template, along with some previous work with Python enumerations, I was able to quickly set up this enumeration: class MarkdownTokenClass ( Enum ): \"\"\" Enumeration to provide guidance on what class of token the token is. \"\"\" CONTAINER_BLOCK = 0 LEAF_BLOCK = 1 INLINE_BLOCK = 2 While Python supports four types of enumerations, I was confident that the simple type of enumeration was the right fit to solve this issue. I did not need the enumeration to do anything fancy, just provide a simple property value for each token that specifies what the class of that token is: container block, leaf block, or inline. With the enumeration in place, it was a simple task to go through all the tokens in the markdown_token.py module and associate each token with its respective class. Using the GFM specification as a guide, each token was matched up to its owning element and assigned the proper class instance for that element. Adding the First Verification With each token classified, the first part of the token verification was to only focus on the blocks, as inline line/column numbers were not yet implemented. Using the new classification variable token_class this was easily accomplished by adding a simple loop over all the non-inline tokens: for current_token in expected_tokens : if current_token . token_class == MarkdownTokenClass . INLINE_BLOCK : continue Next, this was expanded to remember the last token, as such: last_token = None for current_token in expected_tokens : if current_token . token_class == MarkdownTokenClass . INLINE_BLOCK : continue if last_token : pass else : assert current_token . line_number == 1 last_token = current_token While these were not big changes, it was a good start. It made sure that the first token in the document was always positioned on line number 1, which it should always be. If not, the verification code asserts on the failure. Simple. And while I could have added more checks, I decided that was complex enough for now, and stopped there. As I mentioned above, taking small steps with the verification is a far safer bet. And as I would soon discover, it was already bearing fruit! Looking for Issues - Link Reference Definitions When implementing the line/column numbers for each block tokens, as documented in the last article , I noted down scenario tests that looked dodgy for later examination. At that time, I tagged the scenario tests for examples 166 and 168 as being incorrect as they started off with an extra blank line that started on line 2. With this new verification code in place, when those scenario tests were executed again, those new verification tests failed right away. The output of the test was not an issue, as the HTML output for a blank line token in the current output formatter is to output nothing. However, when I added in the line/column number check, it failed as it was the first block token in the output, and it reported that it started on line 2. The blank line was indeed on line 2, but the token was being output at the start of the token array, and then again in the middle of the actual_tokens array. After some digging, I was able to find the error. During the processing of the blank line, the parser checks to see whether there is an active link reference definition, and if so, stops the definition. However, regardless of whether a definition was found, that blank line token is appended to the token_document array. With any other token, this would be the correct thing to do. However, as the processing of the link reference definition requires rewinding on failures , the blank line was output, the parsing was rewound for the failed link reference definition, and then the blank line was output again. Once I figured out what the problem was, it was easily remedied by seeing if a rewind was in progress by checking the force_ignore_first_as_lrd variable, and only emitting the token if it was not set. After some double checking, the scenario tests for example 166 and 168 were executed again, verifying that they were indeed fixed. I then ran the tests again to see what other tests were failing, and it was a mass of SetExt heading failures that caught my eye right away. Looking For Issues - SetExt Headings It was very evident that more than half of the SetExt heading tests were failing from the scenario test summary line. Performing a quick count of the failing tests, there were 13 SetExt heading tests failures out of a total 25 scenario tests. As I have put significant time into making sure that both the SetExt and Atx headings are working properly, I was surprised to see that there were any verification failures, let alone 13 of them. Within seconds of looking at the first of those tests, it was easy to see why the verification was failing. In the scenario test for example 50, the first part of the Markdown text is: Foo * bar * ========= but the tokens being output by that block of Markdown text was: \"[setext(2,1):=:]\", \"[text:Foo :]\", \"[emphasis:1]\", \"[text:bar:]\", \"[end-emphasis::1]\", \"[end-setext::]\", From an HTML point of view, the tokens were correct, as the HTML output for that text: < h1 > Foo < em > bar </ em ></ h1 > was correct. But it was also obvious that the line/column number for the first token was wrong. As the first token, it should start on line 1, not line 2. Even more interesting is that if you visually look at the Markdown, it is obvious that it indeed is the first Markdown element in the document. Starting to debug this scenario test, the answer was quick to come. In the processing of a SetExt heading, as talked about previously , the Markdown for a SetExt heading occurs after a paragraph has completed, transforming that paragraph into a SetExt heading. At the time that the SetExt heading token is added to the markdown token array, the position that is associated with the token is the start of the SetExt Markdown indicator. As such, that is the position that is used for the token. And it does make sense that for every token, the position should indicate where that token occurs. Adapting the Token to Properly Represent Its Data Despite the token position being technically correct, from a line/column number verification point of view it did not make sense, and line/column numbers were what was being verified. As I thought about this, I vacillated between representing the token with the position of the SetExt Markdown element and the position of the block of text contained within the SetExt heading. To me, there just was not a good answer. They both had a valid use and a valid reason to be the one position that represented the token. Battling back and forth, I finally came to the realization that I needed to break out of my current thinking that a token must have only one position. While it is not a feature I want to use frequently, this was an honest case where the token should contain two positions: the primary position to contain the location of the SetExt element itself and the secondary position to contain the location of the block of text contained within the SetExt heading. To accomplish this, I added two new variables to the SetExtHeadingMarkdownToken class: original_line_number and original_column_number . These two variables would be used independently of the line_number and column_number variables to track the position of the block of text contained within the SetExt heading. I wired up the necessary code to pass in the correct values for the two new variables, included adjusting the __str__ function to present them as part of the token. When I reran the scenario test for example 50, I was pleasantly greeted with the following new set of tokens: \"[setext(2,1):=::(1,1)]\", \"[text:Foo :]\", \"[emphasis:1]\", \"[text:bar:]\", \"[end-emphasis::1]\", \"[end-setext::]\", Changing the Scenario Tests Now that the SetExt heading token contained a good set of information, I needed to change the scenario tests to understand the new information. Without any change, those tests would still only know about the primary position. To address this issue, I added the following code: def __calc_adjusted_position ( markdown_token ): if markdown_token . token_name == MarkdownToken . token_setext_heading : line_number = markdown_token . original_line_number index_number = markdown_token . original_column_number else : line_number = markdown_token . line_number index_number = markdown_token . column_number return PositionMarker ( line_number , index_number , \"\" ) While it may seem like overkill to some people, the purpose of this function is to keep the addition of this new logic contained within a single function. This encapsulation came in useful when I added it in to the consistency function: last_token = None for current_token in expected_tokens : current_position = __calc_adjusted_position ( current_token ) if current_token . token_class == MarkdownTokenClass . INLINE_BLOCK : continue if last_token : last_position = __calc_adjusted_position ( last_token ) else : assert current_position . line_number == 1 last_token = current_token Used for both the current_token and the last_token , the new function easily provides the right position for both tokens, with only a small change to the target function. In addition, the adding of the last_position variable gave me a bit of a reminder of the direction that I needed to go in with the consistency checks. More on that in a minute. Refocusing myself on the work directly ahead of me, running all the scenario tests yielded 20 test failures, the original 13 tests plus another 7 tests that contained SetExt heading tokens, just not at the start of the array. For each of those 20 tests, I manually checked the new tokens against the Markdown sample, and once I was satisfied that they were accurate, I adjusted the expected tokens for that test to match the new tokens. And this time, when I ran all the scenario tests, I was greeted with a clean slate of no test failures. Before Going On Up to this point, when adding the consistency checks, I was mostly invoking the tests with the following line: pipenv run pytest -m gfm As the new consistency checks were only concerned with the scenario tests, this was the fastest method to run all those tests. However, before continuing, I wanted to make sure those changes did not have any adverse side effects, so I ran the all the tests in the project using: pipenv run pytest While running all the tests may not have been necessary, I wanted to run them to be confident that I did not introduce any bad side effects. As the complete group of tests can be executed in less than half a minute, the cost of the little bit of extra confidence was easy for me to justify. Adding More Consistency Checks With the SetExt heading tokens addressed and all the scenario tests passing again, it was time to add more consistency checking. As alluded to in the last section, now that the line numbers are being verified for the first token, it was time to add verification of container blocks and leaf blocks on the same line. In this case, the smallest step I could take was to check for two blocks that had the same line number. When this condition is true, the previous block must be a container block, otherwise some error has just occurred. In addition, without worrying about column number accuracy yet, I can also state that that the current position must be to the right of the enclosing block. Adding those changes into the consistency checking was therefore just as easy as its description: if last_token : last_position = __calc_adjusted_position ( last_token ) if last_position . line_number == current_position . line_number : assert last_token . token_class == MarkdownTokenClass . CONTAINER_BLOCK assert current_position . index_number > last_position . index_number else : assert current_position . line_number == 1 While this was not checking everything yet, it was a good step forward. While I was certain that I did not mess anything up with the container tokens, it was good to have the validation that I did not miss anything. It was to my surprise that when ran the scenario tests again, two failures were reported. Duplicated Tokens After I got over my surprise at there being two possible container block failures, I started to investigate those two tests. It was with relief that I noticed that both tests, 218 and 219, were noted as being suspicious during the work from my last article. From a quick glance at the test output, it was easy to spot that the check was not failing due to container block issues, but due to almost duplicate blank line tokens present in the actual_tokens array. Because the line number was the same in both tokens, it looked like a case of a leaf block within a leaf block instead of a duplicated token. Debugging through this, it took me a while to figure this issue out. After a lot of head scratching, I finally had added enough debug information that I noticed that in the code for the parse_line_for_container_blocks function, once the calling of the handle_block_quote_block function was completed, the blank line had already been handled, but the processing continued. After verifying this a couple of times, I tried removing the first call to process the blank line, but some of the other cases where I handled blank lines stopped working. While it did seem a bit of a kludge 1 , on the return from the handle_block_quote_block function, I simply verified that the blank line process was handled already. If that, then the tokens were added to the markdown token array, and the function was returned from. As the scenario test for example 261 was also flagged in the same line as 218 and 219, I took a similar look at the output, of that, but did not notice any issues. I even tried some variations on the data to see if there was an error that was previously exposed, and now fixed, and I was not able to reproduce it. Confident that 218 and 219 were fixed and that I could not reproduce 261, I removed those issues from my issues list. Once again, I re-ran the scenario tests, and all the test passed. Executing the entire batch of tests for the project, I was also greeted with a clean set of tests, each one of them passing. Now to figure out what to do next. Adding in More Line Number Checks? Taking a good look at adding more line number verification, I had a hard choice to make. Either I could try and add more line number verification, or I could start working on column number verification. There were good reasons for both paths, but I had to make a choice. I went back and forth on this decision before taking another look at the existing scenario tests, trying to figure out what I was most concerned about. I decided that while it would be nice to get the line numbers completed, I was more concerned about the column numbers. In the token array for each scenario tests, I was able to track the lines and line numbers more easily that I was able to track the column numbers. In the end, that one observation is what affected my decision: I needed to start verifying the column numbers next. What Was My Experience So Far? For any consistency checking being implemented, it is often useful to have a handful of issues that you know should fail, and then see the new consistency logic failing. The new checks passed that part of the test. In addition, the consistency checks add so far pointed out an issue with how I assigned line/column numbers to the SetExt heading token. Having caught and reported failures on those issues, I was confident that adding consistency checking for the line/column numbers was a good thing to do. Seeing as the cost was still very low, it seemed that the cost:benefit ratio was still heavily in favor of continuing, so that is what I decided to do. Going back to my main issue regarding my confidence with line/column numbers and their accuracy, if anything, I think I believed that I had lost ground. With each check finding something, it was both an affirmation that I was right to question my manual verification abilities and a challenge to find more issues with more consistency checking. Basically, it was both \"good job\" and \"what else ya got?\" in the same sentence. If I am honest, I felt it was a bit of a setback. I had hoped it was going to lift my confidence a bit, instead of a give-and-take that balanced each other out. I knew I needed to do something to boost my confidence, and I just hoped that adding column number consistency checks would be the key to that. What is Next? Having reached a good point in my verification for line numbers, it was time to proceed further down the rabbit hole that is consistency checking by starting to verify the column numbers. And what a rabbit hole it would end up being! According to Merriam-Webster: \"a haphazard or makeshift solution to a problem and especially to a computer or programming problem\" ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/06/15/markdown-linter-rabbit-hole-1-adding-consistency-checks/","loc":"https://jackdewinter.github.io/2020/06/15/markdown-linter-rabbit-hole-1-adding-consistency-checks/"},{"title":"Markdown Linter - Adding Line and Column Support","text":"Introduction As I documented at the end of the previous article , having arrived at the decision to improve the linter's foundation, it was time to tackle the largest issue on the stack: missing support for line numbers and column numbers. While at the time I thought of it as a glaring omission (see this article for more ), the passage of time since that discovery has helped me see that omission in a better light. On its own, the process of getting the tokenization correct was a large enough task but adding the determination of each token's line number and column number to that task would have made that task unbearably larger. Knowing myself as I do, even if I had discovered that requirement during the design phase, there is a really good chance that I would have relegated the determination of each token's original position into its own task. Regardless of what happened in the past, it was obvious that I needed to start working on that task now. Starting to think about this issue, I had no doubt it was going to be a tough task to complete, and I knew that I needed to find a way to break things down even further. As I figured that adding line numbers and column numbers to every token was going to be too much, I reduced the scope even further by deciding to limit the scope to only block tokens. As every inline element is rooted in a block element, I had confidence that this would ensure that the line number and column numbers would be solid in each block elements before adding the inline elements into the mix. It was also a solid dividing line for the task that just made sense to me. With those decisions on scope dealt with, it was time to delve into the depths of line numbers and column numbers. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 26 May 2020 and 16 May 2020 . Why Was Reducing the Scope Good? Each of the implemented and enabled parser test functions includes the list variable expected_tokens and the string variable expected_gfm . The expected_tokens variable is used to ensure that the tokens that are expected from the Markdown document are emitted by the parser. The expected_gfm variable is used to ensure that those tokens can use a simple algorithm 1 to transform themselves into the HTML required by the specification. Prior to this work, their focus was simple: enable that forward translation from Markdown into HTML. Adding in support for line numbers and column numbers was going to be a lot of work, but it was working in a different direction: backwards towards the original Markdown document. There was no denying that this was a difficult task to complete, but due to circumstances, the reduction in scope would leave me with a far simpler problem to solve. In addition, to protect the work already done in the forward direction, the existing battery of tests remained in place, diligently guarding that work. With protection for the already completed work in place, and a clear definition of the work to be done in mind, it was time for me to figure out how to start the process off. Where to start? To help me keep track of things, I added each of the block element names to the readme.md document where I track my issues. As I needed to represent every Markdown block, I moved the question in the issue document regarding the adding of a Link Reference Definition token to the start of this new section and transformed it into a statement. As each block element in the Markdown document needed to be tagged with its position, it was a necessity that Link Reference Definition tokens be added, even if they did not result in any HTML output. Having all the blocks elements listed in that document, I then needed to somehow pick a good place to start. I was aware that wherever I decided to start, that token was going to have a lot more work associated with it as I tested out and implemented the foundation used to add line numbers and column numbers to the other tokens. Whatever process and code I used for that first block element; the plan was to repeat it in some closely related form for each of the other 10 block elements in the list. But which one to pick? In the end, I decided to start with Atx headings. Unfortunately, I did not keep any notes as to why I picked this block element to start. My best guess is that after having a lot of exposure to heading code while implementing the last group of rules, I figured headings would be a good place to start. Following that line of thought, I believe that I also would have guessed that SetExt headings were going to be the more difficult token to change, leaving Atx headings as the best place to start. Either that, or I decided to go alphabetic and ‘a' is the first letter of the alphabet. Either way, Atx headings were first, and it was going to be a fun ride. Adding in Position Markers Taking a good look at the Atx headings and the information that was being passed around, the first thing that caught my eye was a good opportunity to refactor. As I was letting parts of the parser grow organically, many functions included the argument line_to_parse along with an argument that was usually named start_index . This organic pairing made sense in a lot of places, allowing for the easy manipulation of the current index, and occasionally the line_to_parse . However, when adding in support for line numbers, I was going to have to pass in a third variable, meaning it was time to refactor this. As I have said previously : Write it once, write it neat. Write it twice, think about extracting it. Write it three times, extract it without a second thought. And that is where I started to work on replacing the line_to_parse variable and the start_index like variables with the position_marker variable and the PositionMarker class. This class was, and remains, very simple: class PositionMarker : \"\"\" Class to provide an encapsulation of the location within the Markdown document. \"\"\" def __init__ ( self , line_number , index_number , text_to_parse ): self . line_number = line_number self . index_number = index_number self . text_to_parse = text_to_parse self . index_indent = 0 The line_to_parse variable was replaced by the text_to_parse member variable, the index-type variable was replaced with the index_number member variable, with the line_number member variable being introduced. As I knew I would have to support \"indented\" text, such as text after a block quote character or a list start sequence, I added index_indent at the same time. And with that, the easy stuff was ended. It was time to move on to the tougher stuff. Creating the marker Wanting to start at the beginning, I made some modifications to the __parse_blocks_pass function in the TokenizedMarkdown class to track the line number. As I did not have to deal with Link Reference Definition tokens yet (spoilers!), this was a straight forward change, with the local line_number variable being initialized before the main loop and that same variable being incremented at the end of the loop. To encapsulate this information, I created a new position marker as follows: position_marker = PositionMarker ( line_number , 0 , token_to_use ) Instead of passing token_to_use to the parse_line_for_container_blocks function, I instead passed in the expression position_marker.text_to_parse . While I wanted to pass in position_marker by itself, I knew I had some important work to do before getting that would be possible. Stage 1 - Integrating the Marker Locally Using my knowledge of the project, I knew that the path between the __parse_blocks_pass function in the TokenizedMarkdown class and the parse_atx_headings function in the LeafBlockProcessor class was going to go through a number of functions in the ContainerBlockProcessor class. Instead of using a lot of guesswork to determine what that path was, I decided to start at the parse_atx_headings function itself and work my way backwards to the parse_line_for_container_blocks function. In the place of adapting the entire function in one pass, I decided to start with focusing on the passing of the position marker into the function. To facilitate this decision, in the parse_atx_headings function, I wrote some simple glue code to interface between the old style code and the new style code. Before the change, the code looked like this: def parse_atx_headings ( line_to_parse , start_index , extracted_whitespace , close_open_blocks_fn ): \"\"\" Handle the parsing of an atx heading. \"\"\" new_tokens = [] After the change, the code looked like this: def parse_atx_headings ( position_marker , extracted_whitespace , close_open_blocks_fn ): \"\"\" Handle the parsing of an atx heading. \"\"\" line_to_parse = position_marker . text_to_parse start_index = position_marker . index_number new_tokens = [] In the __parse_line_for_leaf_blocks function where the parse_atx_headings function was called from, I created a new PositionMarker instance named temp_marker , with the information from the local function. That new temp_marker variable was then passed into the parse_atx_headings function instead of the previously used line_to_parse and start_index arguments. I knew from the start that this approach was going to be more work. However, by isolating the change to the function's signature, I knew I could keep the parser stable and usable at any point. By keeping the parser in that good state, I knew I could depend on the large bank of tests used by the project to verify any change was a successful change with no unintended side effects. The process was indeed slower than some other methods, but it was a process that I had absolute confidence that I could rely on. Stage 2 - Integrating the Marker Up the Stack In this stage, the process that was applied to the parse_atx_headings function was applied again and again until the __parse_blocks_pass function was reached. With every step going back towards that function, I repeated the same actions that I performed in stage 1, but with the calling function: add glue code, change function arguments, create a temporary position marker, and pass that position marker into the newly changed function. The almost continuous execution of tests at this point was essential. As with any parser, even slight changes in the content or interpretations of the content can cause small ripples in the output that may escape detection if not quickly located and addressed. While I was doing my best to try and ensure that all changes were kept as isolated as possible, it was only the rapid battery of tests that kept my confidence high that I was going in the right direction with no ripples. Another thing that helped me immensely was using Git's staging ability. To facilitate clean commits, Git has a nice feature called a staging area , with the moving changes into this area being commonly referred to as staging. The nice part about Git staging is that, as this article describes , it is very easy to discard any changes made to your local files in favor of the state of that file with any committed changes and staged changes applied. As such, each time I applied this process to another function, I ensured that once the tests were all passing that I staged those changes in the repository. In the cases where I made mistakes, which did happen frequently, I had the knowledge that I was able to revert the relevant files back to their pre-change states, and resume work from there. When it was time to commit those changes, they were already isolated in the staging area, ready to be committed. Stage 3 - Cleaning up Going back down Once I reached the __parse_blocks_pass function, I was able to connect with the real position_marker variable, using that variable instead of creating a new temp_marker variable. I then started working back towards the parse_atx_headings function, replacing temp_marker with position_marker as I went. When I got to the parse_atx_headings function, I then proceeded to replace instances of line_to_parse within that function with position_marker.text_to_parse and instances of start_index with position_marker.index_number . As these changes were localized, they were easy to make, but I still ran frequent test passes just to make sure I was making the right changes. I do acknowledge that some of those test passes were executed out of paranoia, but in my mind, if running the tests an extra time allowed me to step forward with no reservations, it was worth it. Finally, at the end of all this process, I removed the line_to_parse variable and the start_index variable from the function, as they were no longer being used. There was no benefit to keeping those two variables around, so I just removed them. I now had a solid function to parse for Atx headings, referencing the line number and index number in a nice compact object. In addition, I was able to easily trace this compact object from the main processing line of the parser directly to the function itself. The only thing left to complete the change was to wire up the token. Stage 4 - Wiring up the Token Now that I had the position_marker object being passed properly into the parse_atx_headings function, I needed to pass that object into the token's constructor. The first change I made was to the base MarkdownToken object, changing its constructor to allow the passing in of a line_number argument and a column_number argument to be stored in the instance. To complement this change, I made some modifications to the __str__ method to add the line number and column number to the returned string, but only if at least one of those two numbers was not 0. The benefit of this slight change was that the line number/column number pair would only show up in the function's output if that information were provided. As that information was only provided once I added support for that type of token, I only had to worry about the token output changing for the tokens I had already changed. The second change was to modify the constructor for the AtxHeadingMarkdownToken class to accept a position_marker argument. That code was implemented as such: line_number = 0 column_number = 0 if position_marker : line_number = position_marker . line_number column_number = ( position_marker . index_number + position_marker . index_indent + 1 ) Basically, if the position_marker variable was not present, the default value of 0 was used for the line_number and column_number variables. If the position_marker variable was present, the line_number was be moved over from its position_marker object into the function's line_number variable. For the column_number variable was assigned the sum of the index_number member variable, the index_indent variable (at this point, always 0), and 1 . As index values are always 0 based and column numbers are always 1 based, the addition of 1 to the index number ensured it was based properly. Finally, these newly calculated values for the line_number and the column_number were passed into the newly retooled MarkdownToken object constructor. This was the point that I was preparing for, where everything came together. It was now time to see how close I got to the actual test results. If memory serves, I believe I actually closed my eyes after saving my changes and started the execution of the tests. Stage 5 - Addressing Test Results Prior to the very end of the previous step, the most frequent test command line that I used was: pipenv run pytest -m gfm This executed only the parser's scenario tests, reducing the test area to only those tests that were tagged as being part of the gfm group. As I was running the tests with each change, it was important to keep the scope, and therefore execution time, of the tests to a minimum. And up to this point, it worked well, and the tests were solid, passing every time. But as I made the connection between the AtxHeadingMarkdownToken token's constructor calling the MarkdownToken token's constructor, I knew that everything that I had just changed was just a good guess. I hoped that I had wired things up correctly, but at that point, I only knew 2 things for sure: due to the tests, I had not disrupted any of the parsed tokens for the test cases, including their order and content I was passing some transformation of the base position_marker into the token's constructor With everything wired up, I used the above command line multiple times, each time picking off one of the failed tests to validate. With each test, I first calculated what I thought the line/column pair should be, noting it down in the function's comments section. I then validated it against the test results, trying my hardest to not peek at the test results before I did my calculations. Once I had all those tests cleaned up, I did an extra pass through the changed tokens, manually recalculating the line/column pair and checking to make sure they were right. Only after all that was addressed did I change the module in the above command line from gfm to rules , cleaning up any test failures caused by line/column pairs that were now being displayed as part of the output. The Importance of Good Tests While I can talk at length about good practices with respect to tests, I hope I can convey the sincere importance that I associate with having good tests and good test coverage on a project. It is true that I can often \"eyeball\" a simple change to a project, figuring out whether that change is correct. But in my mind, anything beyond a simple change requires solid, repeatable testing. Without that testing in place, you are betting against the risk of something going wrong with your project. Testing is not about finding bugs as much as it is about risk management. If you have a small project that rolls various forms of dice for a Dungeons and Dragons campaign, the impact of that risk failing is very light. Any person I know that would use such a project would also have some backup dice for \"just in case\". For the PyMarkdown project, the end goal is to have a tool that website owners can trust to keep their website's Markdown documents in a proscribed format and style. The impact of too many failures is a loss of trust in the project, with a certain level of loss in trust being associated with those website owners dropping their use of the project. By having good testing of various forms embedded within the project, I can hopefully mitigate some amount of the loss of trust that any failure brings with it. Since I have worked so hard to get the project to this state, I did not want to take any unnecessary risk to the project's stability. The tests are a solid tool that I use frequently to keep any such risk to a minimum, and especially during refactoring, where I rely on them heavily. And as I rely on them heavily, I am also continuously looking for better ways to test the projects that those tests are in. Lather-Rinse-Repeat I honestly tried to find another heading for this section. Try as I might, no other section heading seemed to convey the repetition that I went through other than the phrase: Lather, Rinse, and Repeat . For each of the 10 remaining tokens, the 5 steps outlined above for Atx heading were repeated, with only a few changes to the process. Those changes are outlined in the following sections. Please note that the following sections are ordered with respect to the amount of work needed to resolve them, rather than my usual chronological ordering. When I encountered questions with respect to whether something with respect to that token was done properly or not, those questions were added as action items to the readme.md file to be examined later. I knew this was going to be a slog , and a hard slog at that. It just seemed more efficient to me to note them and move on, circling back to deal with them later. HTML Blocks, SetExt Heading Blocks and Code Blocks There were no changes introduced to the process for these elements. Thematic Breaks There was only one small change to the process for this element. That change was the delaying of the cleanup stage until a later time, as I wanted to get more breadth of tokens implemented to ensure I had the right foundation for the change. The other change that I made was to the MarkdownToken class, thereby affecting all the other tokens. For this change, I moved the code to calculate the line_number variable and the column_number variable from the AtxHeadingMarkdownToken class to the base MarkdownToken class. Once this code was proven, it just made more sense to keep it in the base class than repeating it for each token. Blank Line Token The change required to process the blank line token was not with the token itself, but with the processing of block quote blocks. The __handle_blank_line function was changed in the TokenizedMarkdown class to accommodate the position_marker argument, starting the change for all calls to this function requiring that argument. Other than that change, everything else was normal. Block Quote Blocks and List Blocks Strangely enough, I thought these two types of blocks would take the most amount of work to address, but due to the way they were implemented, only a couple of small changes were required. In both cases, the block start algorithms have to deal with with the possibility of a tab character (often denoted as \\t ) being used as the whitespace between the block start sequence and the rest of the block's data. Having already dealt with tab stops versus tab characters and fixing that issue once, I really did not want to fix it again. I just needed to ensure that the current fix and the previous fix were compatible with each other. To ensure that happened correctly, the modified line text and index numbers were passed to a newly created PositionMarker instance, created after the processing of the Block Quote block and the List blocks. This ensured that any line text modified by the processing of either container block would be retained, while adding the now normal processing using the PositionMarker class. Paragraph Blocks As the catch-all element for Markdown documents, I had a feeling that these blocks would end up near the top of the \"most effort\" list, although I will admit that I guessed the reason wrong. I thought that it would have something to do with lists and block quotes, while the actual reason is due to Link Reference Definitions. More precisely, the reason is due to failed or partially failed Link Reference Definitions. Due to its composition, it is impossible to properly determine if a Link Reference Definition is valid without reading the next line. As documented in the article Adding Link Reference Definitions , its multiline nature and use of various forms of quoted sections mean that unless the parser encounters the end of the closing section or a blank line, it does not know if it had reached the end of the Link Reference Definition. If it reaches the end and for any reason that Link Reference Definition is not valid, the logic in the parser starts adding lines back on to a list of lines that need to be reparsed without them being misinterpreted as a Link Reference Definition. It is precisely that requeuing logic that I needed to alter to work properly with the new PositionMarker class. While the index_number remained untouched, I had to make sure that the line_number variable was properly reset to account for the length of the lines_to_requeue variable when any requeuing occurred. I had a bit of a problem with the initial implementation of this code, so I wanted to take extra time and really boost my confidence that I was handling the change for this token properly. Link Reference Definitions This one should be very obvious… I needed to add the token itself! When designing the parser, I did not see any need for a token that literally has no impact on the output, so I did not add a token for it. While a link element that refers to a link reference definition will show up as a link in the HTML output, the link reference definition itself is not added to the HTML in any form. However, now that I was adding support for all markdown block elements, I found myself in the position of adding the token in to the parser. Adding the token itself was not too difficult. That was accomplished by the usual process of finding the right method, the __stop_lrd_continuation method in this case, creating a new instance of the new LinkReferenceDefinitionMarkdownToken class in that function, and properly populating it. Then the process of adding the support for the PositionMarker class kicked in, and quickly I had a case of a wired-up token with the proper data. And then I went to test it… and I noticed that the tests failed in the transformation of the tokens into HTML. Looking, I quickly determined that I needed to make some additional changes to the TransformToGfm module. This was a simple change, as by definition, Link Reference Definitions have no effect on the output. To accommodate the new token, a simple handler was registered for that token that simply returns the same string that was given to the handler. And then I went to test it… and the line number and column numbers were incorrect. In about half of the tests, the pair of numbers seemed to be widely different than I had manually calculated. Double checking my numbers, I then noticed a pattern. The numbers being reported were for the last line of the Link Reference Definition. Because of its multiline nature, the values associated with the position_marker variable were the ones used when that Link Reference Definition was considered both valid and complete. Avoiding the passing of a single use variable around, I added the starting position to the LinkDefinitionStackToken instance at the top of the stack, and things looked better. As I looked at the token and its __str__ function output, it looked very lean. Based on other tokens, I expected a lot more information to be stored in that token, to be analyzed later. Slowly going through the information gathered during the processing of the token, I ended up figuring out how to properly represent the token. Between the various whitespace sequences between parts of the definition, the various parts of the definition themselves, and both uninterpreted and interpreted parts, the data called for a class with 11 member variables. Slowly but surely, I started working through each of the Link Reference Definition scenario tests, starting with the easier ones and working my way to the more difficult ones. It took me a while to work through each scenario and validate it, but I breathed a sigh of relief when I finished going through all the tests. In the end, it had taken 2 days to complete, along with countless executions of the scenario tests. It was not until I looked at my notes that I remembered something: I had delayed some of the cleanup. Cleaning Up Before calling this round of changes done, there was a decent amount of delayed cleanup to be performed. As part of Stage 3 of the process, there were quite a few times where that cleanup was delayed for one reason or another. As I was about to move on to another issue, I wanted to make sure that cleanup was performed. That cleanup itself was easy, as I had repeated the \"change-test-check\" process so many times that I believe I could perform it while sleeping. Following a variation of that same process, the code was cleaned up in short order. What Was My Experience So Far? As I finished the cleanup, I was happy to have the work of adding in the line numbers and column numbers behind me. After 10 days of work, it was now a large task that I knew was not on my task list anymore. But while I was happy, I was also concerned about the 20 new issues that I had added to my issues list. I was aware that at least a few of those items would be checked out and determined to be false alarms, but that probably left somewhere between 16 to 18 issues to research and triage. On top of that, there was also the fact that each line number/column number pair for the tokens was determined manually by me. While I have faith that I can count accurately for small sets of numbers, I do know that I make mistakes. Some of those mistakes I had already caught while going over the test cases before committing the changes for each of the elements. But it was entirely possible, and indeed probable, that I had missed at least 1 or 2 mistakes. This was not due to lack of confidence, but due to a sense of reality. I know that the project has great coverage and great scenarios, but I also know that I do not have every scenario represented, just the really important ones. That got me thinking. If I found that many errors, was there a way to remove the need for me to manually count those values? Could I automate it? Would it be worth it? Would I find anything? The more I thought about those questions, the more I realized that I needed to explore this area. If nothing else, I wanted to make sure I had not missed anything. But after thinking about it for a while, I realized that I did not want to risk that I had missed anything important. What is Next? And here ladies and gentlemen is where I begin to go down the rabbit hole . At the current moment, the transform_to_gfm.py module is approximately 900 lines long. About 30% of that module is processing overhead, with about 18% dedicated to handling the somewhat complex issue of list looseness. The remaining 52% is consumed with the simple handle_* methods used to translate each token's start and end tokens into HTML. While the calculation of list looseness does add some complexity to the translation, the algorithm and implementation are both relatively simple. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/06/08/markdown-linter-adding-line-and-column-support/","loc":"https://jackdewinter.github.io/2020/06/08/markdown-linter-adding-line-and-column-support/"},{"title":"Markdown Linter - Taking Time To Evaluate","text":"Introduction I have been busy at work on the PyMarkdown project since December 2019, November 2019 if you include project pre-planning. During that time, I have established a very consistent schedule for planning the next weeks' worth of work. Normally I start planning the next article a couple of days before I write the rough outline for the current article, ensuring continuity between the two articles. This occurs concurrently with the last couple of days of development work for the week, so I usually have a very healthy picture of where the project is and where the project is going. This week was very different. For the first time since I started the project, I was unsure of the right direction to take. As I stated at the end of the previous article , I needed to answer one important question: should I write more rules or should I fix more issues? While I did arrive at a final answer, it would eventually take me almost 2 weeks before I was able to properly answer that question. In addition to spending some time to take a good look at the project, I decided to use that time to knock some of the low priority issues off the technical debt list, hoping to gain some useful insight on how to answer the question along the way. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 15 May 2020 and 08 May 2020 . How Did I Get Here? I believe that questions such as features versus issues are important questions for any project maintainers to ask of themselves and their project team. Even more important to me is the policy that any such questions and answers regarding a project must occur with full transparency to team members and other people interested in the project. I know I am not the first person to get stuck on an issue like this in their project, and I will not be the last. Because of that desire for transparency and openness, I wanted to document how I arrived at an answer, with explanations along the way to help others with their decisions regarding similar questions. Basically, I consider everyone reading this article to be an honorary team member. As I alluded to in the introduction, most of my articles start off as ideas that I get while I am working on the project. When I am trying to figure out the direction that I want to go with on the project, I consider both my ability to code and test any changes along with how I am going to document those changes in an article. By the time the end of the \"project week\" starts to roll around, I have usually started to transition from having an idea of where I want the project to go, into a plan of what to do for the project in the next week, and how I will write the article about the execution of the plan the week after. When I get to the last day of the week, three things happen one after the other. First, I perform a small project retrospective to figure out what parts of the last week went well, and where I can improve, being as honest with myself as I can. Secondly, I scan over my commits and notes from that week and come up with a good unifying theme for that week's outline and sketch out a very rough outline for the future article. If things went well, that theme matches the project direction from a week before. If not, I just adjust as I go. Finally, I use both of those pieces of information to compose a plan on what needs to be done in the next week with respect to the project. I have found this process very healthy as it asks three very important questions for the project. How did you do? What did you do? What do you plan to do? More specifically, by asking the questions in that order, my plan for the next week can be an informed plan, incorporating any \"where can I improve\" action items into the next weeks work, while they are still fresh from the retrospective. Where Did This Process Fall Apart? The current issue that I had with this process started when I asked myself the rules versus issues question because of one of those retrospectives. I remember sitting down at my desk, starting to type out a couple of notes to myself as part of my retrospective. There were two main themes that were a part of that retrospective. What went well was the writing of the rules and my belief that I had developed the right framework. What needed improving was that my confidence in the framework was at a high level, but was it was high enough for me to continue developing rules? I did not feel comfortable moving forward on the project without answering that question. But after a long period of time, I had not made any progress on answering it. I silently hoped that moving on to my next task would help. Despite my hope, skipping to creating a rough outline of that week's work did not help me to answer that question either. The prior few weeks had all been about proving that I had done enough work on the project that linting rules were both possible and easy to write. I sincerely believed that I have succeeded with that goal with room to spare. Mission achieved! But with that important question solidly answered in the positive, it did not help me move forward on answering the question on whether the project would benefit more from more rules or less issues. I then tried to force the issue by coming up with a plan for the next week, hoping it would jar something in my head that would \"magically\" resolve the issues I had with the retrospective and outline. It did not work. I just sat there and paused for a long time while staring at the screen. I tried bouncing between the retrospective, the outline, and the planning, but the results did not change. I was at a bit of an impasse. As far as I could figure, the scales seemed equally balanced. If the questions had to do with some manner of data-driven metric, it would have been easy for me to come up with a go-no-go threshold that I could rely on. This decision was not going to be data-driven, but emotion-driven. That would make the go-no-go decision a lot more difficult to pin down to an answer that I felt good with. After making a good-faith effort to arrive at an answer, I decided to choose neither answer. Instead, I decided to do two things. The first thing was to take a good look at the code over a few days, hoping to get extra insight as to where I felt the project was and how to better answer that question. The second thing was to fix some low-level issues that I had noticed and wanted to address. Nothing too earth-shattering, just some small cleanup issues that never seemed to get picked up in one of the other refactoring efforts. I hoped that this two-pronged approach would help me to pick up some other information that would tip the scales one way or the other. In my mind, while it was not much of a plan, it was moving forward. It was taking a step forward in the hope of getting more information. At the very least, the project would benefit by a few issues being removed from the technical debt list. That was level of success criteria for the week that was lower that I was used to, but I was fine with it. That is, if I figured out how to properly answer that question! Taking Time Is Not Taking It Easy While it is true that I have written every line of code in the project so far, I rarely have time to take a step back and see how things all fit together. To quote a professor of mine from university: Sometimes you cannot see the forest because the trees get in the way. As I was looking through the code, I was taking notes, mostly questions for myself. In all honesty, a couple of them were \"does this really work?\" which I resolved to answer for myself by the end of the week. Most of them though were simply me wondering out loud if there was a better way to do something or whether I could change something in a function to make it more readable or maintainable. In the end, as would befit my university professor, I think I was better able to see both the forest and the trees after those couple of days. But while I had hoped that it would nudge me one way or the other in answering my main question, I remained at an impasse. I was able to answer all the questions I had asked myself as I was looking through the code, but none of those answers signified that I had done something wrong. They only suggested small improvements on how I wrote the code. By no means were any of those answers better enough to tip the scales. Fixing Issues With one of my two paths not providing any information to sway the answer either way, it was time to switch to the second path: fixing issues. Better Logging Command Line Support One of the first things on my \"never got around to it\" 1 list was to make sure that PyMarkdown has proper command line logging support. This level of logging support was already available through the test framework but adding that same support to the command line support seemed to always get pushed off. It was time to change that. Adding that support was extremely simple, especially since I have added support like this to many command line programs. From experience, the two parts to adding this feature are adding the ability to control the default log level and adding the ability to redirect any logs that are produced to a file. The actual core code to implement this feature was just 7 lines long, with the interpretation of the command line arguments being the bulk of this change. The existing interpretation code, to handle command line argument parsing through the argparse library, was changed to include support for the --log-level and --log-file options, including a default setting of CRITICAL for the log level. To round out these changes, the log_level_type function was added to verify that any specified log level is a valid log level. Those 7 core lines to add the logging itself may change from language to language, but they almost always are simple modifications of a common pattern. The first part of that pattern is dealing with writing to the console. As many logging frameworks will do this by default, the customization here is to ensure that the desired logging level is applied to the console log handler. The second part of that pattern is to add support for logging to a log file, usually requiring 2 to 3 discrete actions to customize the file logging to the proper settings for the application. As these are the two most frequently used logging modes for command line programs, most languages include good solid templates on how to add this for that specific language. The fun part for me is always in making sure that a change like this is tested properly, and this was not an exception. As this is a new set of command line options, existing tests that listed existing command line options were updated. Additional tests were added to test_main.py to specifically test the new options, including tests specifically around specifying invalid options. I am not sure if it felt good to have this issue taken care of as much as I felt like I should have got to this one before that moment. Being such of a core fixture in other command line applications I have written, I just felt like this should have been addressed a lot sooner. Still, it was good to get it out of the way. Better Logging Support This change was a small-ish change, but it was one that I was overdue to explore. Back at the start of May, in my article on pre-rule improvements I noted that while I might be able to get away with a single static logger variable at the top of my modules, I had not seen any good documentation on the \"right\" way to do it. When I looked at various examples, such as this example at Python Spot, the examples seemed to always show logging within a single module application, and not within a multiple module application like PyMarkdown. As such, I decided to add localized logger variables until I could do some research and figure out the proper way to add logging to each module. It was not until I got the time to do more thorough research that I was able to find a good example of how to log with multiple modules. While the Python logging docs has a good section on \"Logging from multiple modules\", it was actually the section titled Advanced Logging Tutorial that gave me the information I was looking for. While not an example, the guidance that is given near the top of this section is quite clear: A good convention to use when naming loggers is to use a module-level logger, in each module which uses logging, named as follows: Python logger = logging.getLogger(__name__) This means that logger names track the package/module hierarchy, and it's intuitively obvious where events are logged just from the logger name. Yes! After searching for some clear guidance for weeks on this, I finally found something that was both authoritative and definitive. Even more, the last sentence contained a good, solid explanation of why this process should be followed. In the grand scheme of things, this change took very little time. Instead of having a logger instance declared as logger within various classes and static methods, a new instance LOGGER was created at the top of each file per the instructions quoted above. The change from logger to LOGGER was a simple search-and-replace tasks for each file, quickly accomplished. The hard part here was removing any logger arguments that were being passed into functions in favor of the LOGGER instance declared at the top of the file. Testing was also simple, as all I had to do was execute all of the tests again, and make sure I did not miss anything. It felt good to get this one off the issues list and committed to the repository. If I had to guess, I think this one never made it into any of my refactoring lists because things were working okay, with no loss functionality in PyMarkdown because of it. At the same time, there was this persistent nagging when looking at the issues list that I really need to figure out the \"right\" way to do this… and now I know what that is. But did this help me figure out the answer to my question? Nope. Taking some time to go back and look at my half-written notes, I was still at that same impasse. Nothing had changed. Hopefully, that would soon change. Adjusting the Parsing of Whitespace in SetExt Tokens Like the other issues that I addressed at this time, the effort to address this issue was small. Found during the development of rule MD023 , this was a rare case where I felt that code added to a rule could have been positioned better in the core framework. While the added code in the rule was only a small amount of code, it was a case where I knew a better way of handling this whitespace was possible, as it was already being done for the related Paragraph token. The two tokens that the SetExt heading token is related to are the Atx heading token and the Paragraph token. The SetExt heading token is related to the Atx heading token in that they are both heading tokens. The SetExt heading token is related to the Paragraph token as a SetExt token is created by changing an eligible Paragraph token with SetExt markings after it into a SetExt token. As such, when I wrote the code for rule MD023, I was surprised that the logic for detecting whitespace at the start of a Paragraph token was trivial but the similar logic for a SetExt token was more involved. Digging into why those tokens were handled differently, I quickly determined that it was only a couple of small changes that separated the handling of those two tokens. Addressing these differences required a few simple changes during the coalesce phase and the inline processing phase, both ensuring that the processing afforded to Paragraph tokens was also being applied to its kindred SetExt tokens. That was immediately followed up by adding a couple of tests to make sure this change stuck, and then by a change to rule MD023 to make use of a more trivial calculation of leading whitespace. Looking back at my notes while I am writing this article, I believe this was the start of me mentally tipping the scales towards spending time working on the issues. While this was not a big change, I believe that it represented a larger set of smaller things that I wanted to get right before moving on. I believe the change that was occurring was a subtle change in how I was weighing the various categories of issues. That weighing was starting to put more emphasis on fixing issues, specifically issues with the parser. The previous two issues that were addressed, both dealing with logging, did not seem to affect my decision at all. This issue, dealing with the parser, moved the weighing enough that I noticed it. While it was only barely noticeable at this point, that feeling was going to get stronger as I addressed the next issue. Properly Grouping Hard Line Break Whitespace One of the things that I noticed when fixing the previous issue was that where hard line breaks were concerned, they did not have any leading whitespace embedded within them. It was not a big issue. If I was not looking at the test cases for the previous issue, I would not have seen this issue. It was just silently waiting to be discovered. It may seem like a small thing, but I have a \"rule\" that any whitespace before any non-text token goes with the token that follows it. Just before I started writing rules, I noticed that many of the tokens were following this rule, so I decided to apply this pattern as a blanket rule over all the tokens. The benefit to this approach is that I have a consistent expectation of where the leading whitespace will be placed after it is extracted. That benefit allows me to write better, more consistent rules where I always know where to look for that whitespace. The fix for this issue was almost trivial. For the most part, it was adding another parameter to the HeadBreakMarkdownToken constructor, passing it either the line continuation character \\ or the leading whitespace that caused the hard line break to occur. A bit of cleaning up with the resultant variables, and it was done. But as with the previous issue, I could feel the weighing of my priorities changing. This was another small thing, so small that it went undetected until I chanced upon it. But the thing was, it started me thinking: If I was able to find this issue, what other issues were lurking in the parser, waiting to be discovered? Consistently Using the Word \"Heading\" While I was very aware that this was a non-code related task, I felt that it was a good time to get it out of the way. During my documentation of the first three rules , I decided to choose \"heading\" of \"header\" for the reasons outlined in that article. However, even though I had made that change in my articles 2 , I had not made the accompanying changes in the PyMarkdown source. This was a quick search-and-replace, followed by running of tests to make sure things were good. Experiencing no bumps and no typos, everything went fine. While purely cosmetic, it felt good to make sure that the blog and the source code were in sync with each other. Making sure they were in sync just felt good. Removing SetExtHeadingEndMarkdownToken I have long since got rid of the notes I used during the early days of writing the parser for PyMarkdown, so any attempt at figuring out why I added this class near the start of the project on 29 January 2020 is probably going to fail. My best guess, based on what I can see in the GitHub repository, is that perhaps I believed that having a specific end token for each start token type was the way to go. Regardless, since that time I have adopted an approach with a single EndMarkdownToken that refers to its starting MarkdownToken by name. This approach has proven to be quite practical as most of the operations that rely on EndMarkdownToken instances do not need any contextual information except for the starting tokens's name. As such, the practicality of having a specific EndMarkdownToken instance that matches each start Markdown token feels overpowered to me, with little benefits to show for the added complexity. Removing this token was easy. The class was removed from the markdown_token.py module and the leaf_block_processor.py module was changed to add an EndMarkdownToken instance for the related SetExt heading. The rest of the changes in the commit for this change were holdovers from the previous changes, where I had forgot to do a clean build and record the changes. This change was cosmetic, but like other issues detailed in this article, it changed the weighing of the issues even more. Once again, the change was not a dramatic one, but it was enough that at this point, it was noticeable. What Was My Choice? Having addressed a good handful of small issues that did not make the big lists for refactorings, the balance between the two scales had shifted enough that I knew that I had a good solid answer: fix more issues. I did not feel that I would be wrong in adding more rules, just that I wanted to focus on ensuring that a number of the smaller issues were given some focus to ensure they were resolved properly. It also did not feel like I had lost any confidence in writing rules, that was still at a healthy level. In the end, I believe it came down to a solid understanding that if I was going to write more rules with this framework, I wanted to make sure that any obvious issues were dealt with. The largest of those issues that needed to be addressed was adding the proper line number and column number support to the tokens. But it also meant working through the issues that I found during the first 12 rules and either verifying they are issues and addressing them or explaining why they were okay. What Was My Experience So Far? I strongly believe that the process of taking my time and working through those low priority issues gave me some valuable insight into the project that I had missed before. While the primary catalyst for being able to properly answer the question were the parser issues that I resolved, I do not discount the insights provided by looking at the source code at a higher level than usual. I believe that by allowing myself time to absorb the project code at a higher level, it opened some doors in my mind that allowed me to be better influenced by the issues I fixed. I cannot prove that of course, but as with all feelings, it just is. And while it was initially irritating that I could not answer the features versus issues question, I now believe it was inevitable. Especially since I am the only one working on this project, I do not have anyone to remind me to stop looking at the trees and focus on the forest. That change in perspective really helped me to get a clearer picture, and for that reminder, I am grateful. One thing that I did not expect was that the answering of this question taking almost 2 weeks. I started the planning for this block of work on 02 May and it was 16 May before I had the planning in place for the next block of work. The thing is, at no time during the process did I want to timebox this process. It took 2 weeks because that is what I needed to properly answer this question. And I was fine with that. For the large part, I am also okay with spending some time making sure I get the parser right before moving on to authoring more rules. Sure, it means I am extending the development cycle out by at least a couple of weeks, but I think that the time will be well spent. What is Next? Having answered the question of rules vs foundation, it was time to tackle one of the big issues that I had listed: line numbers and column numbers. I knew this was not going to be an easy change, but that just told me I should make sure I do it right the first time! Not one of these which was actually given to me by my family one time. ↩ I rarely go back and change anything in previous articles, except for bad grammar. Since I wrote that article, I have endeavored to be consistent in my use of \"heading\" over \"header\". ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/06/01/markdown-linter-taking-time-to-evaluate/","loc":"https://jackdewinter.github.io/2020/06/01/markdown-linter-taking-time-to-evaluate/"},{"title":"Markdown Linter - Rules - Headings - Part 2","text":"Introduction For any readers that have been waiting with bated breath since last week when I posted my article titled \"Markdown Linter - Rules - Headings - Part 1\", I will now break the suspense. This week's article is titled… drum roll please… \"Markdown Linter - Rules - Headings - Part 2\". Yeah, I know, the title is terribly unoriginal, but like the previous article, there is nothing special about this article except that it details the second group of heading rules that I am creating. With this group of rules completed, except for the two rules mentioned in the previous article , every rule in the initial heading rules group will be completed. Even though I know these rules will not be the last group of rules that I write, the writing of these rules serves an important purpose. The process of writing these rules will help to paint a reliable picture of the stability of the project at this point. This picture will then allow me to determine whether writing more rules or fixing some issues is the best course of action for the project. To be clear, I am not pro-rules nor am I pro-issues, I am pro-project. If the picture that emerges gives me a level of confidence with which to write more rules, so be it. If that level of confidence is not achieved, then I will address any issues that I feel are getting in the way of me being able to add more rules with confidence. Basically, the tests and the issues provide me data, and I will interpret that data to determine the proper next step. Simple. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 28 April 2020 and 02 May 2020 . Dealing with An Omission As I started to write this article, I went over the 15 different rules that I know contain headings: 3 in the \"First Three\" article , 5 in the last article , 2 rules to skip , and 4 rules in this article. Huh? I double checked my numbers, and as far as my math is concerned, 3 + 5 + 2 + 4 = 14. Where did the missing rule go? At the end of implementing the last rule group, I determined that the development of rule MD025 needed to be put on hold, for the same reasons as rule MD041 : YAML front matter. Rule MD025 exists to make sure that there is one and only one top-level heading present in each document. The description for rule MD041 allows that top-level heading to be specified in the metadata for the file, and the plan for rule MD025 is to follow that same pattern. As rule MD041 was put on hold until that metadata support is added, it only makes sense that rule MD025 is also put on hold pending the addition of that same metadata support. And Now, More Rules With this group of 4 rules, the total number of implemented heading rules will be 12. I believe that is a good sampling of rules with which to decide whether to fix more issues or create new rules. To that extent, let's go! Rule MD023 - Headings must start at the beginning of the line This section describes the initial implementation of PyMarkdown's Rule MD023 . Why Does This Rule Make Sense? For this rule, the first answer that popped into my mind was the usual answer: consistency. According to the GFM specification, there is no syntactic difference between: # Heading and # Heading As they are equivalent, just like with previous rules, it makes sense to use the simpler form of the heading to keep things consistent. However, on a more thorough examination, a better purpose for this rule is compatibility. While I recognize that the initial parser features and tests are based on the GFM specification , knowing where other parsers diverge from that specification allows me to start thinking about future \"tunings\" for the project. I do not know how possible it is, but I am wondering if I can tune PyMarkdown's output based on a profile set up for a given Markdown parser being used. While this is still in the \"far-future thinking\" stage, one of the pieces of data I need to understand for that endeavor is how far from my chosen base specification are the specifications for those other parsers. To get an idea of how compatible a normal Atx heading is across parsers, I submitted the text # Heading 1 to Babelmark 1 . Except for the presence of an id attribute in the h1 tag (and that attribute's value varying only slightly), the results were the same across all the parsers, indicating wide compatibility. When I added 2 spaces at the start of the text and submitted that text to Babelmark , only 12 out of the 31 available parsers interpreted the line as an Atx heading. With only 39% of the parsers at Babelmark recognizing the changed text as an Atx heading, it was far from being widely compatible. Based on those results, for the sake of compatibility, it made sense to create a rule to recommend the removal of any whitespace at the start of an Atx heading line. While a standard GFM Markdown parser will treat it properly, with less than half of the sampled parsers handling it properly, it just made sense to avoid that extra whitespace. If PyMarkdown tuning is a feature that I add in the future, I can always revisit this rule and make it aware of the added tuning. As that feature is currently only a maybe, a current, solid rule for headings means no spaces at the start of heading lines. Adding the Rule Adding the code to implement this rule was trivial. The rule simply looks at the extracted_whitespace field of the Atx heading token where the captured whitespace from before the start of the Atx heading is stored. If the value of extracted_whitespace is not empty, whitespace is present, and the rule fails. Using the process from the previous section to test Markdown samples with multiple parsers, I then tested variations of SetExt headings like: My heading ---------- where there was extra spacing at the start of the first line, the second line, or both lines. In each of those cases, the results showed that there was at least one parser that did not recognize it as a proper SetExt heading. If I want a linter that can be useful for a wide variety of parsers, the failure of one parser to recognize the above text as a SetExt heading is still somewhat of a failure. To deal with whitespace before SetExt headings, only a small amount of extra parsing was required above what was added for the Atx headings. When the SetExt token is observed, that token is stored in the setext_start_token variable, with the storing of the presence of any leading whitespace in the any_leading_whitespace_detected variable. As Text tokens are observed, if the any_leading_whitespace_detected variable has not already been set, a simple decomposition of the text is performed to look for leading whitespace on each line, the any_leading_whitespace_detected variable being set to True if any whitespace is found. Finally, when the SetExt's end token is observed, one final check is made against the end token for whitespace. When that is complete, if any whitespace was detected in any of these three phases, the rule fails with the value stored in the setext_start_token variable being used as the failure token. The code for implementing this rule, the tests, and test data were all trivial. Once I understood the constraints that I needed for the rule, it was easy to translate those constraints into test data and source code. Perhaps I am a bit jaded in my viewpoint, but after implementing 8 other rules for headings, adding this rule was just easy. The real interesting part about implementing this rule was using Babelmark and looking at the output from the various parsers. Seeing the different HTML variations that arose from the different interpretations of a single Markdown element made it clearer to me why the GFM specification was written. If there is not a single, clear picture of how parsers translate Markdown into HTML, the authors start to get confused, tailoring their use of Markdown to a specific parser and its output. Seeing that Babelmark output just really drove that point home for me. Rule MD024 - Multiple headings with the same content This section describes the initial implementation of PyMarkdown's Rule MD024 . Why Does This Rule Make Sense? If the parser is following the GFM specification exactly or is a bare bones parser, this rule does not make any sense. When presented with Markdown such as: ## Next Heading some text ## Next Heading some text the parser will generate HTML that is close to the following: < h2 > Next Heading </ h2 > < p > some text </ p > < h2 > Next Heading </ h2 > < p > some text </ p > However, either through its own configuration or through a plugin, most Markdown parsers allow for a mode in which some transformation of the heading text is added to the <h2> tag, usually in an id attribute. Some parsers go further than that, providing an anchor point allowing the reader to navigate directly to the heading by using a full URL with the contents of the href attribute after the normal part of the URL. Once such example is this HTML, which was rendered by one of the parsers for the first heading in the previous sample: < h2 id = \"h2-next-heading\" > < a id = \"user-content-next-heading\" class = \"anchor\" href = \"#next-heading\" > Next Heading </ a > </ h2 > This kind of generated HTML output is popular for two main reasons. First, various combinations of the heading tag's id attribute, the anchor tag's class, and the anchor tag's id attribute allow for stylesheets to be applied to the generated page in a clean and unambiguous manner. Secondly, assuming that the normal URL to the page is https://www.website.com/that-page.html , since the href attribute for the anchor tag is specified as #next-heading , the reader can go directly to that section on the page by using the URL https://www.website.com/that-page.html#next-heading . This concept is so useful that multiple \"Table of Contents\" plugins that I looked at required another plugin to already be enabled that provides this information that is the repurposed by the Table of Contents plugin. Specifically related to this rule, the part that I want to focus on is the generation of the id attributes and the href attribute. If a naïve generator implementation is used, the heading text Next Heading will always be reduced to some form of the text next-heading , without any regard for duplicates. When a more advanced implementation is used, the plugin remembers that it has already generated the text next-heading for the current document. To avoid any duplication, when the parser goes to generate another instance of a heading id based on the text Next Heading , it generates it as normal but also appends a suffix to the generated text, usually something like -1 to keep the generated ids unique. Because Markdown parsers are not guaranteed to perform the proper, advanced generator implementation, this rule plays it safe by failing when it detects heading text that is duplicated. However, to provide further configuration for more compliant parsers, this rule can also be set to only fail the rule in cases where sibling heading elements have the same name. While it is not stated, my guess is that some parsers include extra information in their generated ids that only causes duplication issues when sibling headings have the same text. Adding the Rule Using the default configuration, the evaluation for this rule is very simple. When an Atx heading token or a SetExt heading token is observed, the rule starts collecting the text until it encounters the appropriate end token. At that point, one of two things happen. If this is the first time that text has been seen, it is saved in a dictionary that is cleared at the start of each document. If it is not the first time that the text has been seen, the rule fails. Simple and clear cut. If this rule's configuration is changed to only search for duplicates in siblings, a small amount of a change is required in the algorithm, but not much. The first change is that each of the 6 levels of headings must have its own dictionary. Instead of looking up the collected text in a single dictionary, the text is looked for in the dictionary assigned to its heading level. Basically, if a heading is a level 2 heading, then it needs to verify against the level 2 heading dictionary, and so on. As the heading level that determines which dictionary to use is an integer, it made sense to create an array that contains 6 dictionaries, one for each heading level and initialized to an empty dictionary for each new document. With those changes in place, a bit of working through multiple scenarios provided the rest of the answers. For the increasing heading level case, consider the general case of: ## Level 2 ### Level 3 While it is appropriate to only increase the heading level by 1, hence the creation of rule MD001 , the general case for this rule applies to any increase in heading levels. When going to the new heading level, the level's dictionary must be cleared to ensure that any previous headings do not pollute the rule's sense of duplication. If multiple levels are involved, it makes logical sense to clear all the dictionaries up to and including the new heading level. The other case to consider is where the heading levels decrease, such as with: ### Level 3 ## Level 2 Once again, the general case for this applies to any decrease in heading levels. As the new Level 2 heading is separated from any previous level 2 headings by the ### Level 3 heading, those headings are no longer considered siblings. As such, when moving to the lower levels, the dictionaries must be cleared down to and including the new heading level. Other than figuring out the logic for this rule, and taking a while to do it, the rest of the coding for this rule went smoothly. While the new algorithm based on that logic took longer to figure out than I hoped it would, it was useful to be able to have the test data and test cases on hand from VSCode's output. Taking the information from the Problems window of that editor saved a lot of time when coded into my tests for this rule. I did have a couple of false starts but knowing that the algorithm passed the tests using the VSCode derived test data really increased my confidence that I got the algorithm right. Rule MD026 - Trailing punctuation in heading This section describes the initial implementation of PyMarkdown's Rule MD026 . Why Does This Rule Make Sense? This rule seems to be largely derived from Ciro Santilli's Markdown Style Guide . In this document, Ciro claims that headings are not complete sentences, and as such, should not end with punctuation. Specifically, his examples show very good cases for not ending a heading with the : character or the . character. My best guess is that David Anson, when creating his rule based on that document, wanted to allow his linter's users to be able to extend this concept to include any headings that end with a set of common punctuation characters. As such, the default for David's rule is to fail the rule if any heading ends with any of the .,;:!?。，；：！？ characters, and that set of characters can be easily changed with configuration. While I do not completely agree with this rule, I do agree with the premise behind the rule: headings should never be considered complete sentences. To properly describe that statement and how it applies to various headings, I needed to brush up on my English grammar rules. Even though it has been a few years since I was in high school, I was quickly able to find a good solid definition: a complete sentence starts with a capital letter, ends with a punctuation character, and expresses a complete thought. Using that definition as a rubric, I feel it is useful to demonstrate how that rule applies to the headings in my own documents. For this comparison, I used the layout of the current section of this article as a good example of how to conceptually apply this rule. For each rule in this section, the level 2 heading is always the id of the rule followed by the description. In the case of this rule, that heading is Rule MD026 - Trailing punctuation in heading . That heading is not a valid sentence because it is not a complete thought and does not end with punctuation. The level 3 heading following this section is Adding the Rule . There is a bit more nuance involved with that heading, as I can easily argue that it can be interpreted as a complete sentence, just a very weak one. The saving grace for that heading is that it does not end with punctuation, so it is not advertising itself as a complete sentence. That leaves the heading for this section which is Why Does This Rule Make Sense? . When I first looked at my grammar references for this, it seemed to fail at every point. It starts with a capital letter, ends with punctuation, and looks like a complete sentence. Yes, I said \"looks\"… and it took me a bit to get there as well. Remember above when I said that a complete sentence expresses a complete thought? That is where context comes in. Without the context imposed by this document, the obvious question to ask is \"What rule?\". Because I tied the context of the heading to document's structure, it is not a complete thought unless it remains in the document. As it is not a complete thought on its own, it narrowly fails the complete sentence test. Aside: So, Why Keep It Like That? I know that using partial questions as headings is not a popular choice, and I know it narrowly fails the complete sentence test. So why keep it? For me, it all boils down to using my own voice in my writing, and the authenticity of my writing in that voice. For each of these rules, if I were reading someone else's document, I would ask two questions: Why is it good to do this? What did it take to do this? Wording the second question as a statement is relatively easy. Instead of \"What did it take to do this?\" I used the heading \"Adding the Rule\". It is not glamourous, but it concisely and accurately conveys the image that I want to convey for that section. For the first question, I struggled for a long while trying to rephrase that question as anything other than a question, and it just did not seem correct. It either was missing something, or it just felt like something that I would not say unless I was prompted to. So instead of settling for something that I was not confident about, I opted for using a question that did capture the essence that I wanted in a heading for that section. And yes, I purposefully worded the heading of this section to emphasize that point. That, and it is the kind of question I would ask myself if I was reading someone else's article. Adding the Rule At this point in the development of heading rules, I believe the term \"ditto\" is appropriate. Like a fair number of the rules before this one, any text that is seen between either the Atx heading token or the SetExt heading token is collected for later examination. As that process of collecting the heading text has been well documented in the other rules, I will avoid documenting it from here on out, assuming that avid readers already know it fairly well, instead focusing on the unique bits and differences in the algorithms. Once the heading text is collected and the heading's end token is encountered, the rule's comparison logic is then activated. Quite simply, when the end token is encountered, the last text character of the collected text is checked against the configured set of punctuation characters. If there is a match, the rule fails. If configuration is provided to change the set of punctuation characters to check against, the check is simply performed against that list of characters instead of the default list. At first, I was a bit let down that I saw this as being a simple rule, as the both the code and testing for this rule were very trivial. While I was initially disappointed, after a while I was able to see it as a good thing. One benefit that I started to see is that if these rules are performing a consistent action, it reduces the chance that I am going to get the logic wrong. Perhaps more obvious to me is the benefit that if the logic is indeed very similar, it may be encapsulated into a base class or helper class in a future refactoring, thereby reducing the maintenance costs. When I realized what those benefits were, I became okay with this rule being a mundane rule to write. Mundane equals refactorable equals lower maintenance. Rule MD036 - Emphasis used instead of a heading This section describes the initial implementation of PyMarkdown's Rule MD036 . Why Does This Rule Make Sense? This rule is another rule that was largely derived from Ciro Santilli's Markdown Style Guide . If I had to guess, it seems that Ciro had seen cases where people were using emphasis as headings in Markdown documents, something that is confusing. Doing a bit of easy research, most Markdown tutorial sites, such as Markdown Tutorial , address headings in their first 3 lessons. For people who may ignore tutorials and go straight to cheat sheets, both Markdown Guide's Cheat Sheet and Adam Pritchard's Cheatsheet deal with headings on their first page where they are immediately visible. From this research alone, it is hard to figure out why someone would use emphasis over headings. However, I started wondering if perhaps there were historical reasons for this rule? If you start looking at the tutorials and cheat sheets from a different angle, perhaps the historical angle to justify this rule does make sense. Perhaps these distinct sources put headings near the start of their documents because people were not using headings properly at the time that those documents were written. Thinking as an author, by putting headings near the start of the tutorial or cheat sheet, I would expect that placement to strongly hint that they are important and should be looked at before going with the rest of the document. To explore that concept further, I assumed that I did not know about headings, forcing me to theorize on how to make a heading-like element would work. Trying to forget about this rule, I started working on a list of the elements that I could use. With headings out of the picture, and every one of the other blocks having a very distinct purpose, that left only inline elements. Line breaks are not immediately visible, and links are really meant for navigation, so they were removed from the possibilities list early on. That left backslashes, character references, code spans, and emphasis. Out of those 4, only the emphasis made sense to use to draw attention to some text. Based on those restrictions, the best heading-like element that I came up with was this example: * My Not So Heading * More text What a surprise! That is exactly what this rule is looking for. In particular: the paragraph is a single line, surrounded by emphasis the contents of the paragraph are only simple text, no inline Markdown except for the surrounding emphasis for the same reasons as rule MD026 , the text does not end with a punctuation character Basically, if the only action I do is to remove the emphasis around the text, instead prefixing the line with the text # , it should become a valid heading. As I now had a working theory on why this rule may have been created and what a good justification was for it, it was time to start writing the rule. Adding the Rule While the logic for the rules has been somewhat simple before the point, even a simple glance at this rule led me to believe that the logic for this rule was going to need some heft to it. As it often is with many complex parsing scenarios, it was time to use a Finite State Machine. While there are many good articles on Finite State Machines, such as this decently complete one at Wikipedia , these complex machines boil down to this simple statement: the machine has a finitely small number of states and for each state there are 0 or more transitions from that state to other states. From my experience, there is a certain level in parsing or pattern recognition where a simple comparison is not enough, and a formal Finite State Machine is required. The inverse of this is also true, where the formality and setup required for a proper Finite State Machine may get in the way of a small, efficient algorithm. Consider the algorithm and code for rule MD026 in the previous section. A simple if statement was used to look for 3 types of tokens: a start heading token, any contained text tokens, and an end heading token. While not called out as such, this algorithm was a small state machine with clear states and clear transitions between states. Once the contents of the heading were collected, then rule MD026 did an analysis on it, and determined whether to fail the rule based on that analysis. Breaking that logic out into a full-fledged Finite State Machine had little benefit, as the simple logic was able to complete the task without less than 5 states and without any complicated transition logic. On the other hand, the logic for this rule as defined in the last section reveals that this rule's algorithm requires 5 distinct states: look for a paragraph start token look for an emphasis start token look for a text token look for an emphasis end token look for a paragraph end token If at any point the next token that we encounter is not the type of token required to go to the next state, the machine resets back to state 1 and the machine starts to look for a new paragraph. While the transition logic remained simple, I felt that the \"large\" number of different states made using a Finite State Machine the best alternative. Once I had the states and transitions figured out, the writing of the rule was trivial. The states were represented by a simple set of if statements, each one clearly looking for the correct condition to move the state machine to the next state. If that condition is not found, the current state is reset to state 1, causing it to look for the initial condition again. If the Finite State Machine gets past the final state, the rule fails and the current state is again reset back to state 1, looking for another candidate paragraph to evaluate. What Was My Experience So Far? From the viewpoint of adding new rules, this was a good experience. My feelings, as expressed in the last article , were easily extended to cover this set of rules as well. While I still want to jump ahead sometimes, my confidence in the process of writing rules and the framework in which I write those rules is getting deeper with the authoring of each rule. A good part of that confidence boost is that when I create new rules, the bulk of the time used to author those rules is focused on the rules themselves, and not on how to wring the required data from the framework. It is only the rare exception where I need to figure out how to get some data from the framework. But it is those exceptions that caused me to pause and think when trying to answer the question that I posed at the start of the article: write more rules or fix more issues? This answer is a difficult one for me to arrive at, as there is good data to support both sides. Writing more rules would round out the framework some more, while focusing on the framework will fix known issues with the framework, allowing future rules to be written with less friction. This choice was also complicated by the arrival of new ideas for the project as I am implementing the rules. A good example of this is the Markdown parser \"tunings\" that I briefly talked about back in the documentation for rule MD023 . While it is nice to think about these concepts and how they could make the project better, doing anything more than thinking about them at this stage would be distracting. Even worse, it could derail the project by having me follow that concept down the rabbit hole . If I want to be able to explore concepts such as \"tunings\", which of the two options would allow me to get there faster while maintaining my require level of quality? In the end, I determined that I wanted some more time to think about this. There are a few issues in my backlog, and fixing those issues will give me some more time to determine the best course of action. Sometimes, the best decision is not to decide, but to collect more information. I firmly believe that this is one of those cases. And if I am wrong? It just means the foundation for the project will be that much cleaner and stronger than it was before. What is Next? As I mentioned in the previous section, I feel that the best course of action is to do some refactoring. While it is not a glorious task, it will give me some time to determine if whether to choose line/column support in the parser or adding more rules. Babelmark is a useful online tool that converts a given piece of Markdown text into HTML using a wide variety of Markdown parsers. I find tools like this very useful in exploring possibilities for how to solve issues that I have. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/05/25/markdown-linter-rules-headings-part-2/","loc":"https://jackdewinter.github.io/2020/05/25/markdown-linter-rules-headings-part-2/"},{"title":"Markdown Linter - Rules - Headings - Part 1","text":"Introduction After setting up a good, easy process for writing new rules , it was now time to make some headway on the task of writing those new rules. While the title \"Headings - Part 1\" is not glamorous, it is a very apt description for the way that I planned out the development of this group of rules dealing with Markdown headings. In David Anson's MarkDownLint , there are a total of 15 linter rules that specifically deal with headings. The first 3 of those rules were used as examples in the previous article, leaving 12 rules to implement. For reasons I will follow up on, I decided to leave the implementation of rules MD041 and MD043 for later, reducing the number of rules to a nice manageable 10. Without any unnecessary embellishment, the article's title simply denotes that I am going to talk about the implementation of the first half of those rules. Sure, I could come up with a name like \"The 5 Most Important Rules for Headings!\" or \"5 Rules for Headings That You Cannot Live Without!\", but the truth is that they were simply the next group of rules. Nothing fancy, nothing misleading, just the plain truth. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 24 April 2020 and 26 April 2020 . What About Those Two Rules That Were Left Out? Just to get this out of the way: because it was expedient. I started to look at rule MD041 when I was implementing rule MD002, as documented in the last article . At that time, I looked at rule MD041 to determine if I could just include the work for MD041 with the work to write rule MD002. While the only difference between the two rules is the inclusion of \"YAML front matter\" into the rule, I felt that the difference was a large one at that stage of the project. I knew that I needed to added metadata support to the parser as a requirement for my use of the linter. But the overriding question I had to answer was: Was this the right time to add this new feature to the parser? After a decent amount of back and forth on the subject, I decided against adding it at that moment. From my point of view, at that time it was better to focus on additional linter rules for the project rather than focus on additional features for the parser. While it would be fun to add more functionality to the parser to allow for MD041 to be implemented, I made a judgement call that it could wait a bit. For rule MD043, my decision was based on a combination of the complexity of the feature and the usefulness of the feature to me. Without a deep dive into the rule, it looks like there will be a fair number of edge cases that I will have to consider for rule MD043, easily increasing its complexity. From the usefulness point of view, while I can see how it may be useful, this is not a feature that I see myself using a lot. While I took a different path to the judgement call, the result was the same: Rule MD043 could wait for later. Why Is This Group of Rules Important? While I didn't think about it at the time, I needed this group of rules to prove two important things to myself: that I had chosen the right base framework to write linter rules with, and that I had chosen a good development process with which to write and test those rules. It was only after the development was completed and I was writing this article that I was able to recognize their importance in proving or disproving those two things to myself. Rule MD018 - No space after hash on atx style heading This section describes the initial implementation of PyMarkdown's Rule MD018 . Why Does This Rule Make Sense? This rule is all about catching a typo . This rule surmises that when someone types: # Heading 1 they probably forgot to add a space and really meant to type this: # Heading 1 While it is possible that the author intended to start a new line with the text #Heading , I believe that it is far more likely that a typo occurred. Adding the Rule The two important parts of adding this rule were recognizing that it can take place in a normal paragraph and that it can occur at the start of any line in that paragraph. While I did consider the possibility of text like #Heading being captured as part of another block element, my experimentation with the CommonMark parser did not reveal any case that causes another block element to stop capturing on a line like # Heading . As such, by default the text ends up being captured within a paragraph block using a text token. As is shown in example 47 , an Atx heading can interrupt other blocks, but more importantly, it can interrupt a paragraph block. With these two constraints understood, I was ready to start coding the rule. Leveraging the first constraint allowed me to add code to the next_token function to consider any text token within a paragraph block eligible for further examination. From there, I applied the second constraint by enhancing the rule to look for an Atx heading-like pattern at the start of each line within those eligible text tokens. This was accomplished by breaking down each text token into separate lines and checking them against a Python regular expression . By carefully reading the GFM specification's section on Atx headings , I was able to construct the regular expression &#94;\\s{0,3}#{1,6}\\S , which breaks down into the following components: &#94; - start matching at the start of the line \\s{0,3} - look for between 0 and 3 whitespace characters #{1,6} - look for between 1 and 6 # characters \\S - look for one non-whitespace character I needed the regular expression to be a form of the regular expression to recognize the proper GRM specification form but modified to look for the specific case that I wanted to look for. In this instance, I was looking for heading-like text that is missing at least one space after the starting # characters. To satisfy this constraint I used the \\S sequence, matching any single non-whitespace character, at the end of the regular expression specifically looking for no whitespace characters at the end of the expression. Using my favorite online Python regular expression tester , I was able to verify that I had the correct regular expression right away. The rigorous testing for this rule involved more testing of cases where the rule should not fail than cases where it should fail. But when I boiled all those tests down into what was relevant, there were a couple of patterns that I just needed to test. In testing, this is referred to as equivalence class testing. While sites like this one go on and on about what it is, it breaks down into one statement. Unless there is something special (usually around boundary conditions), entire groups of tests can be considered equivalent and be represented by a single test, if their relevant behavior is consistent across that entire group. Consider a simple calculator application that allows for the addition of 2 integers together. The testing of that application does not need to test every single integer added to every single integer. Just thinking about the work required for that is exhausting! Instead, the testing can be broken down into representative groups such as \"any integer added to 0 equals that integer\", drastically reducing the number of tests required to cover a specific component. Applying this concept to the test cases for this rule, I was able to reduce over 20 tests down to a more reasonable 8 tests. Some of these groups were making sure the rule does not fail if eligible text is in any of the text-containing leaf blocks except for the paragraph block, in which it should fail. The rest of the groups were simple cases based on variations of the regular expression above, that regular expression a reflection of the specification. Confident that I had covered all the conditions for this rule, it was time to move on to the next one! Rule MD019 - Multiple spaces after hash on atx style heading This section describes the initial implementation of PyMarkdown's Rule MD019 . Why Does This Rule Make Sense? Like a fair number of the other rules that are defined, this rule is about consistency. While the opening paragraph for the GFM specification for Atx headings does allow for more than one space after the initial set of # characters, it also specifies that those leading spaces are stripped before the rest of the line is parsed as inline content. Essentially: # Heading 1 and # Heading 1 are syntactically equivalent. As they are equivalent, it makes sense to use the simpler form of the heading to keep things consistent. Adding the Rule The code to evaluate this rule was very simple. When the content of the Atx heading is parsed into a text token, any spaces that are stripped from that token are kept as part of the text token in a separate part specifically reserved for stripped whitespace. To check for multiple spaces, I simply added code that checked to see if a text token existed within an Atx heading, and if so checked to see if that token contained more than one space in that stripped whitespace area of the token. As the code was simple, the tests themselves were simple. The examples from the MD019 rule description page were used verbatim as the only cases needed as that is how simple this rule is. At least it was that simple until I looked more closely at the next two rules. Rules MD020 and MD021 This section describes the initial implementation of PyMarkdown's Rule MD020 and Rule MD021 . These Rules Look Very Familiar While preparing for the work detailed in the last article , I quickly read through the list of rules before starting that work. I believe that because I was focusing on those first 3 rules, I missed an interesting piece of information on rules MD020 and MD021. The piece of information is that rules MD020 and MD021 are variations of the rules for MD018 and MD019 that apply specifically to Atx headings with the Atx_Closed style . That information was both good news and bad news. The good news part was that since I had the code and tests already written for the previous two rules, I could repurpose a lot of that code for these new rules. The bad news part was that I was going to have to rewrite parts of those two previous rules to exclude cases where the latter two rules will fail. However, I believe that the good news here definitely outweighs the bad news, so on to the coding! Adjusting the Old Rules The first part of adding these new rules was modifying the old rules to not fail if an Atx_Closed style of Atx heading is found. Rule Md019 was the easiest one to adjust, requiring only a small change. As part of the normal Atx heading parsing, the number of trailing # characters in the heading is collected in the Atx heading's remove_trailing_count variable. The small change I made was to only fail rule MD019 if that variable was zero, an easy change in anyone's viewpoint. The change for rule MD018 was only slightly more difficult, adding an additional check for the regular expression #\\s*$ 1 to the existing condition. In both cases, simple test cases were added to expressly test the rule to make sure they did not fail if presented with an Atx_Closed style heading. Simple changes, and simple tests to verify those changes. Adding the New Rules Adjusting rule MD019 to fit the parameters of rule MD021 was interesting, but not too difficult. The original check for spaces was modified to set the variable is_left_in_error instead of failing immediately. When the end token for the Atx heading token is seen, it checks to see if that is_left_in_error was set or if the closing # count is greater than 1, failing if either part is true. Checking the end token was required as the parser's pattern is that any leading spaces are applied to the token that follows those spaces, meaning that those spaces before the closing # characters is stored with the Atx heading's end token. Making similar changes to rule MD020 proved to be a bit more difficult. The constraints from rule MD018 were still present, but rule MD020 added the constraint of looking for bad spacing before the closing set of # characters. Starting with the previous regular expression for finding normal Atx headings in paragraphs ( &#94;\\s{0,3}#{1,6}\\S ) and merging it together with the the regular expression for excluding Atx_Closed line failures from rule MD019 ( #\\s*$ ), I ended up with the expression &#94;\\s{0,3}#{1,6}.*#+\\s*$ . To merge the two expression together, I made two small changes. The \\S expression used to indicate the matching of any character that is not whitespace was replaced with the .* expression to match zero or more instances of any character. The # character used to indicate a single # character was then replaced with the #+ expression to match 1 or more # characters, allowing for any number of closing # characters in the Atx heading. Before adding any extra test cases, I used my favorite Python regular expression tester to do some extensive testing on the regular expression. I am happy to report that except for a simple typo, I got the regular expression right on the first try! With the regular expression verification concluding successfully, I moved to add the test cases for both rules. For both rules, I started by adding copies of the test cases from the original rules but modified each one to represent missing spaces at the start of the Atx_Closed style headings instead of normal Atx style headings. From there, I added test cases where spaces were missing from both the start and the end of the Atx heading as well as where spaces were only missing from the end of the Atx_Closed style headings. It was there that I ran into an interesting case. Is This Really A Failure? While the original rules do not specifically identify this as a distinct case, consider the following Markdown: # Heading # While this obviously qualifies as an Atx heading, I can credibly argue that it should be included in MD020's definition of a typo in an Atx_Closed styled heading. While I could have left it out, I felt strongly that this was most likely a typo and the rule should fail on this input as well. Adding this into the rule for MD020 was easy. If the last token is a text token and the rule is currently looking at an end Atx heading token, the rule checks to see if that previous text token ends with a # character. If the # character is found, it fails the rule. After adding the code for this newly discovered case, I spent some time exploring different possible combinations, making sure that I had the right equivalence classes for all 4 new rules. When I was satisfied that I had not missed anything, I move on to the next rule. Rule MD022 - Headings should be surrounded by blank lines This section describes the initial implementation of PyMarkdown's Rule MD022 . Why Does This Rule Make Sense? While I could stay with the consistency answer I used for MD019, a better answer for this rule is compatibility and readability. Speaking to compatibility, there are some Markdown parsers that will not recognize Atx headings unless they are preceded by a blank line or followed by a blank line. To keep things simple, having at least one blank line on either side of the Atx heading keeps the options for parser choices open. From a readability point of view, hopefully it is obvious that: Lorem ipsum dolor sit amet , consectetur adipiscing . ## Next Section Morbi dictum tortor a diam volutpat , ut . is a lot easier to read than: Lorem ipsum dolor sit amet , consectetur adipiscing . ## Next Section Morbi dictum tortor a diam volutpat , ut . Between the coloring of the Atx heading in my editor and the spacing around the Atx heading, it is easy to find the different sections. Without that spacing, I know it takes me a lot more effort to find headings, especially in a large document. In addition to these reasons, I believe that different people have varying requirements on what is acceptable or proper amounts of spacing before and after a heading. I personally know of a few people with visual impairments that find it easier to acknowledge the change in sections if accompanied by extra spacing. To support varying requirements like these, rule MD022 has the lines_above and lines_below configuration values that allows the user to alter the required number of blank lines before and after each heading element. From my point of view, the benefit provided by allowing the user to configure this rule easily outweighs the work that was required to add it. Adding the Rule In documenting the previous 4 rules, I noticed that I almost always talk about testing as a final step, which is far from my practice. As such, I thought it would be useful to use this rule's development as an example of my normal development process. Testing Done Right As I documented in my last post , my usual process for developing a new rule is: Creating a New Rule Creating the Initial Tests Implementing the Rule Thorough Rule Validation I usually talk about running the tests after I talk about creating a rule, because it is in the \"Thorough Rule Validation\" phase that most of the interesting things happen. In this case, it was the \"Creating the Initial Tests\" phase that was interesting. Creating a new rule is a task that I can now perform while sleeping, as I have done it enough times that adding a new rule to the project only requires a little bit of my attention to accomplish correctly. I was then wakened out of slumber when I proceeded to the next step and started going through the initial combinations of test data for this rule. Honestly, keeping all that data in my head caused me to get confused somewhat quickly. There were simply too many combinations to keep in my head with any degree of confidence. As such, I started creating the initial test cases for rule MD022, adding one test case per file just like normal. Working through each set of combinations, I was surprised that at the end of that exercise, I had 22 test files ready to go for testing. Reviewing the variations of text block types, SetExt/Atx headings, valid/invalid spacing, and alternate spacing configurations, it did seem justified that 22 test files were required to properly test the rule. And that number was kept low as I cheated a little bit, leaving any combinations with container blocks (block quotes and list blocks) out of the equation until I could do a bit more research and thinking on them. Writing the Rule Having so many test cases, I was concerned that the logic was going to be incredibly complex, but I was able to boil it down to two simple metrics: the number of blank lines before a heading and the number of blank lines after a heading. The count of lines before any heading is equal to the number of blank lines after a leaf block completes and the heading block starts. The count of lines after any heading is equal to the number of blank lines after the end of the heading until the next leaf block starts. This is where excluding the container blocks paid off, as adding those blocks in to this rule with their containing conditions and ending conditions would have complicated those calculations by a fair amount. Given the calculations as specified in the last paragraph, the code to determine these values was simple, but required some thinking. The blank_line_count variable is incremented if it is not None and is greater than zero. At the start of the document, the blank_line_count variable is set to -1 to make sure it does not fail for a heading at the very start of the document. Whenever the end of a leaf block is encountered, the blank_line_count variable is set to 0 to allow the counting of blank lines to commence. The end token for anything other than leaf blocks is not important to this rule, so in that case, the blank_line_count is set to None to stop counting until the next qualifying end token starts it again. Once I had the calculation of blank_line_count worked out properly, the rest was a matter of perspective. The before count is simply the value of blank_line_count at the time that the Atx heading is encountered. Likewise, the after count is the value of blank_line_count at the time that the next leaf block is encountered. But what if there is no next leaf block? What if the heading block is the last block in the document? That is where the completed_file function comes in handy. Added to the plugin manager for cases like this, this function is called after all the line and token processing for the Markdown has completed. When the completed_file function is called for this rule, it performs the usual check for closing conditions, failing the rule if the conditions are right. With the 22 test cases identified at the start of the development for this rule, the testing was simple, with almost no errors in the implementation of the rule itself, just the normal semantic and typo stuff. I do feel that I must acknowledge that I believe part of the reason this went off so cleanly was because the container blocks are not yet implemented. I do think those 2 blocks alone will easily throw a monkey wrench into the works. But for now, it is a good place to stop with development of this rule. What Was My Experience So Far? After the stress of adding the first three rules, as documented in the previous article , adding these rules to PyMarkdown was comparatively easy. While I still had to contend with the line numbers and column numbers not being reported 2 , everything else in my \"rule development process\" was working fine. There are times that I want to skip the tests and get to the development of the rule and have fun, as I am only human. But if there is anything that my experience as a developer has taught me, it is that having a clear picture of what you want to do before your write the code is essential to shaping the code. If you start with a poor picture for what your code will do, you get poor results. I find it almost always pays off to take the time to draw that good picture, getting together solid test cases and manually calculating their anticipated results once executed against the project. While the ease of rule development is nice, it is the solid nature and usefulness of the core rule engine that is making me happy. In implementing this batch of 5 rules, I did not have to make any changes to the core rule engine. At the same time, the development of the rules was easy, my focus centering on the particulars of the rule itself, and not the infrastructure required to support the rule. While I am not sure if it is happiness or pride, it is a good feeling that the work to get the project to this point is paying off. And while I do know refactoring is needed in some areas, I also have a growing collection of proof that my approach to this project is solid and should be continued. Based on those observations, I believe that any earlier questions about whether I had chosen the right framework and the right development process were answered positively. As an added benefit, I also sincerely believe that my early choices to do things \"this way\" are paying off for the project. While not 100% frictionless, the minimal effort that I require to write new rules is challenging but not difficult, increasing the \"fun quotient\" for the project. I honestly couldn't wait to keep the progress going with the next set of rules. What is Next? Without much fanfare, since this first group of 5 heading rules was accomplished with ease, next on the list if the second group of 5 rules. I do hope that I will have a similar result with those rules! Look for any # character followed by any number of whitespace characters, anchored to the end of the line. ↩ Yes, I am still kicking myself over that, just not as much. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/05/18/markdown-linter-rules-headings-part-1/","loc":"https://jackdewinter.github.io/2020/05/18/markdown-linter-rules-headings-part-1/"},{"title":"Markdown Linter - Rules - The First Three","text":"Introduction With some solid refactoring work completed, it was time for me to write the first set of rules based on all that hard work. To start things off, I planned to work on 2-3 rules that would verify that I had created an acceptable framework for writing and evaluating Markdown linting rules. As David Anson's MarkDownLint plugin for VSCode already has a number of rules defined, it made sense to pick a handful of rules from there as a starting point. This choice was also fitting, as David's NPM-based project was the inspiration for my Python-based project. But how to pick the rules to start with? One of my criteria was that whatever set of rules that I picked, I wanted to be able to extend the set of rules naturally once I finished proving the first few rules were able to be clearly written. The other criteria that I wanted was for those initial rules to be a good cross-section of what to expect in the other rules. Based on those criteria and some quick thinking on my part, it took me less than 2 minutes to realized that the first three rules in David's check list would do just fine! Why Is This Article So Long? To be honest, this article got away from me. Without breaking down this article into 3 articles, each one focusing on its own rule, I couldn't see any intermediate solutions for breaking up the article. These are the first three rules I wrote for the project, and as such, they laid the foundation for all the other rules. It just did not seem right to artificially break up the article. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 17 April 2020 and 23 April 2020 , except for the commit from 18 April 2020 . Why the First Three Rules? Given that David Anson's work was the inspiration for this project, it made sense to start with those rules, outlined here . The first three of those rules easily meet the first criteria, as all three rules deal with examining the headings in a Markdown document. Doing a quick count of all the rules that dealt with headings, I counted 15 such rules. Definitely qualifies as extensible. As for the second criteria, the first rule is a standard rule, the second rule is a disabled rule, and the third rule has configuration that affects how the rule is measured. Between the three of them, I had a lot of confidence that together they would represent a good cross-section of all rules, and therefore satisfy the second criteria nicely. With the three rules selected and a confirmation that these three rules satisfied my criteria, it was time to move forward with implementation. A Quick Aside Are they headings or headers ? While some of my cheat sheet resources like the Markdown Guide's Cheat Sheet refer to them as headings , other resources like Adam Pritchard's Cheatsheet refer to them as headers . Even the CommonMark specification refers to them as headings , but then include a couple of times where the term header is used instead of heading . So which one is right? 1 To keep things simple, I am going to use the term heading in this article and my other articles that deal with headings going forward. That term seems to be the one that is most dominant in the specification, and I believe that the authors of the specification had a good reason for specifically using the term heading . Even if that reason is not documented. Rule MD001 - Incrementing Heading Levels This section describes the initial implementation of PyMarkdown's Rule MD001 . Feel free to examine the code at your convenience. Why Does This Rule Make Sense? This rule is simple: when using headings, the heading level should at most increase by one. Based largely on the W3C's Accessibility Guidelines , this rule just makes sense even without those accessibility guidelines. If you have a heading, and you want subsections under that heading, you use a heading level that is one below the current one. Consider this example: # What I Did On My Summer Vacation I did a lot of things . ## July July is when it started . ### The Pool I went to the pool every day . #### Swim Classes Mom made me do this , again ! ## August This is when everything seemed repetitive . ### The Pool I got really sick of the pool . While it does accurately detail most of my summer vacations as a kid, I believe that it is also a decent example of a report with the various highlights of each section organized under their own headings. It makes sense to begin the document with the level 1 heading describing what the document is all about. From there, it logically follows that to break the document up, there should be level 2 headings with the month names, each heading containing text specific to that month. As going to the local pool was the major part of each summer, it therefore follows that each of the months has its own level 3 heading under which I talk about what I did at the pool during that month. Finally, swim classes were deemed mandatory by my mother, so me and my siblings took at least one session of swim classes each summer. Detailing those classes in the month they happened, under a level 4 heading, just seems to be the right thing to do. As a matter of fact, this example reminds me a lot of the \"returning to school\" report that my mother made us right just before school started, just to ensure that our waterlogged brains still remembered how to write properly. Thanks Mom! Putting my reminiscing about summers as a kid aside, take another look at the headings and the text, observing the natural progression for heading levels, as detailed in the last paragraph. For me, the headings and their levels just feel right, their flow is natural and not jarring. When I need to get more specific with information in each section, I used a new heading one level down from the current heading and added that more specific information under the new heading. From my point of view, it just worked, and I really did not need to think about why it worked… it just did. Specifically looking at the heading levels themselves, while there is a case where the heading levels decrease by more than 1, there are no cases where the heading level does not increase by 1. As an author, it just made sense to author the report like that, adding more detail with a lower heading and a lower subsection, and then popping back up to the right level to continue. While there may have been a scenario in which the Swim Classes section was followed by a level 3 heading and more text, it was not required. In fact, I believe that my freedom to not follow up that section with a \"garbage\" level 3 heading and section text are what makes the flow of the headings work as they do. While I might have taken the long way around in describing my theory behind this rule, to me it simply just makes sense both as an author and as a reader. Adding the Rule This was my first rule using the built-in parser, so I wanted to make sure to lay down some good patterns for myself to repeat going forward. Pattern: Test Format The first pattern I wanted to set in stone is the pattern to specify how to execute tests for a given rule. After experimenting with different formats and combinations, it was the proven test format that I chose back in November that won out. def test_md0047_good_description (): \"\"\" Test to make sure... \"\"\" # Arrange scanner = MarkdownScanner () suppplied_arguments = [ \"test/resources/rules/md047/some_test_file.md\" ] expected_return_code = 0 expected_output = \"\" expected_error = \"\" # Act execute_results = scanner . invoke_main ( arguments = suppplied_arguments ) # Assert execute_results . assert_results ( expected_output , expected_error , expected_return_code ) This format keeps things simple: a descriptive function name, a decent function description, and simple boiler-plate code for the function that can be applied to most tests. Even in cases where I had to add some extra code, such as adding a configuration file for the linter to read in and use, those changes were always applied on top of this template code, not instead of it. And except for those additions, only the variables supplied_arguments , expected_return_code , expected_output , and expected_error were ever changed for any of the tests, even to this day. Basically, my plan was to create the template once, put it through a trial of fire to test it, and then reuse it endlessly once proven. Keep it effective by keeping it simple. As Isaac Newton said: Truth is ever to be found in the simplicity, and not in the multiplicity and confusion of things. Pattern: Creating a New Rule Translating the logic described under the section Why Does This Rule Make Sense? into Python code was easy. First, I created the tests cases described above, and examined the debug information output for those cases, specifically looking at what the final sequence of tokens was. As all the information for the rules was contained within the instances of the AtxHeaderMarkdownToken or the SetextHeaderMarkdownToken 2 , those were the only two tokens I had to worry about. With that knowledge in hand, it was time to move to the second pattern that I wanted to repeat: creating a new rule. While the content of each rule changes, this process is always consistent. First, I pick a test to clone from and copy its contents into a new file. In this initial case, I copied the file rule_md_047.py into the file rule_md_001.py . Any initialization for each rule is performed in the starting_new_file function, so that function is cleaned out except for the comment. Finally, the next_token is cleaned out in the same way, to provide for a clean slate to start writing the rule with. For this rule, rule_md_047.py used the next_line function, so that needed to be changed to override the next_token function instead, as this rule is specifically token based. Except for this one special case, this pattern has now been replicated for each rule in the project. Pattern: Creating the Initial Tests The third pattern that I wanted to get in place was specifying a good set of initial test cases for the rule, prior to writing the rule itself. A strong proponent of test-driven development, I believe that before writing the source code, at least a rough outline of the test code and test data should be written. A common misconception of test-driven development is that before you write any code, you write all the tests. The process is an iterative process, one that grows over time. While this entire process is the pattern that I want to enshrine in this project, the important part that I want to tackle at this point is coming up with a good set of tests and test data to start with. For this project, this initial set of tests and test data are made easy by the existing MarkdownLint rules outlined here . While the rules outlined there do not always have a comprehensive set of Markdown documents to test that rule, they always document at least one good Markdown document and one bad Markdown document. If there was something obvious that is missing, I also try and add it at this point, just to save iterations later. But just in case I miss something, I have another pattern, Thorough Rule Validation , that I will talk about later to try and catch those missed cases. Implementing the Rule Once things were setup, it was time to add the logic. A false failure is not desired when a new document is started, so I added a reset of the last_header_count class variable in the starting_new_file function. In the next_token function, I then added some simple code to test whether the token was one of the two heading tokens, and if so, set the hash_count variable to a non-None value. At this point, I added some debug to the function and ran each of the test cases against the rule, checking to see whether what I expected to happen, did happen. As this rule implements simple logic, the initial logic was validated on my first try. Removing the debug statements, I added some logic to filter out any cases where last_header_count was not set (initial case) or where header_count was not greater than last_header_count (not increasing). With those cases filtered out, it was simple to check for an increase of 1 and to fail if the increase was more than 1. A quick call to report_next_token_error to report the failure, and the basic case was completed. From there, I circled back to the test data, and looked to see if there were any obvious cases that I was missing. It was then that I noticed that the text's description specified headings, but had no test data for SetExt headings, just Atx headings. I quickly crafted some data that mixed a SetExt heading followed by a valid and invalid Atx heading and iterated through the process again. It was only after I was sure that I had not missed anything obvious that I proceeded to the next pattern: Thorough Rule Validation. Pattern: Thorough Rule Validation The final pattern that I wanted to put into place was to be thorough in my rule validation, using both internal data sources and external data sources. The validation against the internal data sources was easy, as I had just finished the source code and the tests code for that rule. However, I put that aside and instead ignored the source code in favor of the the definition of the rule's scenario along with the test data. Based on those two factors alone, I predicted what the outcome of the test should be, then executed the test to verify that prediction. If I encountered an error, the first step I took was to recheck the expected output more rigorously against the rule's scenario and the test data. Whenever doing this, I rechecked the output multiple times just to make sure I had the right answer. From my experience, unless I try very hard, it is unlikely that I will make the same mistake twice. If there was an error in the output, I corrected the error and executed the tests again, as if from the beginning of this section. If there was an error in the rule itself, I would add some debug to the rule and run further tests to check what refinements I needed to make, before restarting all the tests in this section. For this initial rule, I had errors in the test data and the rule, and this attention to detail helped me spot them quickly. After a few iterations, I was confident that the validation against internal data sources was completed, and I needed to move on to an external data source. As the MarkDownLint implementation of the rules was done as a VSCode plugin, it made sense to use VSCode + MarkDownLint as the external validation source. It was with great confidence that I I loaded up the sample files into VSCode to check against MarkDownLint. It was when I looked at VSCode's Problems tab that I got a big surprise. I had forgotten something important: line numbers and column numbers. Line Numbers and Column Numbers To say this hit me like a ton of bricks would not do it justice. I was floored. I was so focused on getting the parsing of the tokens done accurately, I completely forgot to design and implement a way to place the location of the element in the Markdown document into their tokens. It was hard to miss the issue: MD001/heading-increment/header-increment: Heading levels should only increment by one level at a time [Expected: h3; Actual: h4] markdownlint(MD001) [4,1] Honestly, I took me a bit to get over this. It was so obvious to me that I should have seen this ahead of time. When you are reporting any type of linting failure, you need to specify the location of that failure so that the user can fix it. Without that information, the rule is somewhat useless. In the present as I am writing this article, I can better understand what happened and how I missed those number. However, at the time I struggled to find an interim solution until I could start to tackle this properly. I needed to focus on the first cohort of rules, so I tried to put this mishap out of my mind. It was after some a couple of frustrating hours that I added two fields to track the line number and column number of the tokens, setting both to 0. After some additional \"yelling\" at my monitor, I decided to close out that first rule, and moved one to the second rule, confident that I (mostly) had set up some solid patterns to focus on for the future rules. Rule MD002 - (Deprecated) First Heading Should Be Top Level This section describes the initial implementation of PyMarkdown's Rule MD002 . Why Does This Rule (Not) Make Sense? Note the slightly different wording of the heading. As documented on the MarkdownLint site , this rule has been replaced by rule MD041 . The important difference between the two rules is that MD002 looks at Atx headings and SetExt headings, while rule MD041 also looks at the metadata at the start of the document, often referred to as YAML front matter. Because there is an improved rule, this rule is disabled by default in favor of that rule. Whether in rule MD002 or rule MD047, the reasoning for both rules is consistent: each document should contain a clean title. For both rules, this is achieved by looking at the first Atx heading or SetExt heading in the document and verifying that the first heading is a level 1 heading. For rule MD047, the only difference from rule MD002 is that it additionally looks for a specific metadata field that can take the place of an explicit level 1 heading. A good example of rule MD002 is the standard readme.md file that I usually add at the base of a GitHub project. Typically, I start with a file that looks like this: # ReadMe This file describes what the project does , who to contact , etc . While the title is simplistic, it does present a clear indication of what the title and purpose of the document is. If, on the other hand, you see a Markdown document like: Lorem ipsum dolor sit amet , consectetur adipiscing elit . Phasellus felis lacus , finibus eget gravida eget , dapibus vel lacus . Phasellus placerat nisi enim , eu maximus ipsum congue nec . Integer sollicitudin metus urna , quis iaculis ligula condimentum eu . it is hard to figure out what this document is for. Simply by adding a title heading, this can be cleared up. # Random Test Paragraphs Lorem ipsum dolor sit amet , consectetur adipiscing elit . Phasellus felis lacus , finibus eget gravida eget , dapibus vel lacus . Phasellus placerat nisi enim , eu maximus ipsum congue nec . Integer sollicitudin metus urna , quis iaculis ligula condimentum eu . Adding the Rule Having previously explained my process for adding new rules , I'll leave that content out from here on, and just concentrate on what has changed. The logic for this rule is almost exactly the same as for the previously implemented rule MD001 , except that instead of checking for an increase in the heading level, it simply looks to see if the first heading it encounters has a heading level of 1. The implementation of this rule introduced two new concepts: rule configuration and disabling a rule by default. While the next rule, MD003 , will properly deal with the configuration aspect, the main focus of this rule is the ability to add a rule that is disabled by default. To provide options to the user, adding a rule that is disabled has merit. In this case, a new rule was added that is more comprehensive than this rule. However, rather than removing this rule and possibly breaking the configuration of some users, the original rule was preserved for users that are not comfortable updating their linting to use the more comprehensive rule. In addition, there is also a good argument to be made for new rules to be added in a disabled state, allowing people who are upgrading to a new version of the project to control which new features they want. As both examples illustrate, having a rule be disabled by default is a useful feature to have. The code to allow for a rule to be disabled by default was added back in November when I was testing out the plugin manager concept. While there was a good test of the disable feature at that time, it was now time to really test its functionality with this rule. This testing was achieved by looking back at the test data for MD001 and noticing that the first heading in the file improper_setext_header_incrementing.md is a level 2 heading. That means that if rule MD002 is enabled and executed for that Markdown file, I would expect a failure to occur. Therefore, when I executed the MD001 tests again, with rule MD002 in its default disabled state, I expected that no additional failures would be reported. I executed that tests, and this behavior is exactly what I saw in the results. For me, that was enough proof that the rule was disabled by default. To properly test this disabled rule, only a slight change to my normal process of testing rules was required. In addition to the normal information supplied by the test in the supplied_arguments variable of the test, the start of that array was modified to include the elements -e and MD002 . As PyMarkdown allows for rules to be enabled and disabled on the command line, those two additions simply told the command line to enable ( -e ) rule MD002 ( MD002 ). With those changes made to the tests, the rest of the testing went by without incident. Rule MD003 - Heading Style This section describes the initial implementation of PyMarkdown's Rule MD003 . Why Does This Rule Make Sense? In a single word: consistency. I am sure I would not want to read a Markdown document that had multiple heading styles, such as: # ATX style H1 ## Closed ATX style H2 ## Setext style H1 =============== It would be a confusing mess! If I were reviewing that document for someone, I would tell them to keep it simple, pick a heading style, and stick to it. By picking a single, simple style, it would help any readers know what to expect. In terms of styles, there are 6 styles to choose from. The obvious style, the default consistent style, simply looks at the first header and assumes that the style of that heading will be used for the entire document, atx , atx_closed or setext . If the user wants to be more specific about the style, there are two variations on Atx headings styles, and three variations on SetExt headings styles to choose from. They take a bit of getting used to, so let me walk through them. Atx Headings vs Atx_Closed Headings For Atx headings, the two variations that are available are atx and atx_closed , demonstrated in the follow example: ## Atx ## Atx Closed ## The only difference between these two variations are that atx_closed style includes # characters at the end of the heading as well as at the beginning. While the GFM specification has strict requirements that there can only be 1 to 6 # characters at the start of the Atx heading, the only requirement for the closing # characters is that there is at least one valid # character. For any stricter requirements on the closing # characters, a new rule would have to be written to add more stringent requirements for any Markdown documents with the atx_closed style. SetExt Headings vs SetExt_With_* Headings For documents that use the SetExt headings, the obvious issue is what to do if the document requires a level 3 heading, as SetExt headings only support a level 1 and a level 2 heading. To handle this case, the setext_with_atx style is used to specify that level 1 and level 2 headings remain SetExt, while level 2 to 6 headings are to use Atx headings, as in the following example: Setext style H1 =============== Setext style H2 --------------- ### ATX style H3 Without specifying a heading style of setext_with_atx and relying on an implicitly or defaulted setting of setext , the rule would fail on the heading labeled ATX style H3 . To round things out, there is also a style variation setext_with_atx_closed which has the same behavior as the above example, except using the atx_closed style instead of the atx style. Adding the Rule Back in the description for rule MD002 , I mentioned that I would cover the configuration aspect later, focusing at that time on the disabling of rules by default. Having completed the discussion about that rule, it is now time to talk configuration. For any readers following along with the commits in the project's repository, note that the work for rule configuration was performed in changes introduced in the commits for both rules MD002 and MD003. Configuration Configuration was the last component of the rules that I needed to have implemented and tested thoroughly before implementing more rules. It was important to me to get configuration right, as just over half of the initial rules contains some element of configuration. The main part of accepting configuration was to change the main.py command line interface to accept a configuration file in the JSON format, verifying that it was a proper JSON file by parsing it into a simple Python dict object before continuing. Just before the files were scanned, a call was introduced to the new __load_configuration_and_apply_to_plugins function, that function performing the required work to call the initialize_from_config function in each plugin. At that point, if the plugin requires any configuration, it calls the plugin's get_configuration_value function to see if a value of the requested type is present in the map. That might seem like a lot of work, but that work done can be summarized as: load the configuration, let the plugins know about it, and then let the plugin retrieve the configuration if required. Almost everything else surrounding those actions were either making sure errors were handled properly or making sure that the correct information was passed properly to the plugin manager. Before checking the configuration code in for MD002, and then again for MD003, I changed various parts of the plugins to request different information from the configuration store. It was this extra testing that allowed me to simplify rule MD003. The initial code for the rule raised an exception if the configuration value did not match one of the required values. Based on that testing, I changed it to use the default value in that case instead. It just seemed like the right thing to do in that case. The Rule and The Tests Once the configuration was in place, the rest of the development went smoothly. A slight change to the Atx heading token was required to report the number of trailing # characters, but other than that, the core rules engine was stabilizing with no other changes. The rule itself was somewhat simple but reducing the complexity of the check was a daunting task. At first, I wrote the rule with everything in the next_token function, which worked decently well. The first part of that function was a block of code that figured out two important attributes: the type of heading ( atx , atx_closed , or setext ) if the token was a heading token and whether that token contained a level 1 or level 2 heading token. Based on that information, the rest of the code worked out cleanly. If it was not a heading token, exit quickly. If the style was consistent and this was the first heading token to be seen, set the style to the heading type of the current token. With all that out of the way, the actual checking of the styles started. If the style was one of the 3 basic styles, a simple comparison determined if the rule failed or not. In the *with* variations for SetExt, the logic was a little more complicated, mostly dealing with checking the level of the heading. A certain amount of playing around with the code was required to get all the rules validating the Markdown in a clean and simple manner. The tests themselves were simple as well. Before starting on the rule, I had created one test input file with a positive example for each of the style types. By changing the configured style type to apply to the rule, I was able to cover all combinations very quickly. What were negative cases for some tests became positive tests for other cases, and vice versa. The reusability of the data for testing this rule ended up being a big win. Instead of 3-5 test documents for each style, the tests only use a total of 5 documents, not including the empty.md file. Pretty efficient! It was with that battery of tests in place that I worked to reduce the complexity of the rule. I won't try and say I got everything right on the first try. I didn't. But having those tests in place helped me figure out where I went wrong and helped me determine the next changes to make. Having a good set of tests is pivotal in being able to refactor any algorithm, and that includes one the project's rules. Resolving Conflicts Between Rule Test Data The one thing that I had to start watching out with this rule was the test data for one rule raising an error on a previously written rule. In each of the tests that I wrote, I specifically wanted to narrow down the testing to that specific rule, to keep the test more relevant to the rule. With rules MD001 and MD002 being in the same area, it was only luck that they did not cause any interference with each other. For rule MD003 , it caused interference with the test data for the previous 2 rules, where a consistent style for the input data was not a priority. To remove the interference, the PyMarkdown's disable rule feature was used, the opposite to the enable rule used in the testing of rule MD002. Instead of adding the -e and MD002 values to the supplied_arguments variable, the values --disable-rules and MD003 were added. In the tests for rule MD002, rule MD002 was enabled from the command line at the same time that rule MD003 was disabled from the command line. By applying --disable-rules and MD003 to the tests for MD001 and MD002, I was able to resolve the interference from rule MD003 cleanly, getting nice consistent test results. What Was My Experience So Far? The first part of my experience that I want to talk about is change and how I handle it. Specifically, I want to talk about the line and column numbers. While it was painfully obvious after comparing the output in VS Code with my output, it really hadn't crossed my mind before then. I was more concerned with the ability to write a solid rule, and not concerned with the content that would be displayed when that rule detected a violation. Sure, I felt like I should have caught that in the design process, and I gave myself somewhere between 5 minutes and 5 hours to deal with that. After that, I noted it in my \"to do\" document as technical debut, and I just put it behind me. That was not an easy thing to do, and my failure to account for that in my design haunted me for a bit. In the end, what helped me get over it was looking at what I did accomplish. I know it sounds cliché, but in this case, it helped me get a better perspective on the issue. What I forgot to do was add support for line numbers and column numbers. What I did not forget was to build a strong parser that has over 800 test cases that it is passing without fail. That parser also has a simple translation layer that can be plugged in to the parser to generate GFM specification compliant HTML code. Accomplishing those two feats was not easy. On top of accomplishing those two feats was another, more obvious one. I started in 2019 November with an idea of writing a Markdown linter in Python. With the three initial rules that I had just created, I proved to myself that I had made the right choice in selecting to build a parser for Markdown. The successful rules just proved that. The third feat was writing a parser-based linter for Markdown in Python. Yeah, I still feel a bit like a fool for missing something that was obvious, but with those things in my head, it became easier to let it go. Instead of focusing on the 1% that was that design flaw, I made sure to refocus myself on the 99% of the project that was not a design flaw… and moved forward. What is Next? Having completed the first set of rules, I decided that it was more important for me to keep my momentum in creating new rules than to add line numbers and column numbers to the parser. Truth be told, I thought that getting some distance from that problem would help me so it more clearly. With both reasons in place, I started work on the next group of heading based rules. To avoid the same issue with \"cheat sheet\" versus \"cheatsheet\", dictionary.com says the correct answer is \"cheat sheet\". ↩ It was not until the writing of this article that I formally decided to go with heading over header. There is now an item in my backlog to make this change throughout the source code. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/05/11/markdown-linter-rules-the-first-three/","loc":"https://jackdewinter.github.io/2020/05/11/markdown-linter-rules-the-first-three/"},{"title":"Markdown Linter - Core - Pre-Rule Improvements","text":"Introduction Way back in 2019 November, I started this project with a bare-bones framework using a simple dynamic plugin loader. It was a simple proof of concept to determine whether I could create the basis for an extensible linter framework in Python. Once I verified that I could write that framework, I implemented a very simple case to test against: checking to make sure the provided Markdown text ends with an empty line. While that rule was easy to implement, it was when I looked for another rule to implement that I determined that to properly lint Markdown text, I needed a Markdown tokenizing parser. From my viewpoint, unless I had a parser that emitted tokens that were a good representation of the Markdown to lint, the linting rules that I wanted to write would require too much guess work for my own liking. If I wanted to write good, solid rules, I needed to have the right information available for those rules to act upon. I needed a Markdown parser that emits Markdown tokens that I had confidence would be the required, correct information for the rules. Having now written such a parser against the Github Flavored Markdown specification , it was time to move on to the next part of the project: writing rules. However, since almost 5 months had passed since the project started, there were a few changes that were required in the linter's core before I could continue. What Is the Audience for This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 12 April 2020 and 16 April 2020 , and the commit from 18 April 2020 . Source Providers In my experience, following the threefold rule for refactoring is usually a good idea, as its wisdom has borne out true in my projects many times. While not a literal copy of the threefold rule, I choose to remember the rule as follows: Write it once, write it neat. Write it twice, think about extracting it. Write it three times, extract it without a second thought. The original text of \"three strikes and you refactor\" always seemed a little too harsh for me with its negative connotation. In addition, I feel that it does not provide good enough guidance on what to do in the first two cases, just the third one. My version of the rule still adheres to the spirit of the original rule, while fixing the specific issues that I perceive with it. Source providers for the parser are a concept that fits that refactoring pattern very well. When the original framework for the parser was written, it was designed to parse a line at a time to conserve memory. In the parser tests, this information is provided to the TokenizedMarkdown class as a single string, with the internal functions of the TokenizedMarkdown class breaking down that data into individual lines for further processing. With the exception of a single feature's error case 1 , this design has proven to be very useful in reducing the complexity of the parser. It made sense to me to refactor this section of code when considering how to add support for the second source of Markdown data: Markdown files. Starting with the InMemorySourceProvider Now that the project was moving into the rule-development phase, it was necessary to ensure that it was just as easy to feed the parser information from a string as it was to feed it information from a file. As the initial development kept things neat, it was relatively simple to take the logic for grabbing the next line and encapsulate it within the InMemorySourceProvider class as follows: class InMemorySourceProvider : \"\"\" Class to provide for a source provider that is totally within memory. \"\"\" def __init__ ( self , source_text ): self . next_token = source_text . split ( \" \\n \" , 1 ) def get_next_line ( self ): \"\"\" Get the next line from the source provider. \"\"\" token_to_use = None if self . next_token : if len ( self . next_token ) == 2 : token_to_use = self . next_token [ 0 ] self . next_token = self . next_token [ 1 ] . split ( \" \\n \" , 1 ) else : assert self . next_token token_to_use = self . next_token [ 0 ] self . next_token = None return token_to_use This class contains very simple logic. When the instance of the class is initialized, it starts by breaking down the input text into a tuple. The first element of the resultant tuple contains the next line to be parsed and the second element of that same tuple contains the input text to be parsed in the immediate future. Once that calculation has been performed, the rest of the processing is relatively simple. If get_next_line is called and the tuple contains 2 elements, the first element is returned as the next line, and the next_token variable (for next time) is recalculated using the same expression as was used in the __init__ function. When the get_next_line is called at the end of the file, the tuple contains only 1 element. At that point, that singular element is returned as the next line to be parsed, and the next_token variable is set to None to make sure we end the processing properly. Finally, when get_next_line is called and the tuple is set to None , there is nothing left to parse and None is returned, signaling that the provider has reached the end of its available text. To be clear, this is the exact code that was in place for the duration of the parser testing, just repackaged to be in a more reusable form. Its interface is plain and simple: it either returns the next line as a string, or it returns a None object if there are no more lines. Nothing fancy as a class either, just a simple interface: one function to create the instance and get it setup, and one function to read the next line. Continuing with the FileSourceProvider By keeping things simple, creating the class FileSourceProvider was almost as simple as the refactoring to create the InMemorySourceProvider class. While I want to keep options open for future performance experimentation, I just needed something simple for reading a file from the file system. Based on those qualifications, I came up with this: class FileSourceProvider : \"\"\" Class to provide for a source provider that is on media as a file. \"\"\" def __init__ ( self , file_to_open ): with open ( file_to_open , encoding = \"utf-8\" ) as file_to_parse : file_as_lines = file_to_parse . readlines () self . read_lines = [] self . read_index = 0 did_line_end_in_newline = True for next_line in file_as_lines : did_line_end_in_newline = next_line . endswith ( \" \\n \" ) if did_line_end_in_newline : next_line = next_line [ 0 : - 1 ] self . read_lines . append ( next_line ) if did_line_end_in_newline : self . read_lines . append ( \"\" ) Basically, open the file, read in the lines, and process the lines into the format that we expect. The only tricky bit with the class's __init__ function was handling line terminators properly. In fact, that is the only purpose for the did_line_end_in_newline variable, remembering if the current line ended with a newline character before it is removed. Based on independent unit testing of the class, I had problems with the characters at the end of the file, which adding that variable and the final if statement resolved cleanly. I am not sure if I feel that the did_line_end_in_newline variable is a kludge or not, but I do feel that it was the right thing to do in order to maintain the fidelity of the data being read in from the file. Because care was taken in the provider's __init__ function to do all the necessary processing, the get_next_line function is very basic: def get_next_line ( self ): \"\"\" Get the next line from the source provider. \"\"\" token_to_use = None if self . read_index < len ( self . read_lines ): token_to_use = self . read_lines [ self . read_index ] self . read_index += 1 return token_to_use While this function could be more complicated (or simplified depending on your viewpoint), I feel that this is a good example of keeping things basic. The provider reads the information into an array of strings during the __init__ function, and this function simply uses an index to iterate through and return each element of that array. Nothing fancy for now, just some code that is very functional. Fancy code can always be added later. Testing the Source Providers To make sure both providers are adhering to the interface in the same way, I added a decent number of tests in the test_source_providers.py file. In all the tests, the big thing that is being tested is if the source providers return the correct lines given the correct input. If there are 2 line terminators in the input, each provider must return 3 lines, even if the last one is empty. Every test is a variation on that, thoroughly exercising each provider to ensure that both adhere to the interface flawlessly. After all, if the parser gets bad input to tokenize, it cannot help but to produce bad output, even if is only off by a line terminator. Replacing Print with Log This improvement was a long time coming: replacing all of the print statements in the parser with log.debug statements. When I was developing the parser, adding a simple Python print statement was the easiest way to add extra debug to the output of the tests. This information was pivotal in my ability to debug the parser and quickly add new features to the parser with confidence. And in the cases where there were problems with those features, those same print statements were also pivotal in helping me ensure the flow of each function was as I had designed it. Why did I avoid using log.debug statements from the beginning of development, and instead use print statements? I am honestly not sure. I do recall an early experiment in which I used both types of statements, to see which one worked better for me. I remember the experiment, I remember choosing print statements, but I cannot remember why I chose them, knowing I would have to replace them later. I even checked my notes from back then, and nothing about logging vs print. Interesting. Regardless of why I did it, the time to fix it was now. It was a mostly painless transition and didn't take that long to accomplish. To the start of most files, I added the following import: import logging and at the start of many blocks of processing, I added the following line: logger = logging . getLogger ( __name__ ) Then, for each time I called the print function, like this example: print ( \"Line:\" + line_to_parse + \":\" ) I replaced it with the exact same arguments, just changing the name of the called function from print to logger.debug as follows: logger . debug ( \"Line:\" + line_to_parse + \":\" ) After the initial changes to replace print with log.debug , everything looked okay until I ran the normal clean script that I use with the project. This script is a simple script to execute the black code formatter, the Flake8 and PyLint linters, and the full set of tests for the project. When the script got to PyLint, it seemed to go crazy and was emitting lots or warning lines, each line essentially being the same. Reading the warnings carefully and looking at the source code, PyLint seemed to be complaining about each logging function call that involved concatenation. In each case where I was concatenating strings to arrive at what I wanted to log, PyLint raised a warning that I wasn't doing it right. According to PyLint, the proper way to log a message for the above example is: logger . debug ( \"Line: %s :\" , line_to_parse ) Doing a bit more research, the reason for the warning is because the logging library was purposefully created to be lazy. If the log level for a given call is not high enough to cause the string to be logged, doing any kind of formatting or concatenation on that string is wasted effort. Following that logic, the logger follows the same conventions that are used with the percent character ( % ) as the string interpolation operator , delaying the evaluation of the actual string until the logger determines whether the specified string is actually going to be logged. Once a positive determination has been made, the format and the arguments are applied to each other, a resolved string is produced, and that string that is then logged. It took a while to go through each of those messages. I had to examine each concatenation sequence, break it down into its component parts, and verify my changes. Each string that was concatenated needed to be represented in either the format string or the arguments passed to the logger. It was slow and pedantic work, but in the end, I was happy to have a logging solution that was more performant than before. Side Note Note that as a Python logging newbie, I am not 100% sure if I created more work for myself by frequently creating logging instances inside of the project's static functions. It is possible that I can get away with a static logger variable created in the module's namespace at the top of the file, and not worry about creating any other loggers within the same file. However, in all the examples I have seen to date, the logger is either declared at the top of a simple file or within the __init__ method of a class. As a lot of the helper classes are a collection of functions that are labelled with the @staticmethod annotation, I am not sure if one instance at the top of the file is the correct way to go. While it might be more effort than I really need, I am confident that I am covering all the logging situations properly. If I learn something differently about logging, I will come back and revisit it. Better Error Reporting When I initially proofed out the plugin architecture for the linter, I added a BadPluginError class to help identify plugin issues and report them back to the command line. Using this same pattern to integrate error handling for the parser to the linter, I added the BadParsingError class, raising this error when there were exceptions raised during the parser's tokenization of the Markdown text. A bit more typing and a short while later, I had the __handle_error function with refactored content. This newly minted function reused the error handling meant for the BadPluginError class to handle both the BadPluginError class and the BadParsingError class in the same manner. With that tweak done, I had confidence that the error reporting was done, and I wouldn't need anything more serious until far later in the project. That is what I foolishly thought until about 2 days later. After doing some work on implementing the first two rules, I realized that some things needed to be fixed with error reporting. The first change I made was to change the name of the error class from BadParsingError to BadTokenizationError to reflect the problem area more accurately. While it is more of a mouthful to say, it accurately describes that it is a problem with the tokenization of the document, not a generic \"parsing\" issue. A good example of this is when the TokenizedMarkdown class is created. Upon creation, one of the things that it does is load the entities.json file from the resources directory, as detailed in this article . If that resource file is not loaded properly, that code was changed to raise a BadTokenizationError instead of what it was previously doing. Without this file, parsing the Markdown text would still be possible without any issues. But to properly tokenize the Markdown text, the parser needs to know if the named character entities that are provided in the Markdown document refer to valid named entities. It may appear to be a semantic difference to some, but in my experience, it is the attention to detail on little things like that which help improve the project's maintainability. In addressing the above case, I stumbled into the second issue: too many exit points. While it does not show it in the commit for the BadTokenizationError fix documented in the paragraph above, the first pass at addressing that issue was messy. It caught any raised error, did some bare bones reporting, and then performed a sys.exit(1) to stop the program with an error code of 1. Doing a quick search through the code, I stopped counting once I hit the third instance of a sys.exit(1) call in the code. It was time for a refactor. Beginning with the initial case that started the search, I took a quick look at the various search results and came up with a good rubric to follow. If possible, I would try and more correctly classify the error using one of the two existing error classes, BadTokenizationError or BadPluginError , which were already handled at the top level by the __handle_error function. If the error occurred in the main.py file and didn't fall into either of those categories, I would call the __handle_error function directly, striving to give a clean and concise message on what that error was and why it occurred. If I encountered any errors outside of those parameters, I would save them for last and re-evaluate my rules. Based on what I found in the project, the first two rules were enough, as I did not find an error that could not neatly fit into one of those two categories. Instead of a few different ways of exiting the linter with an error condition, __handle_error was now the sole conduit for exiting with an error. I went over the tests in the test_main.py , test_plugin_manager.py and test_markdown_entity_and_numeric_character_references.py files to make sure everything looked consistent, and things were good! What Was My Experience So Far? As I have mentioned in previous articles, I like refactoring, and this stretch of refactoring was no exception. Each of these tasks were little tasks, but I felt better knowing that they were addressed before I shifted my focus to writing the rules for the linter. The change to using source providers would be pivotal in dealing with test sources (strings) and live sources (files) as sibling concepts. Replacing print with log.debug was also pivotal to using live sources, keeping the ability to debug what was going on for experienced users, but not flooding a casual user with all of that information. Finally, getting all of the error reporting to go through one conduit just seems cleaner and more concise to me. Getting these taken care of just felt like the right refactoring to do at the right time. I also realized that I enjoy the cadence I have established with refactoring. While there are short bursts where it is just adding new features, for the most part I have tried to switch off between refactoring and adding new features. I feel that for my development style, I have found a good balance between fixing things and adding things, even if it needs tweaking every so often. Perhaps it is because of that sense of balance that I have been able to focus on this project for so long without losing energy on it. For me, I believe it boils down to a feeling that this project is not about adding features as much as it is about continual improvement of the project towards its goal, taking my time to do things right as I go. Looking forward, I am sure I am not done adding features, refactoring code, or learning something new and interesting. What is Next? Taking a bit of a jump back in time a couple of days from where I am leaving off, in the next article I am going to start talking about the rules that I am developing, and how their development proceeded. Please stay tuned! Due to link reference definitions being a multiline element, some of the error cases required the parser to be rewritten to handle lines that need to be \"rewound\", as documented in this previous article . ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/05/04/markdown-linter-core-pre-rule-improvements/","loc":"https://jackdewinter.github.io/2020/05/04/markdown-linter-core-pre-rule-improvements/"},{"title":"Markdown Linter - Splitting Up The Articles","text":"Introduction Wow! Getting to this point in the parser took a good amount of time, but in my head, I didn't expect it to create 17 articles! Looking back at what I wanted to achieve by writing about the linter as I develop it, it makes sense to me, and I am not sorry that I wrote any of the articles. However, that does leave me with a bit of a problem. This is article 18 in a series on writing a linter, and I have not really done any serious work on the linter yet. Splitting Up the Articles While going back and changing the names of the articles to \"Markdown Tokenizer…\" was attractive, thinking about it made me feel like I would be lying to any readers. In the end, the best option that emerged was to use this article as a jumping off point for the other aspects of the project. Basically, a jump page. While not glamorous, it is honest, and follows one of the things that I am trying to inspire: \"Stuff happens, pick yourself up, dust yourself off, and figure out what to do next.\" Jump Links For the Linter Project Here are the various directions that I have gone in documenting the further development on the project. Chronological Core Linter Linter Tokenizer Linter Rules","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/05/03/markdown-linter-splitting-up-the-articles/","loc":"https://jackdewinter.github.io/2020/05/03/markdown-linter-splitting-up-the-articles/"},{"title":"Markdown Linter - Reducing the Parser's Technical Debt","text":"Introduction First off, despite what I said at the end of the last article , no, I am not going to call this \"Refactoring: The Sequel\"… even if that would be a cool name. From my point of view, refactoring is anything involving getting rid of technical debt, usually increasing the correctness, readability, or maintainability of a project. This can be as simple as writing clear and concise documentation, or a more difficult task involving the rewriting of project code. If a task moves the project in a positive direction while not adding a new feature to the project, I believe that that task falls under the category of refactoring. It is a simple definition, but it has served me well. Since this project has been ongoing since 22 Nov 2019, there have been a decent number of issues that have been added to the parser's debt list, and a decent number of issues removed from that same list due to informal \"refactor weeks\" that I had. Most of the issues that I added to that list had a very specific reason that I decided to put them on the list instead of handling that issue right there. But as I am only human, there were probably a few times where it was just easier to deal with it later. Regardless of how each issue was added to the list of debt, I wanted to try and make a decent size dent to the pile of parser's debt before closing out the main development effort on it. As my goal is to give better perspective on what I am doing and why, I thought an article just focusing on these items would be informative to anyone that has been following the project so far. Just to keep everything above board, I am not going to talk about every individual commit between the two commit points noted in the next section, just the interesting ones. In that uninteresting category are some changes to get ready for future features, some changes to fix cut-and-paste issues from past changes, and some changes to correct something that I forgot to do. While they are necessary, they are not interesting. They just prove I am human and make mistakes just like everyone else. And like everyone else, I sometimes put things off until later. But at some point, the bill is due. For me, it was now that I wanted to pay some of that bill off. What Is the Audience For This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 05 April 2020 and 11 April 2020 . Refactor 1: Matching CommonMark Character Encoding The issue that was the parent of this one was an interesting issue to find, and this issue was generated to perform the cleanup for that issue. The HTML output for the examples in the GFM specification are generated by processing the Markdown text examples through CommonMark , mainly because CommonMark is considered the reference implementation for the specification. In each of the following scenario tests, the parser's output for the href attribute in each link was arguably correct, but the output did not match the output in the examples. To resolve this issue and ensure that the scenario tests passed, I needed to fix the link functionality in the parser so that it encodes the right characters according to CommonMark's \"right way\" to ensure the parser passed the tests. It would have been easy if the encoding misses were all the same thing, but they were not. In the href attribute for example 603 , the ampersand character & is replaced with the string &amp; . In example 510 , the \\ is replaced by the string %5C . However, in example 509 , the ? and # characters are both left alone, and not specially encoded. Just small differences in how the characters were represented, and each one a bit different. To be clear, the main goal of resolving the original issue with those tests was not to figure out what the \"actual right way\" of encoding the href attribute was, but to match what CommonMark was doing. Specifically, I wanted to make sure I have a solid starting point for matching my parser against others, and CommonMark seemed a better candidate than others. After all, they published a specification on what they expect from the CommonMark parser in the GFM specification. But as the above cases only caught a small number of issues in 3 tests, I determined that I needed to figure out what the proper set of ASCII character encodings are for CommonMark. To properly figure out what CommonMark encodes and does not encode, I added in the tests in the test_markdown_extra.py file that specifies, in sequence, each of the possible characters after the space character and before the del character in the ASCII table. While I could have done something clever using the Python string constants , I wanted the test to be explicit about what it was testing. After submitting the newly generated Markdown sample to Babelmark , I copied Babelmark's response for CommonMark back into the test as the response. From there, I just kept on adjusting the encoding code until the HTML output matched Babelmark's output for CommonMark exactly. Did I have to do this? Not really. Technically, I was already meeting the minimum requirements by passing the scenario tests. However, I just felt that it was better to do the thorough job than to leave it to chance. Refactor 2: More Fun with Tabs When people start developing code in any programming language, one of the first things that they learn is that the tab character in most languages is a bad idea. After that, the religious conversations start about whether to replace a tab character with 2 spaces or 4 spaces, but that is not something I want to go into. Those kinds of conversations usually just devolve into bike-shedding within minutes. In the GPF specification, they avoid this conversation nicely by making it clear: Tabs in lines are not expanded to spaces. However, in contexts where whitespace helps to define block structure, tabs behave as if they were replaced by spaces with a tab stop of 4 characters. Good! So tab characters have a width of 4 characters. 1 That sounds right. I then changed my calculate_length function to count every tab character as 4 characters and every non-tab character as 1 character. No problem. Then I went back to fix example 6 which was marked as skipped and example 7 which was added incorrectly as a test. Reading the preface for those tests: Normally the > that begins a block quote may be followed optionally by a space, which is not considered part of the content. In the following case > is followed by a tab, which is treated as if it were expanded into three spaces. Since one of these spaces is considered part of the delimiter, foo is considered to be indented six spaces inside the block quote context, so we get an indented code block starting with two spaces. That statement did not sound right to me. The math was wrong. The block quote start character is followed by 2 tab characters for a total count of 8 whitespace characters. Subtract 1 from that for the whitespace character following the block quote character and you have 7, not 6. I read the entire section on tabs about 4 or 5 times, being either persistent or stubborn, depending on your point of view. Then finally I keyed in on the phrase \"expanded into three spaces\". Something was wrong. I rechecked my math using 3 spaces instead of 4 spaces for that example, and it worked. But it said at the top that tabs were the equivalent of 4 spaces, didn't it? Rereading the preface for Markdown tab characters, for what seemed like the 10th time, it was then that I noticed it. Not a tab width of 4 characters, but a tab stop of 4 characters. While it may seem like a simple difference, it is not. A tab stop of 4 means that if a tab character is encountered, the parser should consider there to be spaces until reaching a column index that is a multiple of 4. In the above example, the tab character was encountered after the block quote start character, effectively at an index of 1 on that line. That meant when the parser encountered the tab character, it should be considered the same as 3 space characters, as it would take adding 3 space character to get from the current position of 1 to the next tab stop at index 4. While somewhat interesting, the impact of those changes was more interesting to me. The first impact was that I needed to rewrite the calculate_length function to expand tabs based on a possibly non-zero index starting point. This wasn't a really big deal as it was a small function, but as the complexity increased enough to where I was worried about properly calculating the length, I added the test_calculate_length module to properly test all of its cases. The second impact was on the block quote blocks and list blocks, both of which are container blocks. Before this change, the start position of an enclosed block inside of a container block was not an issue. With this change, to properly account for tabs and tab stops, the parser needs to know the exact position on the line where the current string started in case it contained tabs. This proved to be complicated enough that I spread it over 2 commits. In between, the third impact was discovered, the tokens for indented code blocks. As a bit of a shortcut, instead of grabbing the required whitespace from the string being parsed, I simply used a string containing 4 space characters as the leading whitespace for the code block. I needed to change that to grabbing the correct whitespace as the length of the tab character was now variable. In the end, 3 small examples, 1 tab character, 3 impacted areas, and a lot of grumbling on my behalf. But it got fixed… it just took a while. Refactor 3: Fenced Code Blocks In getting rid of some of the technical debt, there were a couple of issues with the fenced code blocks that I really wanted to address: fenced code blocks inside of block quotes and fenced code blocks with starting whitespace. The examination of the first issue, fenced code blocks inside of block quotes, started off easy enough. In example 98 , which is as follows: > ``` > aaa bbb a fenced code block is started within the block quote. As such, when the block quote ends, the fenced code block is closed. That part was already working fine. However, when the first and only line in the code block was extracted, it was extracted with 2 extra characters of whitespace in front of it, providing for a token representation of [text:aaa: ] instead of [text:aaa:] . This in turn added the string aaa to the HTML document instead of aaa , causing the scenario test to fail. This issue was easy to fix, making sure that the indent count was considered when returning the remaining string in the fenced code block. Along the way, just to make sure things were parsing properly, I added 2 extra variants of example 98, one with an extra > character at the start of the second line and one without any > at the start of the second line, just to make sure they were properly parsed. Seeing as there was already one issue found in that area, I figured the extra tests would not hurt. Following that quick find-and-fix, I moved on to example 101 and example 103 which also dealt with inconsistent spacing with fenced code blocks. However, in the case of these examples, it was an issue with properly handling the leading spaces before the fenced code block start, code block content, and fenced code block end. In the case of example 101: ``` aaa aaa ``` the fenced code block start sequence is preceded by a single space character, as is the first line of the code block content. The specification clearly states: If the leading code fence is indented N spaces, then up to N spaces of indentation are removed from each line of the content (if present). Looking at the source code, it was obvious to me that any indentation for the fenced code block start was extracted and then immediately thrown out. A quick change to how the fenced code block was storing the information to include the indent count of the fenced code block start, some other code to remove up to that many whitespace characters from the start of any content lines, and it was done. This was the same for example 103, just with more of an additional indent, and the added concern of having one of the lines have fewer indent characters than the fenced code block start line. These two small issues only took a matter of hours before they were fixed, but they were good fixes. I am not sure why, but I felt that these issues were going to be larger than they were and seeing them in my \"to fix\" list was causing me a bit of stress. Regardless of the effort required, it was good to get them taking care of. Sometimes you move the project in a positive direction in big steps, and sometimes they are baby steps. The important thing to remember is that they are both moving the project in the right direction. Refactor 4: HTML Blocks inside of Lists Sometimes I look at some code, and I get a feeling that it isn't quite right. I often cannot immediately point to something specific, but I just have a feeling that usually turns out to be accurate indicator of bad code. That was the case with example 144 . Ignoring any previous statements that I have made about the use of HTML in Markdown, I wanted to make sure that this example was being parsed properly. In this example: - < div > - foo what should be created is a list with an item containing a single <div> HTML item and another list item with the text foo . To be honest, I had to check the HTML and list specifications a couple of times to find out which of them overruled the other. As I should have gathered, the list (a container block) has higher precedence than the HTML block (a leaf block), so when the next list item within the list block starts, the HTML block from the first item is automatically closed. It is probably because of my uncertainty with the specification regarding which block had the higher precedence that I decided to add an extra test, test_html_blocks_144a : - < div > - foo foo I realize it might seem like the extra paragraph is unwarranted, but I wanted to make extra sure that everything was being unwound properly, including the list. Whether or not the extra test really adds anything, I'll double check it later. Even though there was nothing wrong with this test, I still trust my previous experience with code. I would rather double check some code and find nothing rather than not check and miss something important. But at the same time, it had me looking at the list block code, which is how I started looking at the issues for the next section. Refactor 5: More Fun with Lists As I mentioned previously, I often get a feeling that a given set of code is not 100% correct, and this time it was about lists, specifically sublists. While each of the required scenario tests for sublists were passing, there was just this little voice in my head saying that something did not feel right. There was just something there that inspired me to add an issue to my notes to check them out further, but not enough for me to give anything more than a vague feeling. Unlike how things turned out in the previous section with HTML blocks, after I did some digging in the code, I found that my feeling was correct! Due to my digging, I found several problems with sublists, specifically with indenting and switching between sublists and other lists. To debug these problems and to make sure they stayed fixed, I introduced a total of 16 variations on existing, passing scenario tests to properly address these issues. I am still trying to figure out if these issues are due to how I implemented the parser or if they are legitimate edge cases for the specification. Stay tuned for that! These issues fall into 2 categories: indenting and switching list start sequences. My examples will mostly deal with unordered lists, but I added the same types of tests for both ordered and unordered lists. As a good example of what I looked for, consider example 281 : - foo - bar + baz Looking at this Markdown, it should be obvious that it is a pair of unordered lists, the first having two elements and the second having a single element. Playing around with the structure a bit, I decided to keep all the list start characters the same but changed the indent of the second list item to make it into a sublist. * foo * bar * baz And… boom! The parser returned an unordered list that contained another list with 2 items in that sublist. Thinking it was a fluke, I changed the last list start character to the + character: * foo * bar + baz And boom… again! The results were not as expected. Working my way through many of the examples of lists in the GFM specification, I found 16 variations of simple changes to existing scenario tests that created sublists that were parsed incorrectly. If I worked them out on paper or by using Babelmark, the paper and Babelmark results backed each other up, but the parser's results were off. I was missing something important somewhere. But where? Taking a bit of time to cool down, when I started looking at those examples and various patterns of indentation, start character, and sublists emerged. Adding some debug code to aid in the diagnosis, I ran each of the newly added tests again. After some screaming at my monitor in frustration 2 , I was rewarded with a common thread that went across all of the tests: starting conditions. When I start a list in the parser, the most relevant information about the list start is the starting index of the list and the start character. For single level lists and lists that are neat (that is all the sublists are indented the same amount), there are no issues with this approach as everything lines up. But when I started moving the starting locations of the list items and the starting locations of the text within the list items, that is when things fell apart. The big issue was not with the parser going down into the sublists, but in how it recovered when exiting from those sublists, understanding which lists to take off the list stack on the way back out. With a large amount of grumbling on my part, I worked through each of the new scenario tests and added code that not only remembered where the list item character was, but where the text for that item started. This allowed me to properly figure out where the list itself started, allowing me to create better comparisons on whether or not a list should be removed on the way out. It wasn't an easy experience, but one by one the failing tests became passing tests as I worked through the various combinations required to get it working properly. Sure, I grumbled a lot on this one. But I was glad that I followed a feeling and did some exploratory testing to put that feeling to rest, rather than dismiss it! In my experience, there are many times where you can look at code and instantly know it is wrong and why it is wrong. (And no, I am not talking about tabs, or line spacing, or curly bracket positioning, or…) However, the really interesting times are when you have to trust your gut feeling and explore some scenarios, either to find an issue or to be able to increase your confidence to where you do not have that feeling anymore. Whether it is a \"I can see that is wrong\" bug or a \"I feel like there is something wrong there\" feeling, both have a valid place in your arsenal for improving the quality of the project. What Was My Experience So Far? The purpose of this dedicated session of refactoring was not to eliminate all the parser's technical debt, just make a good dent in it. Based on that metric, I feel good that I have reduced this debt a decent amount. Furthermore, I took some steps to reorganize the remaining items into more organized lists, for further study. After making some forward progress with other components of the linter, I will inevitably come back to the list in a few weeks and try and deal with a couple more of those items. Other than the logistics, I feel good about the parser. There are a decent number of issues documented in the list, 40 items in all, but a lot of them are either suggestions for more complete testing or questions about whether something is possible or desired. Doing a quick scan of those items as I write this article, there are not any open items that I am very worried about, just concerns and ideas that I want explored. Basically, I have things that I want to deal with, but it is okay if they wait a bit. It is with mixed feelings that I move from writing the parser to writing the rules that will take advantage of the parser. I know that I am moving from having conquered one challenge to starting another challenge, and I am okay with that. On one hand, that transition means that the hard work I have put into the parser will pay off. On the other hand, it means that I will be subjecting that same work to more stringent tests as I go forward with the rules and the rest of the linter. In the end, I guess it boils down to two things. I have confidence in the work I have done to date with the parser, and if anything is missing or needs fixing, I know I will be able to handle it. I also know that the best way to show my confidence in the project is to move ahead and write rules that rely on that parser. What is Next? While I know the parser is not perfect, and it may never be, I know it is good enough to base the rules on top of it. As such, my next set of tasks involves making some changes to how the parser interacts with the rest of the project code, to ensure that it can handle a wide variety of requirements that I may throw at it! Yes, I know. That does not sound right to me now either. But at the time, I honestly thought it said a tab width of 4 characters. Keep on reading. ↩ While I don't frequently use screaming at my monitor as a form of rubber duck debugging , it does have it's benefits sometimes. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/04/27/markdown-linter-reducing-the-parsers-technical-debt/","loc":"https://jackdewinter.github.io/2020/04/27/markdown-linter-reducing-the-parsers-technical-debt/"},{"title":"Markdown Linter - Adding Image Links and Simple Cleanup","text":"Introduction While it just happened to be the feature that was last on the list, I feel that it was kind of fitting that image links were the last feature to be added. Whether or not an author creates Markdown content for a blog, documentation, or some other purpose, a simple image can often improve the readability of each of those forms of documents. Imagine going to your favorite game blog or art blog and seeing very good descriptions of the topic being blogged about, but no pictures. It just makes sense to add images to documents, where needed, to enhance that document's ability to convey the message that the author is delivering. From there, there are just a small number of cleanups that I want to do before going on, so let's go! What Is the Audience For This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 27 March 2020 and 04 April 2020 . What Are Image Links? As I have covered in previous articles, a simple inline link is constructed as: [ foo ] ( / url \"title\" ) and the corresponding reference link (and matching link reference definition) is constructed as: [ boo ] : / url \"title\" ! [ boo ] both producing the following HTML: < p >< a href = \"/url\" title = \"title\" > foo </ a ></ p > In both examples, by changing the opening character sequence [ to the opening character sequence ![ , we transform the destination HTML into the following image link: < p >< img src = \"/url\" alt = \"foo\" title = \"title\" /></ p > Besides the different starting characters, the first difference is that instead of the a or anchor tag being produced, the img or image tag is produced. Due to that change, the href attribute is replaced with the src attribute, the title attribute is left alone, and the inline text contained with the normal anchor link is replaced with the alt attribute text. The second difference is in the text that is assigned to the alt attribute. For normal links, the link text or link label is interpreted as inline text and placed between the <a> and </a> tags. As this text has been moved to the image tag's alt attribute, a transformation must occur to ensure that only valid text is assigned to the attribute. While not strictly specified as a \"MUST\" in the specification, the images section of the specification mentions: Though this spec is concerned with parsing, not rendering, it is recommended that in rendering to HTML, only the plain string content of the image description be used. This transformation is on display with example 581 as the Markdown text: ! [ foo * bar * ] [ foo * bar * ]: train . jpg \"train & tracks\" is transformed into: < p >< img src = \"train.jpg\" alt = \"foo bar\" title = \"train &amp; tracks\" /></ p > In the above example, the * character indicating emphasis is removed from the string foo *bar* , leaving the attribute value foo bar to be assigned to the alt attribute. For other inline processing sequence, similar transformations are made to ensure that the vital information is kept with the token. The final difference is a small change to the rule that links may not contain other links. This rule is changed so that links may not other links unless those links are image links. Consider example 525 : [ ![moon ] ( moon . jpg ) ] ( / uri ) which is transformed into: < p >< a href = \"/uri\" >< img src = \"moon.jpg\" alt = \"moon\" /></ a ></ p > While this may not look useful, consider the following Markdown: [ ![moon ] ( https : // static . thenounproject . com / png / 2852 - 200. png ) ] ( https : // www . bing . com / search ? q = the + moon ) Once transformed, that Markdown becomes the following image: That Markdown snippet produced an image tag within an anchor tag. This presents the reader with an image of a moon that may be clicked on. When that image is clicked on, the link that surrounds the image link is acted upon, navigating to the link supplied in the outer link. In this case, click on the image of the moon takes the reader to the Bing search page, already primed with the search text for the moon . This feature used often to present styled buttons to a user instead of simple text for them to click on. Personally, I think the right image to click on makes more of an impact and having that ability available to Markdown authors is a plus. Simple Cleanups With image links done, what remained were the various cleanups that I wanted to do before declaring the parser \"done\". While each of these cleanups required code to be rewritten, my stated goal with these cleanups was not to fix bugs, but to get the code base in a more maintainable shape before fixing those bugs. It was my hope that by doing things in this order, it would make any bug fixing that would occur in the future easier to perform and easier to validate. We will see how that worked out in the next article! For now, on to the cleanups! Simple Cleanup 1: Splitting Up the TokenizedMarkdown Class The first cleanup on my list was to split up the TokenizedMarkdown class into more clearly defined classes. While I could have done this at an earlier stage of the project, I wanted to take a different approach with this project. While the ParserHelper and HtmlHelper classes were required for the early stages of the project, there were other possible groupings I wondered about. Basically, I wondered if I would make the same grouping decisions at the end of the parser phase as I would have at the start of that same parser phase. In short, I wanted to experiment. More on the results of that experiment at the end of this section! Going through the code and separating each grouping out was a chore, but a useful one. The first effort was to come up with the larger groupings. Arriving at these groupings was a simple task, as the processing for the tokens is broken up into three sections: the functions for the preliminary tokenization of the input were left in the TokenizedMarkdown class; the functions combining contiguous text blocks were moved into the CoalesceProcessor class; and the functions to handle inline processing were moved into the InlineProcessor class. This was a major chore, but it reinforced the separation between the various processors and classes in the process, so all was good! With those major regroupings undertaken, two of those three classes still contained huge blocks of code, so it made sense to do further refinements along more feature-based lines. The TokenizedMarkdown class was my first target and was broken down into the ContainerBlockProcessor class and the LeafBlockProcessor class, which just seemed like logical groups to extract from the main class. From there, it similarly seemed that the ContainerBlockProcessor class was still too large, so I further extracted the ListBlockProcessor class and the BlockQuoteProcessor class. At this point, I felt more confident that the different functions for tokenization were in solid, well-defined, well-sized groups. Similarly, the InlineProcessor class was too large so I extracted functions into the EmphasisHelper class, the LinkHelper class and the InlineHelper class. The main brains of inline processing remained in InlineProcessor , coordinating when to apply inline processing to the various tokens that needed it. To allow for special processing to be handled properly, the coordination of the [ , ![ , and ] link strings, and the * and _ emphasis strings was also kept in the InlineProcessor class, while the actual processing of emphasis and link were moved to the EmphasisHelper class and the LinkHelper class. The remaining functions that implemented the less intensive inline processing were added to the InlineHelper class. Like my observation for the tokenization classes in the last paragraph, to me this just seemed to be the right groupings for these functions. Comparing my choices against some notes I had scribbled down at the start of the project, I found that I was decent at projecting the larger strokes of the grouping, but really bad at my stab on the more finer groups. While the names are different than the ones I used, I was spot on with the 3 high level processors: tokenizing, coalescing, and inline. Furthermore, when it comes to the specific classes for container blocks and leaf blocks, I had those spot on. And it was there I stopped. I had a couple of scribbles for emphasis being on its own with a question mark beside it, but that was it. The rest of the scribbles were all followed by question marks, including a guess that each leaf block and container block should be in its own class. If I had to guess as to why I thought that way, I would wager that I thought I would need more code for the inline processing while using less code than I anticipated for the block processing. When those assumptions on code amounts changed, it caused me to look at the rest of the code and extract more specific groups into the groups detailed above. The end results? While I feel it was good to do a good high-level design of the function groupings, I need to be prepared that those plans are going to need to be revisited and redefined once I starting writing code. But still, a useful experiment, and the project's code base was also more maintainable at the same time! Simple Cleanup 2: Reducing Complexity Up to this point in the development of the project, I was more intent on completing the parser itself than to complete it with good organization and low complexity. While the simple reorganizations that I documented in the last section were a start, there were three sets of complexity warnings that I disabled until later: the too-many-branches warning, the too-many-statements warning, and the too-many-locals warning. While none of these affected the quality of the parsing, I knew that a result of disabling these warnings was that I would have to start resolving them at the end of the main block of parser development. As that time had arrived, my bar tab was now due. For the most part, the resolution for all three warnings was the same: split the code that the warning was complaining about into smaller, more focused functions. Too many branches? From experience, I have only run into 2 or 3 cases where having a method with more than 10 or 15 branches was the right thing to do. Most of the time, too many branches means you are doing too much in your function, and by splitting up the functionality into multiple functions, you keep the number of branches down and the comprehension on what each function does goes up. It's almost always a win-win. Too many statements? Pretty much the same argument, but exchange statements for branches. Once again from experience, if you have a function that has more than 25-30 statements in it, it is hard for most people to truly understand what that function is doing without a lot of comments. Splitting the functionality across multiple functions allows for those statements to be associated with named functions that describes its purpose, instead of the reader trying to figure out \"what does that section do?\" Too many locals? This is a tricky one with parsers, so I left it for last. In most cases, what I described for branches and statements goes for locals. Too many of them gets in the way of a clear understanding of what a function does. From personal experience, when the number of locals in a function exceeds somewhere between 10 and 15 variables, I usually need good logging or a good debugger to really understand what each of the values should be at any point in the function. Under 10 and I am okay. But parsers? They are often the exception to the rule for a lot of things. The amount of state needed to properly parse something often causes a lot of locals and arguments to be declared and passed around, many of them for temporary computations. For example, whenever a specification says \"collect all X up to Y, except for\", it usually means that one variable is needed for collecting, possibly one variable to report errors, and the \"except for\" at the end most often means passing in some kind of state, often from another part of the program. Since parsers are more grammatically based than other types of programs, their need to interface with these things called \"users\" causes more variations that need to be dealt with than other types of programs. At least from my experience, that seems to be the case. For the most part, I managed to clean up all cases of the branch warnings and statement warnings and tried to address the local warnings. While I did not complete all of them, I felt it was a solid effort to get the parser into a more maintainable shape. And as with all the other cleanups documented here, I do believe that each one is contributing to the health of the project. And that is a good feeling in my books! Simple Cleanup 3: PyCharm Static Code Analysis When I am authoring projects, I typically have a specific development environment or editor for each language. For my C# projects I use Visual Studio by Microsoft, and for Java projects I use IntelliJ by JetBrains. In both cases, these editors are widely accepted as one of the best editors for that specific language. While Visual Studio can handle Python and JetBrains has a Python specific editor (PyCharm), I find that I personally prefer a one language to one editor relationship. As such, I write most of my Python code in Visual Code. However, while I do not use either editor for development, I do find that PyCharm has some usefulness in running static analysis passes on my Python projects. From experience, the hints that PyCharm provides make the cost of the manual use of it as a Static Code Analysis tool worthwhile. And as I was finished with the bulk of the development on the parser part of the project, I thought it was a good time to look at the project through that lens. One immediate benefit was that PyCharm has a very decent analysis engine, and it gave me hints on how to improve the flow of my functions. When none of the other functions that use my function make use of the function's return value, PyCharm told me that it makes sense to remove the return value. When arguments or variables were not being used, PyCharm suggested that they should be removed. And when I got into the bad habit of incrementing a value by using a = a + 1 , PyCharm pointed out that Python also has the += operator and I should rewrite the previous snippet as a += 1 . All in all, a lot of small, but useful improvements to clean up the project. While my manual usage of PyCharm does not replace the need for running Flake8 and PyLint over the project before I commit any changes, I believe it offers me a fresh view of the code base and gives me hints on where it thinks I can do better. As an added bonus, with few exceptions, the changes are minor, and the entire batch of changes can be applied quickly. Personally, I consider using PyCharm in this manner akin to asking a friend to take a read over a document that I have been working on before I submit it. As such, it is not always about making the changes that PyCharm suggests but thinking about those changes and deciding if those changes are what is best for your project. Simple Cleanp 4: Pulling Strings to The Top I wanted to do this cleanup for a while, partially for code cleanliness reasons and partially for performance reasons. In writing the first iteration of the parser, I knew that I was focusing on getting the parser complete and not always following best practices. One of those best practices that I use, especially for parsers, is to make sure that strings that I know are going to be used over and over again are pulled out of their functions/methods and defined once at the top of the class. In the hurry of wanting to get the parser done, I had not kept this practice up. As such, there were multiple instances where I had certain strings, such as the block quote string > , in multiple places, instead of one place. By aggregating these strings to the top of the modules where they were declared, I was able to clean up the code and impact performance. By moving the strings to the top of the modules, it made it easier for me to see the strings and concepts that were in each of the modules. This also has a nice benefit of having a variable name associated with the strings, allowing for easier searching through the project for those concepts. From a performance point of view, this also reduces the number of times that string is declared within the module, using references to the module's instance of the string instead of creating a new instance every time that piece of code is executed. All in all, for both cleanliness and performance reasons, it was a good cleanup to do! What Was My Experience So Far? From a code completion point of view, I felt that I had done well. Sure, there are still 5 scenario tests that are disabled and a number of issues and bugs that I have logged for further research. But other than that, it was finished. I am sure that some of the issues are just clarifications that I need to prove are done right, and that there are some real bugs that I am going to have to fix. I am also sure that those two lists are not going to be static and are going to interact with each other in some way. I am confident that while 1 of the disabled tests will require some interesting thinking on my part (nested image links), the other 4 disabled tests will require some serious changes to how I process lists. But even with these in mind, I also have faith that, due to the large number of scenario tests, that all of these issues will probably only account for less than 2% of all of the scenarios I am ever going to run across, and some of the more obscure scenarios to boot. As such, I believe I can take my time to address them when I have time, and not rush to solve them right away. From a code quality point of view, I was happy to have some time to do some clean up. In their own way, each of the cleanups helped the health of the project. The first cleanup added better organization of the code, the second and third cleanups increased the maintainability of the code, and the fourth cleanup added a bit of a performance improvement. Each of those cleanups moved the project to a happier place, quality-wise, in their own way. Big plus there for me! From a development point of view, getting to this point was indeed a milestone, but it was with mixed feelings that I got here. With respect to the base parser, outside of some issues that I have previously noted, it was complete. Sure, I knew that I needed to fix those issues, but after 713 examples (with 5 examples skipped), it was done. Along the way, I modified the existing 648 examples to add 70 new examples to properly test out the parser. And sometimes it was a hard journey to get those working. Not too complain… but I initially felt that my fun was over. Could I write a parser to properly parse Markdown into intermediate tokens? Yes. Would I be able to finish it while keeping the code coverage and scenario coverage high? Yes. Would I then be able to take those tokens and not only use them to power rules, but to emit the specification's HTML? Yes. For me, determining whether I can complete challenges like that is fun, and having arrived at the answer, I felt that a lot of the project's fun was completed. But in thinking about that some more, I realized that while one part of the fun for the project was over, it was up to me to decide whether I would still consider the project fun. As I largely equate challenges with fun, that means it is up to me to find the challenges left in the project, and to overcome them. Since this is my first real project in Python, at the very least there should be challenges around releasing it. Basically, I just need to take a better look at the project to determine where the challenges are. It just took me thinking about what perspective I wanted to have. What is Next? While I was anxious to get to writing rules, there were some things that needed to be addressed with the parser before I could continue. While I could have left them there and dealt with them later, I wanted to take a good crack at the top items on the list and deal with them now, before the rules were added. So next article: \"Refactoring: The Sequel!\"","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/04/20/markdown-linter-adding-image-links-and-simple-cleanup/","loc":"https://jackdewinter.github.io/2020/04/20/markdown-linter-adding-image-links-and-simple-cleanup/"},{"title":"Markdown Linter - Adding Reference Links","text":"Introduction The end of the main parser is in sight! Two articles ago, the additional of support for inline links was documented, and the last article detailed the addition of support for link reference definitions . In terms of remaining work required to meet the GFM specification, only reference links and image links are left. As image links are just reference links with a slightly different syntax and slightly different rules, it made sense to focus on reference links first. What Is the Audience For This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commit of 26 March 2020 . A Quick Aside I just wanted to take a moment to give some context on why I am providing many examples in this article. If it feels like I am providing a lot of \"extra\" examples, that is because I initially had a few issues with the different link types and their syntax. For whatever reason, the different link types were just not \"clicking\" inside of my head. It was only after I started focusing equally on the rules and the examples, that I was able to match the \"abstract\" text in the specification with the \"concrete\" examples provided. Together they provided the context that I required to truly understand reference links. I hope that by providing good examples in this article, I am providing a similar amount of clarity to help any readers who may encounter similar issues to what I encountered. What Are Shortcut Reference Links? In the last article on link reference definitions , I briefly introduced a concept called a shortcut reference link. As that article was focusing on link reference definitions and not reference links, I introduced it only to show what link reference definitions were capable of doing. A good example of this is the following Markdown where the link reference definition is specified on the first line and the shortcut reference link that utilizes that definition is specified on the third line: [ link ] : / uri \"title\" [ link ] Shortcut reference links like this are the easiest of the three reference link types to understand as everything is kept simple. As with all types of reference links, shortcut reference links are taken care of in the inline processing stage, long after any link reference definitions have been collected. To use a link reference definition, the normalized version of the link label from the shortcut reference link must match a link label from a previously defined link reference definition. 1 To keep things simple for now, I am going to assign words like \"normalized\" to mean a case-insensitive comparison between two strings, with no other modifications. While this is not completely accurate, it will help keep things simple for now, and I promise to revisit it before the end of the article. If the link's link label matches a link reference definition, the shortcut reference link uses the link label as the text within the anchor. In the case of the above example, this produces the following HTML: < p >< a href = \"/uri\" title = \"title\" > link </ a ></ p > When this HTML was generated, the text link was used from the shortcut reference link and the rest of the anchor tag (the text between <a and </a> excluding the text link ) was composed using information from the link reference definition. The benefit of this approach is that the \"marker\" for the link is inline with the rest of the text while the bulkier link data is located elsewhere in the document. This benefit allows the author to better control their own authoring experience by controlling where the link reference definitions occur in their document: after the paragraph containing the link, at the end of the section, or at the end of the document. Without exploring the normalization of link labels (yet!), each link label is parsed to render inline processing for the link label. A good instance of this is example 566 from the GFM specification which adds inline emphasis to the link label: [*foo* bar] [*foo* bar]: /url \"title\" and is rendered as: < p >< a href = \"/url\" title = \"title\" >< em > foo </ em > bar </ a ></ p > The key is that if the normalized link label of both the shortcut reference link and the link reference definition match, everything works fine. If the link label does not match, the text is simply rendered as normal text: < p > [ < em > foo </ em > bar] </ p > Other than that, the only other special thing is that the link label for a shortcut reference link cannot contain another link. This is explicitly stated in the definition of link text which states: Links may not contain other links, at any level of nesting. If multiple otherwise valid link definitions appear nested inside each other, the inner-most definition is used. A good example of this is obtained when the following Markdown text is fed through a GFM compliant parser: [foo[foo]] [foo]: /url \"title\" generating the following HTML: < p > [foo < a href = \"/url\" title = \"title\" > foo </ a > ] </ p > Looking at the HTML, the inner shortcut reference link [foo] was interpreted and the outer link [foo[foo]] was deemed invalid, therefore being rendered normally. Multiple References of a Link Reference Definition In the GFM specification there are instructions and examples on what to do if there are multiple link reference definitions declared with the same normalized link label. However, after a couple of passes through the GFM specification, I was unable to find any guidance on what to do if there are multiple reference links that use a given link reference definition. The closest that the specification gets to this is near the start of the section on full reference links (covered in the next section) which states: followed by a link label that matches a link reference definition elsewhere in the document. Based on this information, along with testing against the CommonMark reference parser, I can safely state that the following example: [ * foo * bar ] \\ [ * foo * bar ] [ * foo * bar ]: / url \"title\" generates the following HTML: < p >< a href = \"/url\" title = \"title\" >< em > foo </ em > bar </ a >< a href = \"/url\" title = \"title\" >< em > foo </ em > bar </ a ></ p > Based on this information and the quote above, I feel that this is a good feature of Markdown. In certain cases where a link is used repeatedly, this behavior can be used to have a single link reference definition to provide the link itself, and multiple reference links pointed to that one definition. In my mind, that is where full reference links come in. What Are Full Reference Links? In John Gruber's original specification , there were no shortcut links and full reference links were referred to as \"reference-style links\". It was only with later parsers that the more compact shortcut reference link was introduced. In my mind, instead of the order in which reference links were historically introduced, I prefer to see the hierarchy of reference links in reverse chronological order, with the shortcut reference links first and the full reference links second. My reasoning for this is as follows. Based purely on efficiency, I typically start with the clearest construct that has the least amount of effort to add. With reference links, my first instinct is to add a shortcut reference link with the link label matching the link reference definition that I need to add to complete it. In 95% of the cases that I come across, I need a single link reference definition and a single reference link, so a shortcut reference link is the best option. For the remaining 5%, I usually have a case where I have multiple reference links referencing a single link reference definition, and I need a reference link that I can use there to good effect. Consider the following example: This is a sentence that refers to [ my link ]. Then , in a separate paragraph , I must still refer to it as [ my link ]. When coming up with this example, I needed to take care to create my example second paragraph in a way that [my link] would fit somewhat fluidly in the sentence. It would be more useful to do the following: This is a sentence that refers to [ my link ]. Then , in a separate paragraph , I can then refer to it as [ my other link ][ my link ]. For me, that is where the benefit of full reference links come in. As I alluded to in the previous example, full reference links are of the form: [text][link] [link]: /url \"title\" Where the first set of square brackets encloses link text and the second set of brackets encloses the link label. Unlike the shortcut reference link, where the link label serves as both the text to match and the text around which to link, a full reference link assigns a block of text to each of those responsibilities. This allows a single link reference definition to be referred to by multiple reference links, each one having tailored link text. The link text is defined in a manner very similar to link labels, including the limitation that link text cannot include other links, demonstrated by the example 541 : [foo *bar [baz][ref]*][ref] [ref]: /uri which generates the following HTML: < p > [foo < em > bar < a href = \"/uri\" > baz </ a ></ em > ] < a href = \"/uri\" > ref </ a ></ p > Similar to how the link-within-a-possible-link example was handled for shortcut reference links, the inner reference link text [baz][ref] is interpreted as a valid link, with the rest of the possible-link's link text being presented as plain text. It is simply an act of serendipity that both the inner link and outer possible-link used the link label [ref] . Because of this act of serendipity, when the inline processor gets to the [ref] text at the end of the line, it is interpreted as a shortcut reference link, in a separate context from the previous link. As such, a second link to the same URI is generated at the end of the HTML paragraph. While I found it easier to visually see the how the above example should be parsed by working through it in my head, it was examples of this flavor that I really struggled with before I combined looking at rule with looking at examples, as detailed above. To be honest, to properly figure these out, I used a pencil and a sheet of paper to visually break down the problem. Only when I had those scribbled notes in front of me did I really get this example. Perhaps it is only me, but it was by literally working through the example and showing my work that I was able to really understand what the parser needed to do. After that, coding the parser to do it was simple. As I have mentioned a few times, figure out whatever works for you, and leverage that. Collapsed Reference Links While the inclusion of collapsed link references into the specification may seem like an unnecessary element, it is an alternative that offers the author leeway on how their Markdown article is constructed. For some authors, the full reference link of [label][label] might be preferred. For other authors, the shortcut reference link of [label] might be preferred. If the author is looking for something in between, the collapsed reference link and it's format of [label][] offers a middle ground. All three of the examples provided in this paragraph are semantically equal and will produce the exacts same HTML. In the end, it is just a matter of preference which of the reference link formats that the author prefers and is comfortable with. Normalizing Link Labels Back in the section on What Are Shortcut Reference Links? , I simplified the term \"normalized\" to mean case-insensitive comparison . The full definition of normalized is a bit more complicated, but not by much. In order of operation: grab the link text in its unprocessed form remove the opening and closing brackets from the link label perform a Unicode case fold (the Unicode equivalent of reducing all letters to lower case) strip leading and trailing whitespace collapse consecutive internal whitespace to a single space Once these steps have been applied to the link label's text, it is that text that is used to determine if it matches an existing link reference definition. In cases where the link label is [foo] or [referenced document] , this process may seem weird or superfluous. But, in the case of example 553 : [bar][foo\\!] [foo!]: /url the parsed inline text is equivalent, but the link is not interpreted as a full reference link. This is because the normalized text for the reference link is foo\\! while the normalized text for the link reference definition is foo! . While both of these strings will be equivalent to foo! after applying inline parsing, their normalized values do not match, and as such, the above example is rendered in HTML as: < p > [bar][foo!] </ p > What Implementation Problems Did I Have? Between the work previously done for inline links and link reference definitions , most of the required foundation work was already in place when I started with reference links. As I required a simple implementation of shortcut reference links to properly test link reference definitions, it was only the introduction of link text for full reference links that required any real block of new code. Even given that solid, tested foundation, there were two issues that gave me troubles as I implemented reference links: using the correct text to determine matching and the order of precedence between different types of reference links. Adding the parsing for shortcut reference links as part of the work for link reference definitions, I naturally did the bare minimum required to get it working. For the link label matching algorithm, it was a simple a == b comparison which worked nicely for all the link reference definition examples except two. To get it working with those two examples, both dealing with case insensitivity, I changed the comparison to a.lower() == b.lower() and then both examples parsed correctly. When I reached the reference link examples that dealt with link label matching, things got a bit more tricky, but not too tricky. Use .casefold() instead of .lower() ? No problem. Stripping various forms of whitespace from the link label? No problem. Using the right text as a basis for those transformations? That took a bit of work. Given that link processing is handled in the inline processing phase, the easiest solution was to add parallel text handling. My thinking was that since the inline processing is constrained to a single continuous grouping of text, I just needed something simple that would only exist for the lifetime of that grouping. To accomplish this, I used the variable current_string_unresolved to keep track of a raw, unresolved version of the string that was being processed in the variable current_string . While that might seem a bit of a kludge 2 , for me it was a simple solution that was contained to the area most affected by the issue. The other option, using an unresolved stream and then resolving it later, seemed to have too many issues to deal with in a manner that I was sure was going to cover all the edge cases. I know this because I tried that first (and second and third), before sitting back and rethinking what the best approach would be from a high level. After three attempts with the unresolved stream and no success, the kludge solution worked on the first try with no issues. Guess it really was not a kludge! Having found a decent solution for using the right text to match against, the only issue I had left to deal with was in dealing with the order of precedence of reference links with themselves and other elements. Detailed in the examples between example 572 and example 579 , these examples give very specific guidance on the precedence to use for each combination. While the examples provided good guidance, the implementation was not always so easy to get right. While getting the order in which to check for the various types of reference links took a bit, it was complicated by the determination of whether a given reference link referenced a valid link reference definition. Out of the 4 days it took for me to complete reference links, one of those days was spent just going over all of the combinations, making sure that the specification detailed them (it did!), and then checking and rechecking each modification that made another of the above examples work. It was only then that I staged my changes and moved on to the next example. In the end, it was very tedious work, but it was worthwhile because it worked. What Was My Experience So Far? For the most part, the implementation of the reference links was a good experience, combining a bit of \"reuse foundation\" work with some \"how do I get this to work properly\" challenges. There was just one dark sport on the implementation of the feature. I have already mentioned my issues with understanding the link specification in the above sections, but I feel the topic is important enough topic to warrant more discussion. Unlike before where I was skipping parts of the specification, this challenge was a genuine case of me reading the specification and not getting a good understanding of it. Even with the provided examples, there were still times where I was unable to comprehend what the specification was doing. It was only after going \"old school\" and getting out a pencil and some paper did I work through it. At one point, I remember thinking to myself: Writing parsers for 30 years and you still need a pencil and paper? It was not one of the brightest moments of the project, but it did happen. It was after what I think was the 7th or 8th time of me trying to understand the preventing the link within a link logic detailed in the section on reference links . Honestly, I am guessing it was the 7th or 8th time, I lost count of the number of attempts. For whatever reason, it just was not clicking for me. It is times like those that I like to break a problem down to smaller components, what I refer to the \"boulders to pebbles\" approach. And for some reason, I needed to break those pebbles down even smaller, and I was very hard on myself for it. No matter who you are, you are going to have good days, bad days, and a lot of in between days. The more that you take care of yourself, the better your chance of being on the positive end of that spectrum. I did not need a pencil and paper because I wasn't capable of figuring out the problem myself, I needed them as a tool to help me figure it out myself on that day. I now look back at the problem and my scribbles and have the mental capacity to understand it was just a bad day. And even if I am being more charitable about the day than it really was. So what? I used the tools that I needed to in order to solve the problem I faced. Simple as that. I started this project with a need and a desire to learn. I completed this feature learning that I still have what it takes to solve the issues that I need to. I also learned that I need to focus a bit more on taking better care of myself and nudging myself towards the positive end of that spectrum. Guess what? I stumble, I learn, and I get up and try and not do the same thing again. Well… not too often at least. What is Next? While I know there is some \"optional\" stuff I need to add to the parser before I can use it on my own website, there is only one more feature that I need to complete before the parser is complete and GFM compliant. With a bit of a mental drum roll… image links are next! As link reference definitions are parsed with the leaf blocks, and reference links are parsed later with inline processing, the term \"previously defined\" refers to any definition in the Markdown document that was parsed, and not \"previously defined\" with respect to the relative locations of the reference links and the definition within the Markdown document. ↩ According to Merriam-Webster: \"a haphazard or makeshift solution to a problem and especially to a computer or programming problem\" ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/04/13/markdown-linter-adding-reference-links/","loc":"https://jackdewinter.github.io/2020/04/13/markdown-linter-adding-reference-links/"},{"title":"Markdown Linter - Adding Link Reference Definitions","text":"Introduction As detailed in the last article , the remaining work left on the main parser consists of inline links, link reference definitions, reference links, and image links. Inline links were covered in the last article. While I could try and come up with some grand reason for doing the link reference definitions next, the truth is simple: they were next on the list. What Is the Audience For This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commit of 22 March 2020 . What Are Link Reference Definitions? In the last article , I introduced inline links and how they present the text to appear in the link (link label), the link itself (link destination), and an optional title for that link (link title). Link reference definitions are a related concept in that they take the link destination and link title parts of the link and store them in a map, to be used at a different time. Basically, where an inline link uses the form: [ link ] ( / uri \"title\" ) a link definition uses the form: [ link ] : / uri \"title\" The main difference between the two elements is that a link reference definition does not add any elements into the HTML document by itself. To utilize the link reference definition, a reference link must be added to the document that has a normalized 1 link label that matches the normalized 1 link label from a link reference definition present elsewhere in the same Markdown document. For example, using the simplest form of reference links, a shortcut reference link , the following Markdown: [ link ] : / uri \"title\" [ link ] creates a link reference definition and uses it, generating the following HTML: < p >< a href = \"/uri\" title = \"title\" > link </ a ></ p > While the normalization rules 1 are somewhat complex, in most cases it just means using the same link label in both the reference link and the link reference definition. Unless you happen to get into the more interesting aspects of the normalization rules, both reference links and link reference definitions are easy to use, by design. Why Use Link Reference Definitions? In the last article , I mentioned that I use inline links exclusively. Now that I have introduced link resource definitions and reference links, I can provide more context on the difference between them. Using a simple lorem ipsum generator , I came up with the following two examples. This first example contains an inline link: Nam efficitur , turpis ac vestibulum imperdiet , nulla mi mollis erat , nec efficitur nunc lorem rutrum metus . Vestibulum dictum lacinia lacus , at ornare quam consequat ultrices . Nam quam leo , aliquet in luctus in , [ porttitor non quam ]( https : // lipsum . com / ). Donec tincidunt augue nisi , sed pellentesque nisl porttitor vel . and this second example contains a reference link, specifically a shortcut reference link: Nam efficitur , turpis ac vestibulum imperdiet , nulla mi mollis erat , nec efficitur nunc lorem rutrum metus . Vestibulum dictum lacinia lacus , at ornare quam consequat ultrices . Nam quam leo , aliquet in luctus in , [ porttitor non quam ]. Donec tincidunt augue nisi , sed pellentesque nisl porttitor vel . [ porttitor non quam ]: https : // lipsum . com / From an HTML output point of view, both examples generate the exact same output. In both cases, the examples are presented as I would normally include them in my articles, folding each line after the 90 characters that I keep my Markdown editor set to. Applying my own stylistics, when I add an inline link I ensure that it begins at the start of a new line to ensure that I can clearly identify it as a link. In trying an equivalent example with a reference link instead of an inline link, the style that I chose was to place the link label delimiters ( [ and ] ) around the link label itself with no change in formatting, with the link reference definition following later in the document. While link reference definitions can precede or follow any reference links that use them, to me this seemed like the right way to do it. From my point of view, I find that the inline reference provides better readability for me and how I read my articles when authoring them. Perhaps it is through having authored and proofed many articles in this format, but to me, not having the link information right in the paragraph feels like a grammatical or spelling error. And while I did not really think about it before, when proofing the Markdown version of my articles, I don't really \"see\" the link destination and link title until I slow down on my final passes. Regardless of the reasoning behind it, I just find it works better for me to read the Markdown version of articles with inline links. Please note that this view is solely my own. When performing a similar evaluation for yourself or your organization, it is important to consider your own criteria when determining which options, reference links or inline links, is best for you. Hitting Implementation Issues Having implemented inline links as documented in the last article , I started working on the link reference definitions thinking they would be easy. In the case of the first example, example 161 , it was in fact pretty easy. [ foo ] : / url \"title\" [ foo ] A complete link label, followed by both a link destination and a link title. All on one line. Then add a simple shortcut reference link to reference the previously added link reference definition. Nice. Compact. Complete. It was when I moved on to example 162 that all of the issues started: [ foo ] : / url 'the title' [ foo ] The first important issue to understand is that link reference definitions are processed as leaf nodes instead of inline text. To keep the memory footprint of the parser low, I made an early design decision to only get the text from the input source one line at a time. 2 While the proper implementation of that design is still in the future, that design limits the main parsing function of the parser to only knowing about the current line, with no built-in capacity for look-ahead or look-behind. Remember this issue, as I will get back to it in just a minute. The second important issue is that unlike all previous leaf node elements, it can take multiple lines to determine if the current link reference definition element is valid. In example 162, as provided above, it isn't until the end of line following the second ' character on line 3 that the link reference definition is determined to be valid. To better highlight this problem, consider example 165 which provides for an exaggerated example of this: [ foo ] : / url ' title line1 line2 ' [ foo ] While the link reference definition as stated in example 165 is valid, it isn't until the 5th line, where the second ' character followed by the end of the line closes off the link title, that the entire link reference definition is deemed valid. By making one small change to the previous example, removing that previously mentioned 5th line, that entire link reference definition is rendered invalid, as follows: [ foo ] : / url ' title line1 line2 [ foo ] After that one small change, instead of a link reference definition followed by a valid link reference, both elements are now just interpreted as plain text. Remember a couple of paragraphs ago when I mentioned \"remember this issue\" when talking about processing link reference definitions as leaf nodes? Here is the payoff. Because of my design choice to process the Markdown document one line at a time, I needed to add extra logic to the parser to allow me to \"rewind\" those lines. In the case of the modified example 165, the entire link reference definition is rendered invalid, and the parser must rewind to the start of the link reference definition. However, when it starts parsing the lines again, care must be taken to inform the parser that it cannot consider the newly rewound lines to be eligible for a valid link reference definition. Painful, but not too painful. Following along from that change, if we do a similar change to example 162 by removing the final ' character from the definition, it poses a different problem. While the removal of the 5th line of example 165 invalidates the entire link reference definition, removing the final ' character from example 162 only invalidates the link title, leaving the rest of the link definition valid. To deal with this, I needed to not only have logic to go backwards towards the start of the link reference definition, but to stop that rewinding if whatever part of the definition that was not rewound turned out to be a valid definition. While the rewinding was a headache and somewhat obvious 3 , aborting the rewinding if a valid link reference definition \"fragment\" was found was not obvious to me at all. That code was painful. Honestly speaking, that logic alone took about half of the 5 days required to get the multiple-line aspect of link reference definition parsing properly. A decent amount of that time was taken up with rewriting the logic for extracting the link label, link definition, and link title to handle being straddled over multiple lines. But the real \"fun\" was making sure that rewinding the lines would properly rewind the token stream and token stack in the parser. While it took a lot of work to get there, it was personally fulfilling when I got it right. It wasn't an easy issue to solve and coming up with a clean solution wasn't easy. As a bonus, my dog stopped looking at me with a funny expression. It was pointed out to me that he did that when facepalmed myself whenever I got the parsing wrong. Personally, I considered that a 2-in-1 benefit! What Was My Experience So Far? There have only been a few issues that have taxed my experience to solve them and implementing link reference definitions was one of them. That is both a good thing and a bad thing. On the good side, due to my experience and stubbornness, I was able to modify my implementation to deal with the issue without having to change my design decision. On the bad side, there is only one test with a link reference definition being within a container block, and that leaves a lot of questions about how to handle failures within those containers. While I noted the later down for subsequent testing, it still leaves me feeling a bit uneasy that I had to modify the parser to handle that. Considering where I am with the parser, I am glad that I hit that issue now, and not after I finished the parser. While it was painful to go through, it did reinforce a few things about the design of the parser so far. The first thing that was reinforced was that, with only one exception, my early decision to do line-by-line parsing is viable. While there may be parser extensions that change that, the number of exceptions in the link reference definition category should be low. If I can then combine that with some Python generator logic, hopefully I can keep the memory profile of the parser low. The second thing that was reinforced was that the general structure of the parser was properly designed and implemented. While I still need to take some time and refactor individual groups of functions into their own modules, the actual function themselves are durable. Except for the link helper functions and a few core functions, I did not have to change any other functionality to handle the failure rewind scenario. In those few functions I changed, it was either to specifically handle that scenario or to pass information back to the main parser function about how to handle the rewind. Compared to past situations I have encountered; I consider it a benefit that I only had to change the small amount of code that I did. Finally, my decision to use inline links over reference links was reinforced. In more closely examining the difference between the two types of links, I believe I better understand and appreciate both types of links. While I agree that writing Markdown with reference links will more closely approximate how the paragraphs will look when completed 4 , the absence of the link destination and link title reduce my comprehension of the paragraph as a whole. I find myself making the same decision as before, but now I believe I can more clearly communicate the options for both types of links and why I chose inline links over reference links. What is Next? The list of what is left to complete in the parser is getting shorter and shorter. With inline links and link reference definitions tested and completed, only the full implementation of reference links and image links remain. While a lot of foundation work has already been set up for these features, I am wary of declaring that it will be an easy job from here on it. But with only 2 features left, I know the start of the real linting work in coming soon! Long winded version: To normalize a label, strip off the opening and closing brackets, perform the Unicode case fold, strip leading and trailing whitespace and collapse consecutive internal whitespace to a single space. Shorter version: reduce or eliminate whitespace and do a case-insensitive comparison. ↩ ↩ ↩ To keep the memory footprint of a parser low, the parser design should try and only keep the information that is required for parsing in memory. By parsing the Markdown input line by line, I do not have to worry about having to load the entire Markdown input into memory before I can begin parsing it. ↩ Except for a Lisp parser that I once wrote, I cannot think of a single parser where I didn't have to rewind at least one complex entity out of the 25+ parsers that I have written. ↩ As per John Gruber's original intentions. Go to this part of his original Markdown article and go to the paragraph preceding the start of the emphasis section. For more complete information on why John felt that reference links were better, look further up in that section for more details. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/04/06/markdown-linter-adding-link-reference-definitions/","loc":"https://jackdewinter.github.io/2020/04/06/markdown-linter-adding-link-reference-definitions/"},{"title":"What is the Audience for my Blog?","text":"Introduction From my experience, the two most critical components for learning are: the focus and desire to learn; and the ability to seek out, handle, and then learn from constructive criticism obtained from people that are more experienced. It is primarily the people that I have seen employ both of those components in unison that I have seen learn and excel in their fields. From the day last summer when I started writing this blog until today, I believe I have proven to myself that I have the focus to continue writing and the desire to keep on writing. It was when I reached out to some friends and colleagues in the writing field for constructive criticism that they all asked the same question: \"What is the audience for your articles?\" Thinking About That Question Wow. It was a pretty obvious question, but at the same time, it was one that I just assumed I knew the answer to. As I started to write this article, I just assumed that the words would just flow out of my fingers… Until they didn't. Instead, I sat at the keyboard for a long while, with a flood of thoughts going through my head. I was pretty sure I was not doing the blog for attention, but what was it really that drove me to take the day or two out of my week to write these blog posts? To get my thoughts out of my head, I started writing different parts of different paragraphs that were prototypes for this article. In the end, each one seemed like it was scratching at the service, not really answering the question. To be clear, none of those paragraphs made it to this final version of the article. In any form. Not even remotely. I knew I had to come up with an answer, but I was hitting a wall. And try as I might, I was not making any progress. It was frustrating… very frustrating. It was while doing something totally unrelated that I remembered a conversation with a colleague about elevator pitches. The general idea behind elevator pitches is that in the 30-45 seconds that you have with someone in an elevator, you can pitch them an idea that is comprised of the 2-3 sentences that are the distillation of the most important points to get across about your idea. If you want to successfully get the other person's attention, you need to \"hook\" them before those elevator doors open and their attention is no longer focused on you, but what is outside of the elevator doors. To me, this concept seems to contain some basic truths. There are a fair number of talented people that can briefly capture your attention by coming up with flowery words about their ideas, but those ideas usually end up just being smoke and mirrors. In response, when people start seeing those kinds of words and sentences coming their way, their experience trains them to disregard a lot of what you are saying. Based on personal experience, this decision is made within the first 3-5 sentences that you speak to the other person. Therefore, if you honestly have something meaningful to say, you should be able to describe it in a short and powerful burst that is simple and straightforward, containing the most important parts of what you want to say. So, how could I apply that concept to the question of audience and how I would answer it? What Is My Goal? It took my quite the while. But ultimately, for me and my blog, it boils down to this: I want my blog to inspire people and help them learn, like people have inspired and helped me in the past. It may seem simple and altruistic, but that is the heart of what I want to accomplish. Not a lot of flowery words. Not a lot of meaningless sentences. Just a simple statement of what I want to accomplish with my blog. It was a good start on what I wanted to accomplish, but I still needed more. How do I accomplish that goal? Having a good statement of my goal, I needed to figure out how my blog was going to accomplish that goal, thereby further defining the audience. Based on my stated goal, I started thinking about the blogs that I usually read. Writing some of those blogs down, I found there were two categories that emerged: ones that I use as a point-in-time resource and ones that I read and learn from. From my point of view, a lot of the blogs and sources out there present the information necessary to solve an immediate problem, but not a lot them offer insight as to how the authors arrived at that solution or what the alternatives are. To be clear, that isn't always a problem. I use sites like Stack Overflow all of the time to get help with a particular situation. However, in terms of being a site that I learn from, Stack Overflow will rarely be one of them. In contrast, the blogs and sites that I keep on going back to are ones that offer that additional insight and help me learn through that insight. They provide something to me above and beyond the normal solutions to problems that are documented on other sites. But what exactly was it about those blogs that helped me learn? What were their hooks? And more importantly, what was the way in which I was going to do this for people reading my blog? Thinking about that information for a long while, and then distilling it down to its essence, I expanded on my stated goal from above to include be a more complete answer. What Is the Audience For My Blog? I want my blog to inspire people and help them learn, like people have inspired and helped me in the past. For technical articles, I feel that I can best do that by focusing more on the why and how of the decisions leading up to the solutions rather than the what of the solutions themselves. For other articles, I feel that I can best do that by being an honest and believable storyteller, helping people to understand issues and situations as I see them. Wrap Up It was an interesting process for me to try and summarize a whole lot of thoughts about why I write my blog and reduce them down to 3 sentences. The above lines took a number of days to arrive at, with lots of writing, even more editing of those words, and even more distillation of those edited words until I got down to the plain and simple sentences that I arrived at. I hope those two sentences help explain what I see as the audience for my blog.","tags":"Website","url":"https://jackdewinter.github.io/2020/04/05/what-is-the-audience-for-my-blog/","loc":"https://jackdewinter.github.io/2020/04/05/what-is-the-audience-for-my-blog/"},{"title":"Markdown Linter - Adding Inline Links","text":"Introduction Having just implemented the Inline Emphasis feature as documented in my last article, I was eager to move forward with the implementation of the links feature. As the group of features under links were the only ones separating me from the completion of the project's parser, I was happy when I noticed that I was starting to think \"when the parser is done\" instead of \"if the parser is EVER done\". I have been working on this project for a while and it was nice to know that in my mind, I could \"see the light\" with respect to this project. In implementing the algorithm outlined in the section Phase 2: inline structure , I chose to implement the emphasis feature first, leaving the implementation of links until the base algorithm and emphasis feature were implemented and tested. Seeing as both of those were accomplished, I felt that it was the right time to apply my success in implementing the emphasis feature to the link feature. I knew that the work on the delimiter stack would easily carry over, but I was eager to see if the implementation of the next part of links would be as easy as the implementation of emphasis. What Is the Audience For This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commit from 17 March 2020 . Why Inline Links? Taking a quick peek ahead, I observed that the link feature in the specification is broken down into four groups: inline links , link reference definitions , reference links , and image links . Based on a quick reading of each section, it was obvious to me that for reference links to work properly, link reference definitions would be needed. Image links are variations on the inline link type and reference link type, the only difference being a different start delimiter. As such, I did not see any benefit to implementing image links before those other two link types are completed. This leaves inline links. Inline links are self-contained, allowing them to be implemented separately from the other three groups. Being somewhat impatient to get the parser done, I decided to go for the sub-feature that was more immediately available: inline links. I Use Inline Links (Almost) Exclusively When writing my articles and documentation, I personally find it more readable to include any links in my documents as a complete unit. This means that when I add a link, I will typically add it in the form shown in the GFM specification at example 494 : [ link ] ( / uri ) As a concrete example of this, the above link to example 494 that precedes the sample link format was created with the following Markdown: [ example 494 ]( https : // github . github . com / gfm /# example - 494 ) While the form can be augmented as such: [ link ] ( / uri \"title\" ) to provide a title, I cannot remember a case where I have used that form. While I do not have a strong reason for or against this format, I believe that I just have not encountered a case where I believe that a title for the link was either desired or required. On the other hand, I have used the following form a few times before: [ link ] ( </ my uri > ) as an alternative to the form: [ link ] ( my uri ) While my use of the angle bracket form is rare, it was useful in a couple of cases where I needed to provide a space character as part of an URI. While different Markdown-to-HTML processors will handle the space character differently, I just wanted something that read well and was mostly universal. What is an Inline Link? For a well-documented answer to this question, please look at the inline link section of the GFM for a word-for-word answer, complete with helpful examples. My own summarization of this section is as follows: an inline link occurs on a single line and comprised of the link text, a left parenthesis, an optional link destination followed by an optional whitespace and an optional link title, and a right parenthesis if in doubt about any punctuation characters in the below constructs, backslash escape them the link text is any text that appears between the opening square brackets ( [ ) and the closing square brackets ( ] ) the link destination is any non-space, non-control characters, with special rules for the < , ( , and ) characters the link title, if included, is contained within a single-quoted string ( ' ), double-quoted string ( \" ), or a parenthesized string ( ( and ) ) After 5 revisions to try and keep my answer \"minimal\", that is it! While I could leave it in a more complicated state, that summary is what I believe I have it broken down to in my head. For me, the first two points are the pivotal ones, setting up the link component order and exclusions needed to create valid inline links. The last three rules are just simplifications of what is needed to represent each of the three components, rounding out the definition for inline links. Keeping to those rules, when I am picturing Markdown links in my head, I usually think of these two forms: [ link ] ( / uri \"title\" ) and: [ link ] ( / uri ) Between the 5 rules stated above and these two examples, I believe I am keeping it simple and minimal, ensuring my consistent use of links. Let me dive into those a bit more. In terms of the order of components, the link text and both outer parentheses are required for the inline links, but both the link destination and link title are optional. However, due to how they are documented, if a link title is desired, a link destination must precede it. Basically, the component order is always: the text that shows up inside of the link's anchor, the link URI itself, and a title to use when the link is traversed. Once again, I keep it simple to make it easy to use. For remembering when to use punctuation characters and backslashes to escape them, I once again try to keep it simple. The second rule is the embodiment of that. My definition of \"if in doubt\", as stated in the second rule, is that if I have to think \"is this punctuation part of the link or not?\", I have doubt. Therefore, if I am authoring a link and am not sure if I should backslash escape punctuation within a link, I escape it. Those first two rules are specifically tailored for me and to how I write my articles. My primary goal in coming up with those rules is to allow me to author rough drafts of my articles as I go, links included. During the rough draft phase, as efficiently as possible, I need to leave enough information in the added links to allow me to finish each link in subsequent passes through the document. While the \"as I go\" element is not present in the subsequent passes when I clean up the link, I do need to make sure that I keep those passes as efficient as possible. By keeping those rules simple, I reduce the amount of friction incurred by adding links to the documents, therefore keeping those passes efficient. An additional benefit to using those rules is the simplification of the specification's acceptable rules. Specifically, the wording of my two first points helps me avoid a lot of the weird cases included in the 41 test cases for inline links. The two most frequent reasons for the examples containing weird cases are the inclusion of newline characters and the inclusion of extra punctuation characters. The \"single line\" part of the first rule helps me avoid any of those newline cases, and the \"if in doubt\" part of the second rule helps me avoid any of the cases with extra punctuation. My conservative estimate is that by adding those extra words to my personal rules, I was able to reduce the number of \"applicable\" cases for links that I am authoring in half, if not a bit more. While that might sound like a weird statement to make, for me it means that I when am authoring a document and adding a link, I can keep my focus on the what I am adding to the document, and not focus on trying to remember all of the weird cases for links and how to avoid them. For me, that is a plus. Implementing the Algorithm Having constructed and tested the delimiter stack and emphasis parts of the algorithm, as documented in the last article on Inline Emphasis , it was time to implement the look for link or image part of that algorithm. Once again, in an effort to keep things simple, I added very minimal support for image links (as detailed in the algorithm), but that support also added assert False statements to ensure that any scenarios with images were clearly identifiable and not executable. This helped to prevent me from accidentally working on testing image link features before adding the real support for them in a subsequent feature. While the link part of algorithm does not have as many special cases as the 17 rules for emphasis, there are a decent number of elements to keep track of when implementing the links portion of the algorithm. I found that by following the examples and the algorithm as stated, I was able to quickly isolate the changes for each example. This isolation allowed me to to cleanly implement each small change with a clear idea of what was needed to be accomplished. As I am human, there were a few issues I initially had in following various portions of the algorithm. In all those cases, a quick re-read of the section helped me get the proper information I needed to implement that portion of the links. Out of the 41 total examples for inline links, only the first 6 are ones that I would consider normal examples, the remaining 35 testing special cases and boundary conditions. As such, I started with the first example, example 493 , and added the code necessary to do a simple parsing of each of the components of that example: link text, link destination, link title, link format separators and link whitespace. Then I simply started working my way down the list of examples, refining each implementation with each example that I worked on. With each example that I cleared; the implementation visibly got closer to its final implementation. I am not too proud to admit that on my first reading of a lot of the examples, I questioned what their worth to the feature was. However, as I worked down the list of examples, the questioning changed into enjoyment. Each new example added a small layer of complexity to the implementation, like a piece of a puzzle cleanly fitting into place. Even concepts that I worried about, like new lines and backslashes, were given enough examples to clearly and properly demonstrate how they worked. There were a couple of times where I questioned whether an example was really needed, but those instances were few and far between. Where I had Problems The addition of inline links to the project went without too many issues. The most prominent of these were the proper encoding of special characters in the links and the interactions between links and emphasis. In terms of the special characters, the main problem that I had was in selecting an interpretation of the term \"special characters\" that was consistent with the GFM specification, and hence the CommonMark implementation. The first group of characters, the ones to replace with named HTML entity names is small, so examples such as example 514 and example 517 were easy to implement and get right on the first try. From an HTML author's point of view, it was obvious that the following Markdown from example 517: [ link ] ( / url 'title \"and\" title' ) should produce the following HTML: < p >< a href = \"/url\" title = \"title &quot;and&quot; title\" > link </ a ></ p > Taking that a bit further, the next groups was still simple. Any characters that are represented by more than 7 bits needed Unicode encoding, which I also thought was obvious. Once again, from an HTML author's point of view, the following Markdown from example 511 : [ link ] ( foo % 20 b & auml ;) would obviously translate into the following HTML: <p><a href= \"foo%20b%C3%A4\" > link </a></p> Verifying this was correct was easy. I had to look at the project's entities.json file for the information on the &auml; symbol. From there I quickly verified that it is represented by the Unicode sequence \\u00E4 , which becomes the sequence %C3%A4 when encoded with utf-8 . This was all done with Python's urllib module, specifically with the urllib.parse.quote function, and it got all of these right on the first try. The issue came to the proper encoding of characters with an ordinal value below 127 (or 7F hex) that were not control characters and not alphanumeric characters. By default, the only character that is considered safe according to the urllib.parse.quote function is the / character. When the parser emitted the HTML for those many of those remaining characters, it replaced the actual character with the percent-form of that character. While both approaches are usually syntactically equivalent, the comparison failed because the URI was not the same as the example's output. It was only over the course of a couple examples that the punctuation character safe list from above was constructed. The second group of issues came around the interaction between normal inline processing and the processing used for links. In cases such as example : [ link * foo ** bar ** `#`* ]( / uri ) all the inline processing occurs within the link text section and is pretty unambiguous to what the intent is. However, in the cases of example 529 to example 534 , it is not as clear as to what the intent of the author was. In the case of example 531 : [ foo * bar ]( baz * ) it is not clear to me at all what the author would have intended with this Markdown. Can you figure it out? The good news is that the GFM specification is clear on the precedence of each of the inline processes, but even still, it took me a bit to get that precedence properly implemented. What Was My Experience So Far? Except for adding a metadata feature and a table of contents feature, both which are unofficial extensions, every other feature that I normally use when writing articles is now implemented and tested. Having hit that personal milestone, it is a good feeling knowing that I am THAT close to being able to run PyMarkdown against my own articles to lint them. The only other core feature that I sometimes use is the image link feature, and I know that is just a couple of features away. In terms of the feature implementation and testing for this feature, the issues I had were either caused by my misreading of the specification or caused by trying to skip forward in the examples, and not following the example order set out in the GFM specification. As I have mentioned numerous times in this section of past articles, the GFM specification is well thought out and battle tested from many implementations. While I do recognize that and heap praise on them, at the same time I seem to think that I either know better than they do or know where they are going, hence my skips forward. I need to stop that, as it seems to be keep getting me in trouble. Except for a couple of \"didn't I learn this already?\" moments, things went fine. My guess is that I will learn to trust this specification properly just before I finish the last example. Go figure. In all seriousness, I am going to try and put more effort into following the specification and its examples in their proper order for the next feature, and try and get it locked in. If I had a great specification, I need to learn to lean into it for support, and not fight against it. At the start of this article, I expressed an interest to see if the implementation of inline links would be as easy as the implementation for emphasis. While there are differences, the 17 rules around emphasis and the looking for a complete inline link after having found the link label itself, I am very convinced that the effort required to implement each of them was pretty similar. As a bonus, having taken a quick look at all of the features in the link group, I am pretty sure that the work for inline links will be heavily leveraged to complete the link group itself. What is Next? Emphasis and link support and delimiter stack? Done and tested. Inline support for links? Done and tested. Before going on to reference links and image links, it only made sense to do link reference definitions next, so that is where I implemented next, though not without a lot of difficulties.","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/03/30/markdown-linter-adding-inline-links/","loc":"https://jackdewinter.github.io/2020/03/30/markdown-linter-adding-inline-links/"},{"title":"Markdown Linter - Adding Inline Emphasis","text":"Introduction I have been reading and implementing specifications for a long time. I truly believed that I had seen with and dealt with every situation presented in any specification. I was therefore surprised when I started reading the GFM specification and I noticed an entry in the table of contents labelled Phase 2: inline structure . Thinking \"if it is in the table of contents as a separate entry, it must be important\", I started reading that section, waiting for some big revelation to appear. That Phase 2 section starts with an example which is an extension of the example used in the preceding section in the document. After a bit of comparison with the previous example, it should be obvious that the string \\n was changed to softbreak and the string \"Qui *quodsi iracundia*\" was changed to str \"Qui \" emph str \"quodsi iracundia\" . Going back to my article on Autolinks, Raw HTML, and Line Breaks , the translation from \\n to the soft line break (or soft-break) is easy to see, the inline parser code to perform that translation having just been implemented. And looking at the provided example, emphasis looked like it was going to be easy. So why include implementation notes on it if it was so easy? On the surface, the translation of emphasis markers to emphasis would seem to be easy to understand, if not for the start of the next section of Phase 2: By far the trickiest part of inline parsing is handling emphasis, strong emphasis, links, and images. This is done using the following algorithm. I was kind of skeptical, but I continued reading… and reading… and reading. After a couple of thorough reads of the section in the GFM specification on Emphasis and strong emphasis , I started to understand why emphasis is not as easy as it seems. At its root it is pretty simple: use one * for emphasis and ** for strong emphasis. No problems there. If you need a subtle change in the emphasis rules, primarily around intra-word usage and use around punctuation, use _ instead of * . Okay…why do I need so many rules? Isn't it as easy as this ? Isn ' t it * as easy * as ** this **? It took a bit more reading, but for me, it was some questions from the preface of the specification that made me understand. The big revelation finally hit me in the form of those questions and the ambiguity that they raised. How are the following cases from the specification of strong emphasis interpreted? ***strong emph*** ***strong** in emph* ***emph* in strong** **in strong *emph*** *in emph **strong*** How about the following cases of nested emphasis? *emph *with emph* in it* **strong **with strong** in it** And finally, how about intra-word emphasis? internal emphasis: foo*bar*baz no emphasis: foo_bar_baz For most of those examples, I could come up with at least 2-3 different ways that they could be parsed. As such, it is probably a good idea that there are 17 rules to help with emphasis. But before I could work on the implementation, I needed to do some more research. What Is the Audience For This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 10 March 2020 and 13 March 2020 . Emphasis is Hard and not always Consistent For any readers that have been following this series, it might be apparent that I have veered away from my usual habits. Usually I label all of my Markdown code blocks properly to allow my static site generator to take care of the proper coloring. However, for the above examples, I just marked them as normal text blocks without any language specified. This was not a mistake; it was a conscious decision. When I started entering the fenced code block for the text, everything looked normal. However, once I added the Markdown language tag at the start of the block, that all changed. What I saw in my VSCode editor was the following: Publishing the page and looking there, I saw that same text rendered as: And finally, running it through my favorite GFM test link, the following was generated: strong emph strong in emph emph in strong in strong emph in emph strong Three different platforms and three different ways to interpret the emphasis. To me, it was then that it was apparent why a consistent way to implement emphasis was needed in order to produce consistent results. Utilizing the Wisdom of Others Instead of trying to figure everything out myself, tempting as that can be sometimes, I decided to implement the algorithm as stated in the GFM specification with one small change: no links for now. To accomplish this, I kept to the algorithm's recipe as described, but I left the implementation of the look for link or image section for later. My plan was to focus on the emphasis itself before moving on to links, which I still believe was a very good decision on my part. In looking at the algorithm through a bit of a sideways lens, I observed that there were two main tasks that the algorithm accomplishes: building the delimiter stack and resolving that same delimiter stack. Without taking a two-phase approach to the algorithm, I determined that I would be multitasking two separate objectives, and not doing either of them very well. I just felt strongly that it was better to split the task in two and focus on each one until it's completion. In the end, I was going to finish both part of the algorithm, so if both tasks were done, emphasis would be implemented. The first task: building the delimiter stack. Building the Delimiter Stack Taking a high-level look at the algorithm, I came up with the following two observations. In cases where a delimiter is not used in a special way, it needs to be represented by a simple text node. Where it is used in a special way, it may need to be removed and properly replaced with the tokens that the delimiters represent. The \"may\" part of that last statement is there as, after looking at some of those examples, it may be possible to have a case where the parser only uses some of the emphasis delimiters, but not all of them. Once such example is the string this **is* my text . To accomplish this, I created a new class SpecialTextMarkdownToken that is a child of the TextMarkdownToken class. In cases where the delimiter is not completely consumed, this allows the new token to be easily consumed by existing code, without any modifications needed. Supporting the additional parsing requirements, the new SpecialTextMarkdownToken tokens adds the repeat_count , active , preceding_two , and following_two fields, enabling the algorithm to be implemented properly. Finally, but most importantly, to ensure the algorithm works properly, that new token is added to the delimiter stack. To properly test that I had produced the correct tokens and delimiter stack, I started by enabling one of the emphasis tests and examining the resulting tokens. I compared those tokens against the tokens that were there before, looking for changes that I expected to be induced by the presence of the emphasis delimiters. As I was only building the delimiter stack at this point in the process, this was easy to verify. A solid example of this is the emphasis test for example 364 , which is the example that I selected as my initial test. That example contains the string foo*bar* to test against and is a pretty easy example. Before the delimiter stack was built, I verified that the string was represented by a single TextMarkdownToken token containing the string foo*bar* . After the delimiter stack was built, that same string was represented by 4 tokens: a TextMarkdownToken token with the string foo a SpecialTextMarkdownToken token with the string * a TextMarkdownToken token with the string bar a SpecialTextMarkdownToken token with the string * In addition, the delimiter stack contained both SpecialTextMarkdownToken tokens in the order that they appear. At this point in the process, that was the expected token output, and I was happy to see it work so well. As one good test was not solid proof to me, I continued to make sure that I had decent results after building the delimiter stack by repeating this examination with 5-6 other tests. While there were a couple of small issues along the way, I was able to quickly get to the part where I had a high degree of confidence that the delimiter stack was correct. Now it was on to the resolution of that stack. Resolving the Delimiter Stack Skipping past the look for link or image section of the GFM specification, I sat down with my headphones on and poured over the section on process emphasis a couple of times, taking notes as I went. I then went back to the emphasis scetion of the GFM and started pairing each of the 17 rules to each note that I made and was happy to find a 1-to-1 mapping between the two lists. And yes, that is right… 17 rules. It did take a while, but I validated each step in the specification against each of the rules, making detailed notes as I went. While it was slow to emerge, by the time I reached the last rule I had a plan to follow. To implement each rule, I would implement the tests in order of the rules that they support, with a slightly irregular start order. Basically, instead of starting with a \"1 2 3 4\" order, I started with a \"3 1 4 2\" order. The reason for this weird ordering is because the algorithm starts by identifying eligible closing emphasis, then trying to locate a matching start emphasis. To accommodate that, I decided to start with the close and start pair for the normal emphasis ( * character) following that same pattern with the strong emphasis ( _ character). While I was thankful for the examples in the GFM specification before implementing emphasis, having the emphasis examples, rules, and examples-by-rule-groups made me even more thankful. Starting with rules 3 and 1, I was able to get solid code written to implement what was needed to support those rules. The next two rules, rules 4 and 2, were added using the first two rules as a building block. For the remaining 13 rules, I just felt that each rule and the examples to support it just naturally flowed, each one adding another small piece of the puzzle. While it did take some time to get all 17 rules implemented and the tests for all 130 examples passing, it all felt like just another step. It was a nice feeling. Emphasis - The Base Cases As I progressed with the rules, it was nice to see the natural progressing in the sequences that I was enabling for the parser. This progression was so natural and straightforward, I want to take the time to show how things progressed over the course of the 17 rules. The truth is that when I was finished, it felt more like 4-5 rules than 17 rules. In order of enablement, first there were the simple sequences enabled in rules 1 to 4: This is *my* emphasis. < p > This is < em > my </ em > emphasis. </ p > This is my emphasis. Rules 5 to 8 added strong emphasis: This is *my* **strong** emphasis. < p > This is < em > my </ em > < strong > strong </ strong > emphasis. </ p > This is my strong emphasis. Rules 9 and 10 added clarity on how to handle nested emphasis: *foo**bar**baz* < p >< em > foo < strong > bar </ strong > baz </ em ></ p > foo bar baz Rules 11 and 12 add clarity on escaping delimiters and how excess delimiters are handled: ***foo** < p > * < strong > foo </ strong ></ p > * foo Emphasis - Resolving the Ambiguity of Complex Cases Using just the prior rules, there were many representations that could be inconsistent from parser to parser. These rules are used to resolve that ambiguity. As these rules resolve ambiguity, I am only showing the HTML output in a code block instead of showing the HTML output in a code block and the actual HTML itself. Rule 13 adds the concept that one <strong> is preferred to <em><em> : ******foo****** < p >< strong >< strong >< strong > foo </ strong ></ strong ></ strong ></ p > Rule 14 adds the concept that the <strong> tags should always deeper than the <em> tags, if used together: ***foo*** < p >< em >< strong > foo </ strong ></ em ></ p > Rule 15 adds the concept that in a case of overlapping emphasis spans, the emphasis span that ends first overrides the interpretation of the second possible emphasis span as an actual emphasis span. *foo _bar* baz_ < p >< em > foo _bar </ em > baz_ </ p > Rule 16 adds the concept that if two emphasis spans have the same closing delimiter, the shorter of those two emphasis spans is interpreted as an actual emphasis span. **foo **bar baz** < p > **foo < strong > bar baz </ strong ></ p > Rule 17 adds a clarification that number of inline elements are more tightly grouped that emphasis spans, meaning that the parsing of those inline elements takes precedence over the parsing of the emphasis spans. *a `*`* < p >< em > a < code > * </ code ></ em ></ p > Emphasis - Rule Summary Without implementing a Markdown parser, it might be hard to appreciate how natural the process of building the emphasis support into the parser was. But please believe me, it was a thing of beauty. The best way to describe it was that as I was implementing each section, I jotted down notes for myself that usually started with \"what about…\". It was usually within a rule or two that I scratched off that note as it was no longer an issue. By the time I got to the end of the emphasis implementation, all the notes were scratched off. That kind of progression when implementing a specification are rare, and it was just wonderful to witness. What were the bumps on the road? During the implementation to support inline emphasis, other than \"fat man finger\" typing errors, there were only a couple of issues that impeded its addition to the parser. The first was because I got impatient and tried to implement one of the rules ahead of time and the second was caused by a subtle change I made to the algorithm and the side effects that I needed to abate. Other than those two issues, the development just flowed easily from the rules and their examples. The impatience issue occurred when I was trying to take care of a side-effect of rule 11, but I was trying to take care of it when implementing rule 1. It sidetracked me for a couple of hours without any resolution. It was not until I double checked that it was indeed part of rule 11 that I abandoned it until later. The \"funny\" thing is that when I did get to implementing rule 11 in the proper order, it was implemented with almost no friction in less than 5 minutes. Because of the way the rules build on each other, in hindsight it makes perfect sense that I ran into issues trying to implement part of rule 11 at the beginning. The change issue was caused by a subtle change that I made to the algorithm to better deal with Python's data structures. To simplify the implementation in Python, the references in the delimiter stack were made with indices (or \"indexes\" for Americans) instead of pointers. In the \"if one is found\" section of the algorithm there are two cases where the algorithm calls for the removal of nodes from the delimiter stack. Instead of removing the elements of the delimiter stack and recomputing the indices, I just altered the algorithm to work better with the active field. While this did cause a few small issues that needed to be resolved, in the end I believe it still was a better course of development to take. What Was My Experience So Far? Having taken time to do my research and to have a solid plan emerge, it was nice to see that the plan paying off after the code for the first couple of rules were implemented. Often when you plan an approach for a project, in your head you always think things like \"I'll have to switch to plan B if I see…\". In this case, while I still thought that, I was able to stick with my first plan the entire way through. That was nice and gratifying. I also cannot stress how impressed I am with the authors of the GFM specification and the effort they took to specify the emphasis elements. Not only did they work hard to resolve any ambiguity with 17 rules, but they provided a solid road map for implementers to follow with a suggested approach. For me, the beauty of those two parts of the specification is how they weave together, each one reinforcing the other one. In its simplest form, they took something that I was sure was going to be a headache and made it very easy to implement correctly. Basically, I started this part of the parser with a feeling of dread, thinking that I would plan an approach and switch to plan B, or even to plan F. But with some good planning on the behalf of the authors of the GFM specification, it went off without any major issues. It was cool to see that happen. What is Next? Having had a very smooth time implementing emphasis, it was very tempting to just dive in and tackle all of the link support at the same time. However, in looking at the different types of links in the specification, I decided that inline links would be a good place to start. That is where I will pick up in my next article!","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/03/23/markdown-linter-adding-inline-emphasis/","loc":"https://jackdewinter.github.io/2020/03/23/markdown-linter-adding-inline-emphasis/"},{"title":"Markdown Linter - Verifying Base Scenarios","text":"Introduction In the wrap up for my last article , I provided extra information on my decision to write a Markdown parser as part of the PyMarkdown project. My belief is that the writing of an effective Markdown linter requires a parser that emits an intermediate set of tokens instead of the usual HTML output. Accordingly, from the start of the project, all the scenario tests for the parser have dealt with those generated tokens and not the usual HTML output. Based on my experience to date, both the belief in a non-HTML parser and the decision to test the tokens have proven to be the right choices. During the project development to date, I have not observed a specific case where I believe that there would have been a substantial benefit in comparing the output to HTML instead of the intermediate tokens. However, even with that in mind, the lack of a direct comparison between the output specified in the GFM specification and the output of the parser started to erode my confidence, even if just fractionally. The questions that came to mind were simple ones. Did I keep the right information when parsing the Markdown to enable a HTML transformation of the tokens? Did I pick the right token to represent a Markdown element in the token stream? Do the tokens contain enough information in them to allow me to write linting rules off them? For me, these questions are relevant given the nature of the project. Looking at those three questions, I quickly realized that answering that last question was impossible until I start writing the linting rules. Sure, I could take a guess, but that is all it would be. However, I realized that I could probably answer the first two questions and that there was significant benefit to be gained from doing so. If I can write a token-to-HTML translator and apply it to that token stream, when the HTML output for all scenarios match, I have answered the first question. And while I cannot answer the second question completely, if the translation to HTML is simple enough, I will have proven that I am headed in the right direction with respect to making good choices for the representative tokens. While I cannot prove that those choices are not perfect choices until the rules are written, I can at least prove to myself that my token choices are in the right direction. What Is the Audience For This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 29 February 2020 and 06 March 2020 . Finishing Up the Scenario Tests For me, this task was essentially a bookkeeping issue. As the remaining features were the emphasis elements and the link elements, it felt like the right time to march through the remaining scenarios and implement them as scenario tests. These scenario tests fell into two categories. In the first category, if the scenario needed to test a failure using already implemented features, I copied over an existing test, changed the Markdown input, executed the new test, and copied the tokens from the newly executed test output in that test, manually verifying the tokens as I copied them. Basically, I figured that if the scenario test is failing in a manner that will not change even when the owning feature is implemented, then completing the test was the best choice for the project. In the second category, I did the same thing except I stopped before the execution step, instead adding a @skip tag to the test's definition. In this way, I was able to add the bulk of the remaining tests without having tests that would obviously fail getting in the way. While this may have seemed like busy work, it was meant to give me a realistic picture of how close I was to finishing the parser, and it worked. By executing pipenv run pytest , I executed every test and was able to look for entire modules with skipped tests, getting a good indication of what features and testing was left. From a wholistic point of view, it was good to see that out of the 875+ tests in the project so far, there were only just over 100 tests left to go before the parser would be completed. Being able to see how close I was to finishing the parser was definitely worthwhile! Adding Test Grouping I knew from the start that this would be the monotonous part, so I tried to make sure that I could stage the changes as I brought more scenario tests online. The first thing I did was to add a new marker for PyTest to the project by adding this line to the setup.cfg file for the project: markers=gfm This change allowed me to use one of PyTest's grouping features: marking . By changing a scenario test's definition from: def test_atx_headings_032 (): ... to: @pytest.mark.gfm def test_atx_headings_032 (): ... I included that test into the gfm group. While I can still execute that test by itself by entering pipenv run pytest -k 032 , I could now execute all tests in the gfm group by entering pipenv run pytest -m gfm . This command was invaluable during development of the parser. After adding HTML translation support to a scenario test, I ensured that it was added to this group, thereby staging the scenario test with its owning feature. After completing the change to make the test pass, I then executed all the tests in the gfm group to ensure that I didn't break anything else in the process. While it caused me some issues from time to time, it was an extra watch over the completed work, one that I appreciated. Adding Translating into HTML Translating any stream into something requires a loop to process through each element in the stream, with some mix of emitting data and altering state. I created the TransformToGfm class to handle that translation, with the transform entry point to facilitate the transformation. At this point in the implementation, this class was very simple. As each token was seen in the loop, its data was emitted with only minor additional processing required. Adding this support into existing tests was easy, just monotonous. Using the same test 32 that was used in the prior example, that test changed from: @pytest.mark.gfm def test_atx_headings_032 (): \"\"\" Test case 032: Simple headings \"\"\" # Arrange tokenizer = TokenizedMarkdown () source_markdown = \"\"\"some markdown\"\"\" expected_tokens = [ ... ] # Act actual_tokens = tokenizer . transform ( source_markdown ) # Assert assert_if_lists_different ( expected_tokens , actual_tokens ) to: @pytest.mark.gfm def test_atx_headings_032 (): \"\"\" Test case 032: Simple headings \"\"\" # Arrange tokenizer = TokenizedMarkdown () transformer = TransformToGfm () source_markdown = \"\"\"some markdown\"\"\" expected_tokens = [ ... ] expected_gfm = \"\"\"<p>some markdown</p>\"\"\" # Act actual_tokens = tokenizer . transform ( source_markdown ) actual_gfm = transformer . transform ( actual_tokens ) # Assert assert_if_lists_different ( expected_tokens , actual_tokens ) assert_if_strings_different ( expected_gfm , actual_gfm ) In order of appearance, an instance of the TransformToGfm class was added, the expected_gfm variable was set to the expected HTML, the transform function was called, and then the contents of the expected_gfm variable were compared against the output from the transform function. Except for the expected changes to the expected_gfm variable for each test, this transformation was repeated for each test as support for the feature it enabled was added. Translating the Leaf Blocks Translating the leaf block tokens added in this article and this article proceeded quickly, encountering only a few unexpected issues. These issues fell into two categories: the handling of HTML blocks and code blocks, and the handling of newlines in the HTML output. Most of the issues that were uncovered for leaf blocks dealt with the processing of HTML blocks and code blocks. As mentioned in previous articles, these two leaf blocks are special in that they maintain firm control over the formatting of their content. To accommodate these two leaf block types, the handling of the TextMarkdownToken was changed to accommodate the stricter output requirements of those blocks, mostly ensuring that whitespace was preserved. Other than that, the only other changes needed for processing was to change most of the tokens to expose certain fields, allowing the translator to access the token's attributes cleanly. From a rendering viewpoint, I had guessed that any newlines in the HTML output were going to be a problem from the start and I was right. While the GFM is purposefully vague on when to add newlines in the translation from Markdown to HTML, it was a vagueness that I could not avoid. As the main push for this article's work was to add proper comparisons of the GFM's HTML output for each example, I had a hard choice to make. Either I make modifications to each of the 673 scenarios as I copied their HTML output into the scenario tests, or I needed to ensure that the translation replicated the HTML exactly . After a lot of thinking, I decided to go with the exact HTML output path, hopefully removing any possible errors that may have occurred during the translation of the HTML output for each scenario test. When I thought about both options, I just felt that I would almost instantly regret making any changes to the HTML output, as it would no longer be synchronized to the GFM specification. Considering that, I figured it was better to be consistent and do a bit more work on the project than to start changing the scenarios. My current focus was on enabling the HTML comparisons, and I knew it was going to take more effort and time to get them right. As such, I decided to add a fair number of \"if this look like\" conditions to add or not add newlines, with plans to later refactor the code to look for better groupings down the road. I do not like adding technical debt just for the sake of expediency, but I just felt it was the right decision at the time. I figured by adjusting the translator with little tweaks here and there would give me a more complete picture on what needed done for a proper refactor later. It was not a perfect decision, but it was one that I felt I could live with. When all the leaf blocks were completed, I did notice a decent boost in my confidence. Except for some issues with getting newlines right, the translation of leaf blocks was straightforward. Knowing that I had made good decisions so far gave me that boost… something that I would need sooner rather than later. Translating the Container Blocks While the translation of the leaf blocks went smoothly, I hit a number of issues dealing with the container blocks added in this article . While the block quotes themselves were not really an issue, the list blocks that caused me a lot of trouble. In implementing the list block support in the translator, I was able to quickly get to a point where the tags themselves were being emitted properly, but the whitespace around the tags were off, especially with the newlines. That was frustrating, but with some helpful observations and experimentation, I was able to get that taken care of relatively quickly. Following that triumph, I spent a few aggravating days trying to figure out why some list items contained <p> tags and why some list items didn't contain <p> tags. I tried a couple approaches based on the surrounding tags and tokens, but each of them failed. It wasn't until I was looking at the specification again, when I took another look at the lists section and noticed the following paragraph in the lists section : A list is loose if any of its constituent list items are separated by blank lines, or if any of its constituent list items directly contain two block-level elements with a blank line between them. Otherwise a list is tight. (The difference in HTML output is that paragraphs in a loose list are wrapped in <p> tags, while paragraphs in a tight list are not.) That was the information I was searching for! While the actual implementation is a bit more complicated than just that, that is the essence of when I needed to add the paragraph tags. The complications in implementation arose as the examples became more complex. For example, based on the above description, it is easy to see that this modified example 294 is a strict list: - a - b - c and this unmodified example 294 is a loose list: - a - b - c From the above lists section quote, since there is a blank line that separates two of the list elements, it is a loose list. Pretty easy and straight forward. Implementing this aspect of looseness was decently easy but did require some non-trivial extra code. Basically, go back to the start of the current list, then go through each list element in the list, looking to see if the Markdown element before it is a blank line. If so, mark the entire list as loose and apply that throughout the list. However, when dealing with lists and sublists, it was not so simple. For example, consider the Markdown from example 299 : - a - b c - d Understanding the Markdown list blocks can be nested and following the guidance from the above quote, you can deduce that the outer list is tight and the sublist is loose. To make the leap from handling the previous 2 examples to this example would mean that I needed to find a way to add scoping to the translation. Without scoping, when the translator processed the above example, it saw 3 items in the same list, with the second element making the entire list loose. Scoping was required to allow the translator to determine that the a and d items were in one list and the b/c item was in it's own list, therefore determining the correct looseness for both lists. The code itself to handle scoping was simple, but the tradeoff was that the translator was slowly becoming more complicated, something that I was not happy about. It was not in dangerous territory yet, but it was something to keep a watch out for. In addition, while the difference between a list being lose or strict is obvious in hindsight, at the time it was very annoying. It took me the better part of 4 days to do something that was obvious. Even better than obvious was the fact that it was plainly spelled out in the specification. But as I had to do a number of other times during this project, I picked myself up, dusted myself off, and continued on with the inline translations. Translating Inlines - Backslash Escapes, Character References, and Code Spans During this translation process, first I hit a high, then I hit a low, and then I saw the payoff of both with the translation of these inline elements into HTML. These elements consist of the backslash escapes, character references, and code spans elements, and were added in this article . Apart from a couple of typos and the code spans, adding support for these features flew past quickly. The backslash escapes and character references were already being processed along with the text tokens, which in turn were already tested with the leaf blocks. The only new code needed was for code spans, but those additions were quickly made by copying the work done for code blocks and simplifying it a bit. Other than a couple of typos that I also needed to correct; the entire body of this work was completed in just under 3 hours. And to be honest, that included me grabbing some well-deserved dinner. Based on the days of trying to figure out list blocks and paragraph tags from the last section, it was nice to get a real easy set of changes. It wasn't anything challenging, just… nice. Translating Inlines - Raw Html, Autolinks, and Line Breaks Rounding out the series of translations were the tests for raw html, autolinks, and line breaks. With features just added in the last article , the tests for these features were added with only a couple of issues, similar in severity to the issues from the leaf blocks. The largest group of issues encountered were issues with character encodings in the autolinks feature. Some of those issues were due to Unicode characters being present in the Markdown but needing to be properly encoded and escaped when present in URIs. Some of the issues were because the characters present in the URIs are special characters and had to be escaped to prevent them from being encoded twice. However, the most annoying issues were differences in the language libraries that caused the translator to URI-encode a different set of characters than in the GFM specification. Specifically, it looks like the Commonmark parser uses the Javascript libraries to encode URIs, while the PyMarkdown project uses the Python libraries. I wasn't too concerned with these issues at the current time, so I made sure to add some notes to address these concerns later and kept on marching forward. The big catch with these changes was with the scenario test for the GFM specification's example 641 : < a href = 'bar' title = title > While it might seem like a small difference when looking at a web page, the PyMarkdown parser emitted a HTML block and a tag as content instead of emitting a simple paragraph containing text, as follows: < p > &lt; a href='bar'title=title &gt; </ p > Looking at the HTML output in the example, it is very clear that it should be a paragraph containing text, but somewhere in the copy-and-paste process I had accepted the wrongs tokens as correct. Digging into this issue, I quickly found out that a single omission in one of the functions of the HtmlHelper module was not checking for whitespace between the tag's attributes, therefore thinking that it was a valid tag when it was not. Within 5 minutes, I had a fix implemented, and the test corrected, and the last scenario test that was currently implemented was now complete! As a strange aside, I may not have the only parser that has made this mistake. When I was writing this article in VSCode, as usual, I had to check the example's Markdown a few times. The Markdown in the above section was generated with the following fenced code block: ``` Markdown < a href = 'bar' title = title > ``` Its a pretty simple example, and I typed it in as I usually do. Create the fenced block, add in the text from its source, verify it a couple of times, and then add in the language specifier. As soon as I typed in Markdown , a strange thing happened. The a for the tag name turned into a deep blue, as expected. But then both attribute names, href and title , turned light blue while the attribute values, `bar` and title turned red. I added a space before title and then deleted it, repeating this experiment a couple of times, looking for color changes. There were none. For whatever reasons, the coloring library that VSCode is using to color Markdown text seems to believe that example 641 contains valid Markdown. Weird! What Was My Experience So Far? In writing automation tests as part of my professional job, I have a clear distinction in my responsibility to my team. I am not there to break their code or to find flaws in it, I am there to help them find issues before they become problems. If possible, I stress automated tests over manual tests, and I also stress being able to adopt consistent processes to shine a very focused light on anything that is different. The mere fact that I found a couple small issues with the parser, even at this late stage of the project is fine with me. I am helping my team (me) find these issues before the code is released and impacts other people and their processes. While it was a large pain to go through, I felt that I closed part of the testing the loop by consistently adding HTML output verification to each parser scenario test. The mere fact that the issues were found proves its own worth. In addition, there was a small efficiency and confidence boost because I do not have to guess anymore as to whether or not I chose the right tokens. The HTML output from the examples proved that I made the right choices. In the end, what it boils down to for me is that while adding the HTML output verification to each test was painfully monotonous at times, it paid off. While only a handful of issues were uncovered, it did find at least one issue, which itself boosted my confidence in the project. Regardless of whether any issues were found, knowing that the tokens that the parser was generating were being properly translated into the GFM specification's HTML output was worth it. No more questioning whether the tokens would translate into the proper HTML… I now had proof they did! What is Next? Whenever a protocol specification states something like \"and here is a suggested way of…\", it usually means that at least 2-3 groups of people implementing the protocol specification had issues. So, it was with a bit of dread and a bit of confidence that I started looking at dealing with inline emphasis, the topic of the next article.","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/03/16/markdown-linter-verifying-base-scenarios/","loc":"https://jackdewinter.github.io/2020/03/16/markdown-linter-verifying-base-scenarios/"},{"title":"Markdown Linter - Autolinks, Raw HTML, and Line Breaks","text":"Introduction Having completed another refactoring session, I have confidence that the PyMarkdown project code has been returned to what I feel is a healthy amount of technical debt. After taking a deep breath and a good look at the features left to implement, I realized that the list is now decently short: emphasis, links, autolinks, raw HTML, and line breaks. Rechecking the section on inline structure in the GitHub Flavored Markdown (GFM) specification, it is hard to miss the fact that the emphasis and link elements have their own \"here is a suggested approach to implementation\" section while the implementation for parsing the other elements are left up to the reader. Deciding that the authors of the GFM were trying to tell me something, I decided to focus on autolinks, raw HTML, and line breaks first, leaving the emphasis and links for the last chunk of features. What Is the Audience For This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 22 February 2020 and 27 February 2020 . My Take on HTML in Markdown Back in the article Markdown Linter - Adding HTML Blocks , I provided a recommendation as follows: When it comes to HTML blocks, I implemented them as part of the parser because they are part of the specification. But because of the complexity in understanding HTML blocks, I whole heartedly recommend avoiding using HTML blocks if at all possible. The big problem I have with HTML blocks is that there are 7 types of HTML blocks, and each one has a different ending condition. As an author that heavily uses Markdown, I want any documents that I create to be simple and easy to read. In my opinion, without a lot of memorization, HTML blocks are not simple at all. Raw HTML elements are on the complete other end of the spectrum. If I had my way with the GFM specification, I would demote and remove HTML blocks entirely and replace them with raw HTML. Raw HTML is exactly as it sounds: what you type is what you get. The good news is that if you follow a couple of easy to memorize rules, you can do this: NEVER start a line with a HTML tag Markdown is an authoring language, and should only use HTML sparingly In support the first rule, my reading of the start conditions of each of the HTML block types in the GFM concludes with the observation that all start conditions begin with the text \"line begins…\". Thus, I created the first rule to ensure that I never inadvertently trigger a HTML block. Any non-whitespace text and then any HTML is fine, just not by itself at the start of the line. My second rule may seem like my own opinion, but I believe it is a solid rule. While it is useful to read the GFM section on What is Markdown? , that section boils down to this one quote from that section by John Gruber: The overriding design goal for Markdown's formatting syntax is to make it as readable as possible. The idea is that a Markdown-formatted document should be publishable as-is, as plain text, without looking like it's been marked up with tags or formatting instructions. There is little question that having the ability to add a HTML tag where needed is a good feature of Markdown. However, I would contend that any HTML tags that are added are contrary to Gruber's stated goal for a document to not be marked up with tags. As such, unless Gruber's readability goal is dismissed, anything more than sparing use of HTML in a Markdown document is harmful. Don't get me wrong, but there are good use cases for HTML, but they are rare. In the 5-6 years that I have been authoring documents in Markdown, there are only two times I have ever used HTML tags in Markdown documents. The first time is in the writing of these articles on the Markdown parser for the PyMarkdown project and the other time was for a project that used the <ul> and </ul> tags to satisfy a legal documentation requirement for text underlining. From talking with other developers using Markdown parsers that underling text is a popular request along with a blanket request to disable all HTML tag support in Markdown, mostly for security reasons. So, for what it is worth, that is my take on HTML in Markdown, and my reasons for that opinion. Your mileage may vary. Raw Html Unlike the rules that an author needs to memorize to properly use HTML blocks, using raw HTML in Markdown is very simple: its either a legal HTML tag or it gets interpreted as normal text. No \"if it's a blah tag, then…\" rules. Just valid or invalid. No ending conditions as raw HTML is inline processing of text. Simple. Easy. Clean. The reason that I can be comfortable in saying that raw HTML is simple is the following block of Markdown text: Start and End Tags - < a href = \" link \" > link text </ a > \\ Self - Closing Start Tag - < br /> \\ Alternate Parameter Enclosing - < b2 data = ' foo ' > \\ Simple Alternate Parameter - < c3 foo = bar /> \\ Gratuitous Parameter Example - < d4 foo = \" bar \" bam = ' baz <em>\"</em> ' _boolean zoop : 33 = zoop : 33 /> While the grammar is broken down in the Raw HTML section of the GFM specification, it follows the HTML 5 specification for how to construct the tag and which tag formats are valid. From a document author point of view, this is simple. When adding HTML to a Markdown document, the author either knows how to author a web page in HTML or has a person or web page that tells the author what HTML to insert into the document. In either case, assuming that those tags are valid, those tags are emitted exactly as added, with no extra baggage added during the translation. In adding these HTML samples to the above Markdown example, I am also following my own rules: never start a line with a tag and use it sparingly. None of the lines start with a tag, ensuring that none of the text is parsed as a HTML block. And while I went a bit overboard with the HTML specifically as it is an example, I can honestly say it is one of the less-than-5 times I have used HTML in Markdown. I think that qualifies as sparingly. The raw HTML inline processing was very easy to add, as the rules are very simple: its either a valid HTML tag, or not. Not much to add. Autolinks Until I started reading the specification, I had no idea that Markdown was capable of a lot of things. In this case, I wasn't aware that it can make a decent attempt at creating links with very little effort. As far as using them in my articles and documents, I am not sure about them yet, but at the very least they are an interesting concept for Markdown to include. The concept is simple. An URI contained within the < and > characters is interpreted as a link to that URI, with both the link and the link text being set to that value. There is also a variation of autolinks that uses any email address that matches the email address regular expression in the HTML5 specification . For email address autolinks, the link is set to a mailto scheme for the link and the email address for the link text. Real simple examples of these autolinks are contained within the following Markdown: My website : < http : // example . com > My email : < head . honcho @example . com > which generates the following HTML: < p > My website: < a href = \"http://example.com\" > http://example.com </ a ></ p > < p > My email: < a href = \"mailto:head.honcho@example.com\" > head.honcho@example.com </ a ></ p > To make sure they are not interpreted as HTML blocks, they are prefaced with text, according to the rules I established in the last section. While a properly created autolink should not be interpreted as either of the HTML elements, I prefer to keep things simple. \"NEVER start a line with a HTML tag unless it is a validly formed autolink\" just seemed too much. As mentioned before, your mileage may vary. The actual HTML output is simple, as denoted in the second paragraph of this section. In looking at autolinks a couple of times for this article, my feeling about autolinks as a document author is that there is not enough control of the output. Unless I am missing something, the following Markdown is equivalent to the above HTML, and it gives me more control of the document: My website : [ http://example.com ] ( http : // example . com ) My email : [ head.honcho@example.com ] ( mailto : head . honcho @example . com ) In the end, while autolinks were as trivial to add as raw HTML, I think I'll stick with my explicit links. Line Breaks Wrapping things up for this group of features are the line breaks: hard breaks and soft breaks. At 14 examples for hard line breaks and 2 examples for soft line breaks, only the indented code blocks (11), the tabs (11), and the paragraphs (8) are in the same ball park for the low number of examples needed to adequately demonstrate that given structural element of Markdown. In addition, if you really look at the 14 examples for hard line breaks, there is a good argument to be made that there is large amount of replication between the two character sequences, reducing the \"actual\" number of examples down into the same 8-11 examples range. As indicated by the number of examples, line breaks in Markdown are really simple to use, inheriting its line break rules from HTML. In both languages, if the author wants to specifically break a line after some text, they must use an element that forces a line break before its content, use an element that preserves line breaks within its content, or specify a hard line break itself. A good example of the first case are the grouping of Markdown lines into a paragraph by separating them with a blank line. This is best shown in the following Markdown from example 190 : aaa bbb ccc ddd which generates the following HTML: < p > aaa bbb </ p > < p > ccc ddd </ p > and is displayed as the following: aaa bbb ccc ddd In this example, the (soft) line break that occurs in the Markdown between aaa and bbb , and then again between the ccc and ddd , is kept as it is translated into HTML. However, when the HTML is rendered, that line break is not displayed. When displayed, the first pair of characters are displayed, followed by a line break, and then the second set of characters. As a general default rule, Markdown blocks force a hard line break before displaying their contents, to ensure that the content is understood to be different. For the second case, a good example of it is a slightly modified version of the Markdown from example 110 : foo ``` bar bam ``` baz which generates the following HTML: < p > foo </ p > < pre >< code > bar bam </ code ></ pre > < p > baz </ p > and is displayed as the following: foo bar bam baz The first thing to notice is that, as described in the last example, when a new Markdown block is started, the default rule for displaying HTML forces a hard line break to be rendered, keeping its content distinct from the previous content. As such, in this example both the <pre> tag and the <p> tag are displayed with a line break before them. While this behavior can sometimes be overridden with styles and style sheets, it tends to make things more confusing and is mostly avoided by HTML authors. The second thing to notice is that the line breaks within the code block are preserved as-is. Both code blocks and HTML blocks maintain a very tight control on how their data is translated and displayed, being the only two block elements that preserves any line breaks within its content. In terms of other elements, it should be no surprise that code spans and raw HTML are the only two inline elements that also preserve line breaks within their content. That leaves the final use case, where a Markdown author wants to force a hard line break outside of any of the previously mentioned constructs. But, how does an author do that? The often used and not-so-visible option is to end a line with 2 or more spaces that is also not at the end of a block. Merging examples 657 and 666 together produces the following Markdown: foo < space >< space > < space >< space >< space >< space >< space > bar foo < space >< space > generating the following HTML: < p > foo < br /> bar foo </ p > and is displayed as the following: foo bar foo Note that for the sake of clarity with this example, the string <space> is used in place of the actual space character itself. The two spaces at the end of the first line cause the HTML hard break tag <br /> to be inserted into the data, generating a line break not only in the generated HTML, but also in the displayed HTML. In contrast, since the two spaces at the end of the third line closes off the paragraph block, they are simply stripped away and not replaced with a hard break. This was a smart move as any Markdown following that paragraph will be in a new block, the starting of the new block will, by default, force a hard break in the display of that block, as noted above. In contrast, the second way to force a hard line break is to end the line with the \\ character, as shown in the following Markdown: foo \\ bar foo \\ While there are changes in the Markdown from the previous example, the generated HTML remains the same. After my initial confusion between Python's \\ line continuation character and the Markdown's \\ hard line break character (as documented here ), the explicit hard line break character grew on me. What Was My Experience So Far? If I am being honest with myself, I was not sure at the beginning of the project if I would ever get to this point. With only 2 inline elements left to process, not including the link definitions deferred from before, the parser is getting close to being able to handle a full and rich Markdown document. As the GRM specification contains over 673 scenarios, there were times that I thought I would just give up or use a \"mostly\" complete parser… something that was just barely good enough. But getting to this point, close to having a solid parser completed feels great! Sure, there have been cases where it took me a day or two to figure out how to do something properly, such as list blocks and block quotes. Those were tough. And as I am writing this article at a 2-3 week delay from when I made the actual changes to the project, I know that there are some more bumps in the row yet to come. (No Spoilers!) But the important thing is that while those things are hard, I face them with the mentality that I talk about in my article on Embracing Something Hard . Maybe its just how I am, but for me part of the challenge is to embrace something hard and work my way through it. It was when I was thinking about some of hard stuff that I had tackled previously and how easy this block of features was that I started wondering. Looking back in the project at anything written before this commit on 27 February 2020 , it is hard to find any test that compares the output to HTML, though all of scenario test output from the GFM specification is written as HTML. What's with that? Going back to my article on Collecting Requirements , I determined that to properly write a Markdown linter, I needed to be able to take the output from a parser as a set of tokens, not output as HTML. The entire reason that I have taken steps to write this Markdown parser is that there are no current parsers that do not output interpreted Markdown as HTML. Furthermore, the previous section on autolinks is proof of that need. Producing a simple link to a web page, I can generate it using an autolink, a raw HTML tag, and a HTML block. Tokenizing the output before that translation to HTML is the only way to ensure that I am linting the Markdown properly. To get me this far, testing against the tokenized output of the parser was the right thing to do. The linter is going to observe and consume the tokens from the parser, so they are the right thing to test. But the questions of whether I was generating the correct tokens started to bounce around my mind… What is Next? As I mentioned in the last section, I had some concerns about whether or not the tokenization of the Markdown was correct, so I decided to go all out for the next section and add the remaining scenario tests from the GFM specification. To close the loop on the testing, I also went through all the existing tests and added a new class that transforms the PyMarkdown tokens into HTML, comparing that output directly against the GFM specification. Stay tuned!","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/03/09/markdown-linter-autolinks-raw-html-and-line-breaks/","loc":"https://jackdewinter.github.io/2020/03/09/markdown-linter-autolinks-raw-html-and-line-breaks/"},{"title":"Markdown Linter - Taking Time to Refactor -- Post-Easy Inlines","text":"Introduction When I was working on implementing the inline code spans , as detailed in the last article, I performed a thorough scan of the scenario tests and their source data, noting down any issues I found. I knew that I had missed the mark on how to internally represent Atx Headers, and I was curious about how many other things I had missed. Having found a decent handful of issues to fix, I decided to spend some time to address those issues before adding more of the inline processing. In my mind, it was better to take a week and try and keep the issue count low than to continue forward, possibly compounding the cost of fixing those issues. As I am in a somewhat ideal scenario, with nobody pressuring me for features or issue fixes, I figured it was best if I took advantage of that to the best of my abilities and \"do it right\". What Is the Audience For This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 14 February 2020 and 20 February 2020 . Why Refactor Now? As the owner and sole developer on the project, my decision to stop and refactor was any easy one to make. It also did not escape my attention that making a decision like that isn't always that simple for larger projects. If this were a work project or a team project, the group working on the project would have to meet to figure things out. The team itself usually plays a critical role in assessing the cost and benefit of each task, but in the end it is a balancing act for managers and directors to figure out which tasks are the best ones to focus on at any given time. As it is only me on the PyMarkdown project, I get to conveniently shortcut those conversations in favor of simple decisions based on my own experience. From my years of experience, I find that there are usually two driving forces that make me support a decision to refactor a project. The first driving force is the cost of any applicable technical debt on product code. As I consider tests integral to a project's quality, my definition of the term \"product code\" includes all source code and resources required to execute the project normally and to perform testing used to certify that same project as shippable. With almost 700 scenario tests for PyMarkdown's parser, even a small change can generate large ripples through the code base, especially the scenario tests. As the scenarios for a feature are only added when that feature itself is added, each feature added therefore adds a significant amount of product code for any change to ripple through. It also follows that each extra test impacted by an issue means an increased cost to fix that issue before proceeding. Therefore, if the goal is to keep overall costs down, refactoring more frequently during development seems logical. The second driving force is less quantifiable, but equally important to any type of project. That force is the flow of the team working on the project. Based on past projects, I know that I work more efficiently if I focus on a group of similar things for a while, then shift to another group when done. The larger the difference is in the skill sets between the two groups of work, the more relaxed I feel about the work. This sense of relaxation allows me to get into a better flow. As this project is being written by me in the evenings and on the weekends, keeping a good flow for the project keeps me focused and energized about the project. As I am the sole developer on the project at the time, keeping myself motivated in a necessity! While one force is more empirical and the other is more feelings based, both forces worked together to convince me that it was yet again time to refactor the PyMarkdown project. Issue 1: SetExt Headers, Paragraphs, and Block Quotes I needed one of the smaller issues to get me warmed up, so after looking over the list of possible issues to work on, I decided on this one. During the review of active scenario tests, I noticed that the text === was in the paragraph tags for the specification's example, but the tokens that the parser were omitting had them outside of the paragraph. It seemed simple enough but looks were deceiving. It took me a bit of digging to find two possible reasons for the string === to be kept inside of the paragraph: laziness and non-interrupts. Working backwards, the concept of laziness with block quotes and list items is that you can omit any of the leading characters if the next non-whitespace character after the omitted characters is text that continues the paragraph. In essence: > this is a block quote is equivalent to > this is > a block quote That got me halfway there, but there was still the question of how the === would be kept as part of the paragraph and not as a SetExt header indicator. I read the section on SetExt a couple of times before the following lines clicked: However, it cannot interrupt a paragraph, so when a setext heading comes after a paragraph, a blank line is needed between them. Bingo! I wish it did not take multiple reads through that part of the specification, but specifications are sometimes like that. Based on those discoveries, I temporary rewrote the scenario for scenario 63 to: foo bar === The goal of this was to remove the laziness out of the equation while working on the interrupt issue. When I reran scenario test 63, I correctly got a single paragraph with 3 lines in it. Whatever the issue was, it was not just simple parsing of paragraphs. Taking a step forward, I added the block quotes back into the picture, changing that scenario text to: > foo > bar > === It was at this point that the scenario broke, apparently thinking that the third line was a SetExt header and should close off the paragraph. Issue reproduced! To be sure, I applied the laziness concept to the block quote, reverting the scenario back to its original text of: > foo bar === and validated that the behavior was the same. Armed with a good reproduction case for the issue, good debug output, and a general area in the source code for the cause of the issue, the issue was identified and fixed in quick order. This issue was very specific, so only the one scenario test was impacted, which was a good thing. The problem was that during the development of block quotes, something made me think that anything looking like a SetExt header should close off a paragraph, hence I added code to do just that. I checked the code a couple of times, and that was the only scenario test referencing that code, so I just deleted it. While the research on the issue was a bit more effort than I originally thought, fixing this issue was a great warm up to the next couple of issues. One issue, one scenario test impacted, and I was refactoring. Issue 2: Python, Markdown, and Line Continuation Characters The next issue for me to work on was a subtle copy-and-paste error, one that flew under my radar until I took a solid look at it. This issue did not show any indications of failure until I started my scan of the scenario tests. The only reason I found this one was that I went looking for any error, not something specific. In Markdown, the \\ character at the end of the line is used to denote a hard line break, not yet to be implemented in the PyMarkdown project. In Python, the \\ character at the end of the line is used as a line continuation character, telling the Python interpreter to treat the text before the character and the text after the character as a single line. Hopefully, any readers see where I am going with this. When I added the scenario test for scenario 60 , I did a copy-and-paste on the Markdown input to the new scenario test, a process I have done for 99% of the scenario tests in the project. To accomplish this, I pasted the following Markdown text between the \"\"\" characters denoting the Markdown to use as input: Foo \\ ---- resulting in the Python code: source_markdown = \"\"\"Foo \\ ----\"\"\" After pasting each scenario's Markdown into the scenario test, I try to ensure that I replace every instance of the \\ character with the \\\\ characters to properly represent the backslash character in a Python string. As I am only human, there are times that I forget to do this. Luckily for me, if a \\ character is not paired up with a valid character to escape, the Python interpreter will generate a warning like the following: ======================================================== warnings summary ======================================================== test\\test_markdown_setext_headings.py:346 C:\\old\\enlistments\\pymarkdown\\test\\test_markdown_setext_headings.py:346: DeprecationWarning: invalid escape sequence \\> expected_gfm = \"\"\"<h2\\>Foo\\\\</h2>\"\"\" -- Docs: https://docs.pytest.org/en/latest/warnings.html As the backslash character in Markdown is used with punctuation characters and the backslash character in Python is used with alphabetic characters, this is usually a very solid detection scheme for finding copy-and-paste misses. In this case, that check failed. The good news here is two-fold: an easy fix and a very localized fix. This fix was easy as I just had to apply the missed substitution. It was localized mainly because I had not yet implemented hard line breaks. And yes, it meant that when I did implement hard line breaks, I triple checked my line endings to avoid this issue showing up again. Momentum was increasing, so it was time to step things up! Issue 3: Code Blocks, Indenting, and Blank Lines Having resolved a couple of warm-up issues, I felt it was time to tackle some larger issues. Each issue in this group either deals with vertical space issues or leading space issues within a code block. The vertical space issue was that blank lines were not being folded into the text blocks properly, causing foreseeable issues with parsing complete blocks of text soon. Given some Markdown text, such as: ``` abc def ``` I expected that the output tokens would include a fenced code block with a single text block inside of it with three lines present. Instead, there were three tokens present, the first and last were text tokens with strings and the token in the middle was a blank line token. While the token representation was technically correct, parsing it would be awkward. For the specific case of blank lines within a code block, it made sense to merge the blank line tokens into the surrounding text tokens. The leading space issue was a bit more subtle but equally simple. To properly parse text blocks within a code block, an appropriate amount of whitespace may need to be removed from each line as it is combined. As always, it is the details that matter, and it is easy to gloss over them. In the opening part of the indented code block section of the specification, the following line is present: The contents of the code block are the literal contents of the lines, including trailing line endings, minus four spaces of indentation. The similar line for fenced code blocks reads: If the leading code fence is indented N spaces, then up to N spaces of indentation are removed from each line of the content (if present). Basically, this means that the following indented code block: fred frank should produce a code block with the text fred on the first line and the text <space>frank 1 on the second line, having had the first 4 spaces removed. The fenced code blocks are a bit more nuanced, in that: ``` fred ``` is parsed as <space><space>fred 1 and: ``` fred ``` is parsed as fred , based on the extra indenting of the fenced code block start. While the line containing the text fred is the same in both cases, the number of leading spaces before the fenced code block are different, resulting in the different outputs. Prior to fixing this issue, text lines were combined in a simple manner and no whitespace was removed from the start of any lines within code blocks. To properly address this issue, not only did these two rules need to be followed, but the existing code to properly remove leading spaces for each line within a normal paragraph needed to be preserved. It took a bit to get it right, but with a good number of scenario tests to keep things honest, it was easy to get it right quickly. My original estimates for the impact of this issue was 15-20 tests, and it thankfully remained within that range. While the initial number of scenarios covered by these issues was 15, I expected other scenarios to use code blocks to show how their feature worked with code blocks, adding another 3-7 scenario tests in the process. Looking back, I think I got off nicely with the scope of these changes. Issue 4: Paragraphs and Indenting Feeling energized from fixing the issues documented in the previous sections, I decided to keep with the theme and attack the issue with leading spaces in normal paragraphs. Similar to the prior issue, the rule for normal paragraphs is that all leading whitespace is removed as the text for the paragraph is pasted together. While it is hard to point to an exact quote from the specification for this rule 2 , example 192 clearly shows this as the Markdown: aaa bbb is translated into the HTML text: < p > aaa bbb </ p > The fix for this was similar to the change made for the code blocks issue, but instead of specifying a fixed number of whitespace to remove, the combine function was changed to accept a value that indicates the removal of all whitespace. The original 9 cases were quickly tested with the fix, and things looked solid. Originally, it looked like the changes would be confined to the original 9 cases, but I suspected that the number would at least be double that, as this fix would affect any scenario involving a paragraph with multiple lines. While a number of the cases were simple cases, when all was said and done, there were 59 changes to scenario tests in the commit for this issue. Even so, those changes were quickly made, and the scenario tests were re-verified. While this took a lot of work to get right, it felt good to get this one resolved. It was hard for me to quantify this to myself, but the parsing of the paragraphs always looked like they had too many spaces. It was nice to finally figure out why! Issue 5: Trailing Spaces in Scenarios During the early development phase of the project, I wanted to get going with the scenarios as quickly as possible, so I copied each scenario's Markdown text from the GFM specification exactly as-is. This was not a problem, except that in a small number of cases, there were lines in the scenarios that ended in one or more whitespaces. These trailing whitespaces raised the trailing-whitespace warning when I ran the PyLint program over the project's code base, as I do with each set of changes. Determined to deal with the issue later, I added a few comment lines like this: # pylint: disable=trailing-whitespace to disable the Pylint warnings that occurred, knowing I would have to fix them later. At this point in the project, it seemed like a good time to deal with this. As this issue was largely out of sight, everything was fine. That is, everything was fine until I hit a couple of problems in a row that did involve these scenarios. Instead of immediately noticing the trailing whitespace and the comment, I started debugging each issue without noticing either the comment or the whitespace, and was dumfounded by why the parsing was not as was suggested by the Markdown text that was clearly visible. When I took a step back to really read the scenario tests, I then noticed the comment at the top of each of the problem test functions, and then it clicked. But it took a lot longer than it should have. Instead of \"just dealing with it\", I decided that refactoring was a better solution. The fix was an easy one too, something I should have thought of earlier. The ASCII BELL character is represented by \\a in Python strings, and to the best of my knowledge is seldom used in most Python programs or Markdown text. As it has a very low probability of being used in any scenarios, I replaced the terminating whitespace characters with \\a characters, then added .replace(\"\\a\", \" \") at the end of the sample string. It was then a simple matter of going through the other 6 scenarios with trailing whitespaces and repeating this fix. While issues like this may seem small, having to disable a PyLint warning did not feel right, even if it helped me maintain momentum at the time. It just felt really good to solve this issue properly. Issue 6: Getting Atx Headers Right In the last article, I started looking for issues after wondering if a code span could work within an Atx Header, realizing that the specification allowed it, but my current implementation did not allow it. As stated in the preamble for scenario 36 : The raw contents of the heading are stripped of leading and trailing spaces before being parsed as inline content. Basically, my decision was based on my usage patterns, not the specification. In my experience, I have only ever done Atx Headers in the form: ### This is my header When I read the specification, I glossed over that section, only thinking of Atx Headers as containers for normal text. However, based on the specification, the following text is also allowed: ### This *is* `my` header As a result, instead of the header text This is my header being generated by the first sample, I can use the second sample to generate the header text of This <em>is</em> <code>my</code> header . Neat! The change itself was pretty simple and confined to the parse_atx_headings function. As it was a simple change, the accompanying change in each test was also pretty simple: take a single Atx Header token with text, replace it with an Atx Header token without the text, a Text token with the text, and an End token for the Atx Header. While I was concerned that the fix for this issue was going to be more widespread, it was confined to 22 scenario tests, and was easy to verify. Issue 7: Bringing the Tabs Back Looking for something to finish the refactoring session with, I decided to tackle one of the longstanding fixes that I had some reservations about: the bulk conversion of tabs to spaces. While it was a good fix at the time, I suspected that there might be problems with the code blocks where all text is supposed to be preserved literally, including tab characters. In a stroke of luck, all the affected scenario tests are in the test_markdown_tabs.py file and the places where tabs are important can be grouped into 2 distinct groups. In the Markdown specification, there is a distinction between whether there is enough whitespace for an indented code block with 4 spaces, or not with less than 4 spaces. To address those cases, I simply added the is_length_greater_than_or_equal_to and is_length_less_than_or_equal_to functions. While I could have simply used a not modifier to get the same effect, I thought it was more readable to simply spell it out. For cases where the actual length was needed, the calculate_length function determines the length of the string, allowing for the length of a tab to be 4 characters while every other character is assigned a length of 1. While this was not a very technical issue to fix, it helped me return things to a known good state, with confidence that tabs were being treated properly. Before this fix, I was always concerned that the bulk translation of tab characters to spaces would introduce a hard to diagnose issue. With that translation removed, that concern went away. What Was My Experience So Far? At various points in the development of PyMarkdown, I have wondered if my thinking should be more realistic with a \"two steps forward, one step back\" feel to it. Maybe it is just who I am, but with a few exceptions, I see almost all of this development as stepping forward with quality, and hence, all positive. I like the fact that I am implementing some new features, then doing some refactoring, and repeating. It gives me a solid perception that the project is on stable footing at every stage, accumulating little technical debt along the way. Something that struck me at this point was how easily I seemed to fall into a rhythm that works well for me and the project: implementing a couple of features, noting down any issues as I implement, and then fixing a couple of the more pressing issues before repeating the pattern. I am not sure if that kind of a pattern that everyone else works well with, but it seems to work well for me. To a certain extent, it also helps me write these articles, as writing about quality software is very different than the development of that software. For me, I find that they complement each other very well. In terms of energy, keeping that rhythm going and writing these articles is helping to keep me charged up about the project. While I have written my share of parsers in my career, they have almost always been for work projects with a specific goal and deadline to achieve. Being freed from those restrictions does come with its benefits, but not having someone looking over your shoulder means that you have to take on that role yourself. These articles, while initially created to talk about my approach in creating quality software, also server the purpose of keeping me honest and responsible to any readers. Call it a backup plan, but it seems to be working well! What is Next? Going back to the specification for features to implement, I decided to start at the end and get the line breaks, autolinks, and raw html inline processing taken care of. While I do not use them frequently myself, they are interesting aspects to the GFM specification, and perhaps learning about them will increase my use of them in my everyday Markdown usage. The string <space> represents a space character, which by it's very nature, is invisible. ↩ ↩ The specification states \"The paragraph's raw content is formed by concatenating the lines and removing initial and final whitespace.\" This is the closest reference that I could find to removing whitespace. Perhaps initial means per line? ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/03/02/markdown-linter-taking-time-to-refactor-post-easy-inlines/","loc":"https://jackdewinter.github.io/2020/03/02/markdown-linter-taking-time-to-refactor-post-easy-inlines/"},{"title":"Markdown Linter - Starting Inline Processing","text":"Introduction Maybe it is just me, but I love the feeling of completing a good round of refactoring where I really get to focus on making sure that the foundations of the project are stable. If it helps any readers, I imagine it as a spa day for your project where the project just gets some personalized attention and cleaning up. While the project is not in a perfectly clean state, I know that I performed a decent amount of tidying up in that direction, work that will help the project as it grows. With the project cleaned up, and with the new changes to make the text blocks continuous, it was time to start on the inline processing. The first three inline elements to be implemented were the first three elements in the GitHub Flavored Markdown (GFM) Specification : backslashes, character references, and code spans. These elements allow Markdown to escape certain characters, replace a text sequence with a single Unicode character, or indicate that some text is literal code. Each of these elements has its own special use, and are used very frequently when writing Markdown documents. And if those reasons were not good enough, they just happen to be the first three sections in the specification's inline processing section. What Is the Audience For This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 08 February 2020 and 14 February 2020 . Inline Processing In Markdown processing, there are two main types of processing that occur: processing to sort the text into blocks and the processing of the contents for those blocks. Courtesy of the specification , another good explanation is as follows: We can think of a document as a sequence of blocks — structural elements like paragraphs, block quotations, lists, headings, rules, and code blocks. Some blocks (like block quotes and list items) contain other blocks; others (like headings and paragraphs) contain inline content — text, links, emphasized text, images, code spans, and so on. While it was not readily apparent on my first read of the specification, inline processing occurs only on the content of leaf blocks that do not strictly govern their content. As code blocks contain the literal content for their output and HTML blocks contain the literal HTML content for their output, inline processing is not applied to those blocks. Inline processing is applied to the content of the remaining blocks, the headings blocks and the paragraph blocks, which just happen to be the most frequently used blocks in most Markdown documents. Backslash Escapes Having completed most of the processing required for the leaf blocks and container blocks, it was time to move on to the inline processing of the content within those blocks. The first of the inline processes to be worked on: the backslash escapes. For readers familiar with backslashes in modern programming languages, Markdown's usage of backslashes is similar, but with a twist. In modern programming languages, a backslash character is used in strings to escape the character following the backslash, using that next character to denote a special character. For each special character to be represented, a distinct backslash escape sequence is used to represent it. For example, most languages include the escape sequence \\n for a line feed or end-of-line character. This backslash escape is used so often that many programmers use the terms \"slash-en\" or \"backslash-en\" instead of referring to the \\n character sequence as the new-line character it represents. The twist that I mentioned earlier is that Markdown, unlike programming languages, uses backslash escapes to only escape the following ASCII punctuation characters with themselves: !\"#$%&'()*+,-./:;<=>?@[\\]&#94;_`{|}~ For example, the string \\! will emit the sequence ‘!', but the string \\a will emit the sequence \\a . Coming from a programming background, this took a bit of getting used to, but it makes sense. As Markdown is used to create a human readable document, authors should not be concerning themselves with control characters, but with how the content is organized. From that point of view, escaping the punctuation characters used to denote organization makes a lot of sense. It then follows that each processing character is included in that string of characters, and that the most prominent use of backslash escapes in Markdown is to avoid block and inline processing. Because a backslash escaped character is emitted as part of the backslash processing in the parser, any other processing of that character by the parser is effectively short-circuited. This simply allows the punctuation character to be represented without the parser mistaking it for any kind of processing instruction. For example, to include the text &amp; in your document the sequence \\&amp; can be used to escape the ‘&‘ character. 1 Another example is that the following text: \\ - this is a paragraph will generate the text - this is a paragraph as part of a paragraph, instead of creating a list item containing the paragraph this is a paragraph . In both cases, the backslash escapes are used to tell the parser to just treat the escaped character as itself and not to perform any further processing. As useful as that is, backslashes escapes cannot be used in code blocks, which have been covered previously, code spans, which are covered later in this article, or autolinks and raw HTML, which are covered in a future article. Implementing support for backslash escapes was simple, as it just required a change in how the characters were interpreted. As the text was still contained within a single text block, it was just a matter of making sure the right characters were emitted. This was relatively easy as the processing was easy: is the next character a backslash? if not, emit a backslash and resume normal processing if so, check to see what character follows if that character is not in the escape list above, emit a backslash and resume normal processing if so, consume that character and emit that character Basically, if there is a valid backslash sequence, emit the second character and consume it, otherwise, emit first character (the backslash character) and continue. The limits on where backslashes can be used was easy to implement, as there were only a few places where they were not allowed. Character References Character references are an important part of HTML, and as such, Markdown has solid support for them built in. Character references allow for the inclusion of special characters into the document, without the need to rely on the file editor to support Unicode characters. In addition, the document's writability and readability is often enhanced by presenting the reader with the text &copy; instead of the symbol ‘©‘. Think about it this way. As a document author, you want to add the copyright symbol to your document. Where do you find it on your keyboard? If it is not there, what is the clearest and easiest way to add it to the document that is not tied to a specific editor? Markdown addresses this issue by reusing the HTML5 method of specifying character references. For each character reference, it starts with the ‘&‘ character and ends with the ‘;' character, with characters between to denote the type of character to reference and what the actual reference is. Named character entity references are the easiest to read, as they contain some form of the name of the character they represent, such as &copy; for the copyright symbol. The full list of named character references that are supported is at the HTML5 entity names document . 2 As an alternative to the &copy; named reference, the equivalent numeric references &#169; or &#x00A9 may be used instead. While the result on the rendered page is the same, I feel that the named references are more readable than the numeric references. However, in cases where there is no named reference for a given Unicode character, the numeric references are very handy. Like the way in which backslash escapes are handled, there are certain blocks that the character references cannot be used in. In particular, they are not recognized in code blocks and code spans but are recognized in most other locations. For example 3 , given the following Markdown text: ``` f & ouml ; & ouml ; f & ouml ; & ouml ; ``` the character references in the fenced block info string are recognized, but the character references within the code block are not recognized. As such, after translating this Markdown into HTML, the following HTML is expected: < pre >< code class = \"language-föö\" > f &ouml;&ouml; </ code ></ pre > In the example, as expected, the character references that feed the class attribute for the code tag were translated, while the character references within the bounds of the code tag, which are used to denote a code block, are left alone. Similar to my experience in processing the backslashes, the implementation for all three-character references were processed in roughly the same manner. Instead of a single character to look for with backslash escapes, character references have a set of allowable character sequences, but otherwise the processing is the same. Once again, the processing was simple, just follow simple rules. However, while it was not particularly difficult, determining the proper handling of the entities.json file used as a reference for HTML named entities took a bit of thinking to get right. The main decision was whether to download it each time, cache it somewhere once downloaded, or just do a \"one-time\" include of it into the project as a resource. In the end, I decided to take the later path, placing the file in the pymarkdown/resources/ directory. My assumption is that file does not change that often, perhaps once a month at its worst. As I added the file exactly as it was downloaded from the source at the HTML5 home page , I believe I can check on it from time to time, updating the file when required. With that decision made, I just needed to do some research on the best way to include resources into a project, and the rest was once again just following well documented instructions. Code Spans Code spans are like code blocks, in that they both protect the characters that are within their confines. However, while code blocks are designed to protect multiple lines of text, such as source code examples, code spans are designed to protect text within a single paragraph. To create a code span, the text to be protected is simply surrounded by one or more backtick (‘`') characters on each side, making sure that the number of starting backticks and closing backticks are the same. As a simple example, the Markdown `foo` produces the text foo within a special HTML tag that has special styling associated within it. Like how code blocks protect blocks of text that are already formatted in a specific way, these code spans use that styling are used to specify targeted text that already has meaning attached to it. In my articles, as with other blog authors that I have read, I use code spans to indicate that certain strings have literal meaning to them, such as the literal text to type in at a keyboard. One good example of this from the previous section are the examples of the various Markdown sequences needed to produce the copyright symbol. If I had simply added the text &copy; to the Markdown document, it will be interpreted as a character sequence, and the ‘©‘ symbol will be generated. By placing backticks around that text, such as `&copy;` , those characters are contained within a code span and the text is preserved literally. And for that last sentence where I needed to include the literal text including backticks, I just made sure to include more backticks around the text than were contained within the text, such as `` `&copy;` `` . 4 I knew that the parsing and rendering of the tokens was about to get more complex in order to properly implement the code span processing. To keep the code span, the text before it, and the text after it in the right order, I changed the inline parsing to allow for a markdown token to be emitted. When the new code span Markdown token is emitted, the surrounding code first adds a new text block containing any text collected up until that point, emits the new token, and then resets the collected text back to the empty string. This correctly ordered the tokens and is generic enough to hopefully future-proof similar parsing in the future. There were only a small number of issues with the existing scenarios that needed to be addressed, now that code spans were handled properly. Fixing those tests was simple and just required resampling the parser's output. But during that testing, I realized I had made a mistake with the handling of one of the header blocks. When I wrote the original code for the Atx Header blocks, as documented in the article on Parsing Normal Markdown Blocks , I hadn't thought about code spans or other more complex inline elements as part of an Atx header. As such, I therefore I wrote a simple implementation that represented the header text as a simple string within the token. Double checking the specification, I verified that there were no restrictions on using code spans within a SetExt or Atx header block. As such, I needed to rewrite the parsing code to support having Atx header blocks contain text blocks, instead of simply including the enclosed text in the Atx Markdown token. Instead of tackling that as part of this group of code, I decided to look to see if there were any other \"little\" things that I missed, and I found a few of them. Basically, of the issues that I found, most of them were small variations of the scenarios, things that just got lost in the shuffle or lost in the translation. As such, I thought it would be best to take some time, try and note them all down, and then tackle them together before continuing. As the only scenario test that was affected was example 339, I believe that temporarily skipping that test and taking the time to fix those issues was the right call. It would mean that I would have to wait a bit before I could say that code spans were done, but when they were done, I would know that I did them the right way. That was, and still is, important to me. What Was My Experience So Far? I usually read a specification thoroughly and identify most of the edge cases on my first pass. However, I must admit that I dropped the ball with that on this project. And to be totally honest, I do not expect that it will be the last time either. It is a big specification, and there are going to be hits and misses along the way. What matters to me is not whether I make the mistakes, but that I do not have enough use cases, scenarios, and tests to help me identify any mistakes. With 673 scenarios already identified in the specification, I know the coverage for scenario will be good, but there will be gaps that I will still need to address. Whether it is my dropping the ball or the specification dropping the ball, the work on the these three inline elements has improved my confidence that I am prepared to deal with any such issues that come up. A good example of this is my reading of the specification around the use of Atx headers. I know I missed the part where the specification, in the preamble to example 36 says: Contents are parsed as inlines: In retrospect, not only is this one of the few times inlines with Atx headers was mentioned but there is also only one scenario that covers them, example 36. So, from one point of view, the specification could have more scenarios dealing with inlines and Atx headers. From an additional point of view, it was mentioned and I just missed it. From my personal point of view, it does not matter either way. What matters is that I had enough process and tools in place to catch it. And once I saw that issue, it helped me take a deeper look at some of the other tests, finding small issues with the output from those tests. From a quality point of view, my confidence was holding steady or increasing. As I mentioned a couple of paragraphs ago, I do not expect to be perfect, I just hope to have the right tools and processes in place to help me figure out when I miss something or get something wrong. Sure, I realized that taking some time to work on fixing these issues was going to put my work on the linter on hold for another week. But my confidence that the linter was on solid footing increased because I found some issues. For me, quality is not about being perfect, it is about movement in the right direction. And finding those issues, was a step in that right direction. What is Next? After documenting those issues at the end of the test_markdown_list.py file, I thought it was best to do a quality pass and resolve those issues before moving on to other inline processes. As such, the next article focuses on what bugs I found in the scenario tests, and how I addressed them. Just to be complete, the character escapes in the next section also provide a way to include the ‘&‘ sequence in Markdown. Using character references, this is by using the text &amp;amp; instead of \\&amp; . While both produce identical output, I prefer the first for it's clarity. Your mileage may vary. ↩ To keep things simple for parsers, this file is maintained as a JSON file that is easily interpreted with a small amount of code in most current languages. ↩ Note that this example is a slightly modified version of example 330 from the GFM specification. ↩ For a good example of this, see example 339 in the GFM specification. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/02/24/markdown-linter-starting-inline-processing/","loc":"https://jackdewinter.github.io/2020/02/24/markdown-linter-starting-inline-processing/"},{"title":"Markdown Linter - Taking Time to Refactor -- Post-Block Implementation","text":"Introduction The title of the article is not very glamorous, but it describes the changes I made to the project after the block processing and before the inline processing. From a project completeness viewpoint, all the block elements were done except for table blocks and link reference definitions, and those were temporarily shelved. The big decision before me was whether to plow ahead with inline processing or take some time to clean things up before continuing. After weighing the options in my head for a while, I decided to take some time to tidy up my work on my PyScan script and document it in this article . Part of that decision was based on the time of year (it was the holiday season) and the other part of the decision was based on timing for the PyMarkdown project. At this point, the blocks were mostly finished, and the inline processing was the next feature to be implemented. To me, it just made good sense to clean up the PyScan tool, write an article or two on it, and refactor some of the PyMarkdown project before moving forward with inline processing. What Is the Audience For This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 20 December 2019 and 31 January 2020 . Refactor #1: Extracting Function Groups The first things that I wanted to refactor were the generic helper functions used for parsing and the helper functions used for dealing with HTML. While the parsing helper functions were already at the end of the tokenized_markdown.py file, it made sense to move the HTML helper functions down to the same location at the end of the file. Once that was accomplished, it took me about 2 milliseconds to figure out that they should be in their own modules. Hence, the parsing helper functions were moved out into the parser_helper.py file and the HTML helper functions were moved out into the html_helper.py file. Along the way, proper unit tests were added for these functions. As part of the normal process of parsing the Markdown document, they had been battle tested by their usage, but having their own dedicated unit tests was the right thing to do. The unit tests for the parsing helper functions were all added with filenames that are the string test_ followed by the name of the distinct function that they test. As the HTML helper functions more tightly coupled that the parser functions, I kept their unit tests coupled by added all of them to the test_html_tags.py file. This refactoring was performed to reduce the complexity and maintenance of the main module. By moving these functions to well-defined modules of their own, I instantly found that it was easy to find functions in either module, instead of search for them at the end of the main file. For me, that feedback is always a good sign that the refactor was the right thing to do. Refactor #2: Reducing Complexity with is_character_at_index_one_of As I was looking through the code for the previous refactoring, I noticed that there were a few functions that were \"too big\". From experience, I find that these types of functions usually have more than one responsibility, and reducing those responsibilities reduces their complexity. The first example of this that I found was the is_fenced_code_block function, which included the following series of lines: if ( ( len ( extracted_whitespace ) <= 3 or skip_whitespace_check ) and start_index < len ( line_to_parse ) and ( line_to_parse [ start_index ] == \"~\" or line_to_parse [ start_index ] == \"`\" ) ): While the first two lines were specific to the function, the last two lines followed a pattern that happened again and again in that module. Because those last two lines are really checking to see if the next character is one of the two values, I extracted that logic into a new is_character_at_index_one_of function for the ParserHelper module, and added tests into test_is_character_at_index.py module. This refactoring had a noticeable impact on the complexity of each of the modules that used the new function. This impact was a reduction in the number of branches in each function, with each count decreasing by one for each character to look for. As an example, the is_fenced_code_block code above went from the snippet above to the following: if ( len ( extracted_whitespace ) <= 3 or skip_whitespace_check ) and ParserHelper . is_character_at_index_one_of ( line_to_parse , start_index , self . fenced_code_block_start_characters ): That is a reduction from 5 branches down to 3 branches, making that function and that module less complex in the process. In addition, instead of reading those two lines and trying to figure out what they are doing, the function call to the is_character_at_index_one_of function eliminates the \"what is it doing\" step, making it easier for someone reading the code to understand those lines. Refactor #3: Simplifying The close_open_blocks Function This refactoring was a simple one, but for me it had a noticeable impact on helping me get a clearer understanding of the function. Prior to the change, the close_open_blocks function had 2 distinct responsibilities: determine if the element on the top of the stack needed to be closed, and if so, remove that element from the top and close the block. While I was able to read the function and use it properly, I often had a little question in my head about whether I was using the function properly. After the refactoring, the code containing the first responsibility remained in the close_open_blocks function and the code for the second responsibility was placed in the new remove_top_element_from_stack function. When I looked at those two functions during the writing of this article, I was able to see a very clear picture of what each function is doing, with clear delineations of those responsibilities. The close_open_blocks implements a while loop with 4 distinct ways to exit out of the loop, and the remove_top_element_from_stack function remove the top element, adding the appropriate tokens to the document's token stream. Clear and concise, hence easy to read. This function is core to the processing of the blocks and making it clearer was important to me. While it was a small refactor, it increased my confidence that the function, and any functions that called it, were operating properly. I believe that my confidence increased because it went from one \"messy\" function to two separate functions with very clear intentions. By rewriting the code into two functions and keeping each function simple, the messiness vanished. Refactor #4: Cleaning Up the determine_html_block_type Function I will admit, the next candidate, the determine_html_block_type function was a mess. At 88 lines long and 24 branches, it was clearly a function with too many responsibilities. Like the work documented in the previous section, I started to take a look at this function and try and figure out what it was doing. When I finished, I came away with three responsibilities that the function was performing: handling the special case (html block types 1 to 5), handling the normal cases (html block types 6 and 7), and some cleaning up of the results for html block type 7. That was two responsibilities too many. Similar to the work above, the determine_html_block_type function was broken up along the identified lines of responsibility. The check_for_special_html_blocks function was created to handle the special cases, the check_for_normal_html_blocks function was created to handle the normal cases, and the determine_html_block_type function contained orchestration logic for calling those two functions, plus the special cleaning up for the html block type 7 logic. While this function is not as core to the parser as the close_open_blocks function, its refactoring had a similar effect. Each of the added functions contained a single responsibility, this making the usage of all three functions together easy to determine. For me, that was good progress. Refactor #4: Clearing PyLint warnings During the writing of this article, the first thought that came to mind when writing this section was that I should be ashamed that it took me until this point to address the PyLint warnings on the project. Taking a bit of a deeper look into how I felt about this, I believe it had to do with where I draw the line between \"just playing around\" and \"real code\". For me, I believe this transition is when a project moves from a Proof-Of-Concept project to a Real™ project. I am not 100% sure, but I believe that it was at this point, give or take a couple of days, that I felt that this was a real project. While it is hard to pin down why, I believe that having the block section of the specification done helped my mind crystalize that the project is going to happen. It was as if someone had whispered \"This is going to happen\" in my ear, and that I needed to tidy things up. Once I figured that out, it just felt like a natural transition, nothing to be ashamed about. Now that this was a Real™ project, I needed to ensure that any PyLint warnings were addressed or suppressed. While I prefer to address these issues, some of the warnings, such as the too-many-arguments warning, occupy one of my grey areas. Especially with parsers, a lot of state information needs to be passed around, to ensure the parsing is performed properly. This often results in functions that take too many arguments. At this stage of the project, I decided to suppress those warnings, with a number of too-many-locals warnings until later in the project, when I have a better sense of how to optimize those function calls for this parser. This refactoring helped me remember an adage a friend taught me about software: \"It isn't if there is a problem with your code, it is a question of how often the problems within your code occur.\" More of a realist than a pessimist, he figured that each line of code brought a new set of issues and bugs with it, and it was our job to discover and handle those issues that our users would find before they found them. For me, it was a good refresher in humility when developing software. Refactor #5: Reducing Complexity with the \\ at_index\\ Functions Ever since the section above on parser functions , I had been slowly searching for other patterns that I could refactor. In the process, I found a group of patterns that were not complex, but would benefit from a small refactor. Basically, a refactoring of that pattern wouldn't make a lot of difference in reducing the number of branches, but it would reduce the complexity of the functions by making them easier to read. The group of patterns that I found all centered around finding out whether a character or a string was at a given location in the string. Specifically, the parser contained three of these patterns that I felt were worth extracting into their own functions: is_character_at_index , are_characters_at_index , and is_character_at_index_not . None of these functions would facilitate a large improvement, but the change from the following text: if ( start_index < len ( line_to_parse ) and ( line_to_parse [ start_index ] >= \"0\" and line_to_parse [ start_index ] <= \"9\" ) ): to this text: if ( ParserHelper . is_character_at_index_one_of ( line_to_parse , start_index , string . digits ): produces more readable code by simply stating the intent of those lines, instead of leaving the reader to interpret them. While I admit that it was not a big change, to me this refactoring provided some extra confidence that the project was getting to a cleaner place. Sometimes refactoring produces big, measurable impacts, and sometimes they produce little ripples that are barely noticeable. However, sometimes those little ripples can mean a lot, and worth a lot. Refactor #6: Increasing Code Coverage After a few of these parsing refactors, I noticed that the PyScan numbers for code coverage were in the high nineties, which is very good for a number of projects. However, in a project that was designed from the ground up for high code coverage numbers, such as the PyMarkdown project, there is almost always room to do a bit better. In the case of the implementation of the HTML blocks, I implemented defensive programming to try to ensure that edge cases were protected against. In re-reading the HTML block specification a couple of times, the focus of the specification seemed to be focused on the main use cases, not the edge cases. As such, the code coverage report gave me good input on how to add 4 new use cases that helped ensure that the edge cases for HTML blocks were fully covered. This type of refactoring is a difficult one for me to justify to some people, but I feel strongly about it. The justification centers around what level of code coverage is considered \"good enough\". For myself, there are 2 main factors that weigh into my decision on what is good enough with code quality: was the project designed with testing in mind and what is the effort required to address the next issue. In this case, as minimal effort was required to add the 4 simple scenario tests to address the issue, I would easily argue that it was not good enough. From my point of view, the small cost easily justified the benefit. Refactor #7: Translating Token Strings to Actual Tokens Having made the jump in my head from a Proof-of-Concept project to a Real™ project, I decided it was time to change the stack tokens and Markdown tokens from simple text strings to actual token objects. Up to this point, I was more concerned that the tokens looked right in the output stream, and there were only relatively few cases where that output needed to be interrogated later. With inline processing on the horizon, which would heavily make use of token content, it made sense to me to undergo this change before the inline processing started. The two places where I had made this tradeoff were the stack tokens and the Markdown document tokens. The stack tokens denote where in the processing the parser is and the Markdown document tokens denote what was found during the processing. In both cases, it was more important to me to see the right patterns being parsed than to tie them down to a given structure. Based on experience, I wanted to do the least possible work to get to this point, and then have the structure for each object emerge. For the StackToken object, the structure that emerged was simple. Each class is responsible for any of it's own variables, but also for providing a read-only, text version of these variables, assigned to the base class's extra_data variable. In this way, the base class can include a number of the useful functions without requiring any knowledge about the child classes. By implementing the __str__ , __repr__ , __eq__ , and generate_close_token in this way, each child class was kept very simple and straightforward. In addition, instead of using Python's isinstance function to figure out the type of token, I added is_* methods for each token type to make the code referencing the tokens more readable. The refactoring for the MarkdownToken object was almost the same as for the StackToken object, but with a couple of key differences. With the StackToken , the token itself was the focus of the object, whereas with the MarkdownToken , it is the data contained within the token that is key. The other big difference is that MarkdownToken objects are the artifacts that will be consumed and analyzed by the PyMarkdown project, not just an internal representation. As I had a lot of positive success with the design and use of the StackToken class, I modelled the MarkdownToken class in a similar fashion, keeping in mind the differences and altering the design to properly accommodate them. From a design point of view, things did not change things that much, but I needed to make sure those objects look and function right, as they are very visible. This refactor was a long time coming, but I felt that it was the right time to do it. As I mentioned in previous sections, the project felt more like a Real™ project and not a proof of concept. With a good bulk of the parsing completed, and with a solid opinion of how I was going to orchestrate the remaining pieces, it was the right time to nail down how those tokens would look to users of the project. While I could have done that earlier in the project, I believe that I would not have been able to do so with the same confidence that I made the right choice. For this project, I believe that leaving the tokens in the raw form to this point was the best move possible. Refactor #8: Consolidating Text Blocks Of all the refactors that I have talked about in this article, this refactor was the one that I really needed to do. Inline processing addresses the group of features that expand on Markdown text within the blocks, and a lot of those processes assume that the text within their blocks is one long string to process. Currently, the text tokens were distinct and disjoint, each one added in the order they were processed. To get ready for inline processing, those text tokens needed to be consolidated. There were two possible paths to take to accomplish this: deal with the processing as the text tokens were added or deal with them in a subsequent processing step. As I want to keep the processing logic as simple as possible, I decided that a follow-up step to consolidate those tokens was the best course of action. To accommodate this change, I added the coalesce_text_blocks function to simply go through the document tokens, look for 2 text tokens beside each other, and append the second token's text to the first token. Then, in the transform function, instead of just returning the results from the parse_blocks_pass function, those results were passed to the coalesce_text_blocks and those were returned. While this change was a relatively small change, it impacted the token output for a lot of the test cases. In a meaningful way, that impact increased my confidence that tackling it was the right choice to complete before inline processing started. The impact of the change on the test cases validated that it was a far-reaching change, one that was better to have happen before the next stage of processing. What Was My Experience So Far? Unlike the other articles in this series, this article was about how I took a bit of a breather and focused on improving the quality of the PyMarkdown project. As the next set of features involves inline processing of text blocks, I believe whole heartedly that taking that break to focus on refactoring increased my confidence that I was on the right track with the parser. Why do I feel that way? Looking into the near future, I know that inline processing will increase the complexity to the project, and any effort to reduce the project's complexity ahead of that will directly help reduce the complexity of the inline processing. Further into the future, there are extensions to Markdown that I will need to add that will also increase the complexity of the project. Add to that my plans to comply with other Markdown specifications, such as the CommonMark specification, which will also increase the complexity. Why refactor? I want to keep the project simple and uncomplicated. From a software quality point of view, each refactor makes the project simpler and more uncomplicated. While some of the changes did not move the needle on the software quality meter much, each change helps. In the end, I refactor projects to keep them simple. As Einstein said: Make everything as simple as possible, but not simpler. What is Next? Having teased the addition of inline processing to the project for most of this article, the next article will be on the implementation of the first 3 aspects of inline processing that I tackled.","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/02/17/markdown-linter-taking-time-to-refactor-post-block-implementation/","loc":"https://jackdewinter.github.io/2020/02/17/markdown-linter-taking-time-to-refactor-post-block-implementation/"},{"title":"Markdown Linter - Adding HTML Blocks","text":"Introduction Having completed most of the Markdown block elements, as documented in the last two articles on leaf blocks and container blocks , I wanted to go back and revisit the HTML blocks that I deferred. For anyone following this series, in the Stopping At A Good Place section of the \"Parsing Normal Markdown Blocks\" article, I determined that there were 3 types of leaf blocks that would be difficult to implement, so I deferred them. Between my lack of use most of those deferred features and my distinct status as the first user of the parser, I thought this was a decent trade off in the short run. With increased confidence from implementing the other block types, I thought it was a good time to deal with this block type. Before continuing, I believe it is important for me to highlight some information about HTML blocks in Markdown. I have never needed to use HTML blocks or raw HTML (covered in a later article) in any of my own Markdown documents. Quick research revealed that there are some interesting cases where injecting HTML blocks is a benefit. However, that same research also noted that allowing either type of HTML in Markdown is a potential security issue, and as such, may be disabled for a given Markdown-to-HTML generator. Regardless of my usage patterns or security patterns, I wanted to be sure to include it in the PyMarkdown project for completeness. What Is the Audience For This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commit of 20 December 2019 . This work includes implementing the parsing logic for the HTML Blocks as documented in the GFM specification and implementing the parsing to pass all the scenario tests for HTML blocks that were previous entered. Why Did I Defer HTML Blocks? The HTML blocks were one of those 3 deferred types because a quick reading of the specification lead me to believe the HTML block implementation would be tricky. Why? Take a minute and read or browse the HTML blocks section of the GitHub Flavored Markdown (GFM) Specification. Don't worry, I'll wait while you do that. Done? What did you think? My initial read of the specification made me think that it was going to be a complete mess to figure out properly. After a walk to clear my head, I took another look at the section. When I factored in the work, I did to implement the container blocks, this section looked like it would be tedious, but not too bad. After handling container blocks within container blocks, the straight parsing of a leaf block would not be too bad. Right? I admit, it still looked kind of daunting to me. From my observations, the 42 use cases for the HTML blocks was far more than the count for all of the other block groups 1 except for the list items group, at 47 use cases. And yes, the count is mostly larger than the count for container blocks (with block quotes at 22 use cases) and one half of the use cases for list blocks (with lists at 25 use cases and list items at 47 use cases). The data backed my daunting feeling up, which was a relief. Thinking about how I got to that feeling, I realized that in reading the specification, I was telling myself a story about how hard it would be to implement based on the sheer number of use cases. So how was I going to change that narrative I was telling myself? Changing the Narrative In the last article , I mentioned that one of my family's favorite sayings is: \"Stuff happens, pick yourself up, dust yourself off, and figure out what to do next.\" \"Stuff happens\" was the decision to defer the HTML blocks, \"pick yourself up\" was the decision to pick them up again, \"dust yourself off\" was figuring out why I deferred the blocks, leaving the \"figure out what to do next\" part. One of my favorite tools to figure out what to do next is to see if I can change the narrative, or story, with whatever it is that I am doing. Why is this important? There are facts and opinions that are part of every story. Did the main character go to the cantina before boarding the freighter with the smuggler? That is a fact. Whether or not the captain of that freighter is a smuggler can be an opinion, depending on supporting facts. How much trust the main character had in that smuggler when boarding the freighter is mostly an opinion. The closer something is to a fact, the harder it is to change. Opinions can be changed in many cases if you can find the right story to tell. The HTML blocks having 42 use cases to define its behavior is a fact, and facts do not change easily. Taking a deeper look at the 7 categories at the start of the specification's section on HTML blocks, I can make a good argument that there are 3 sets of HTML tags instead of the 7 presented: the meta tags, the special tags, and everything else. Furthermore, the first 20 use cases present general cases while the next 18 use cases, and the last 4 use cases talk about specific rules and why they were specified. Given this information, I can change the story I am telling myself by breaking down the previous story into smaller stories, each with a specific focus. Instead of one group of 42 use cases, I can have 3 smaller groups: 1 for general HTML blocks with 20 use cases, 1 for specific HTML blocks with 18 use cases, and finally a \"wrap-up\" group of 4 use cases that better explains why the specified rules are important. Why is this better? At 42 use cases for HTML blocks, it is the second biggest block of use cases, and is somewhat scary. Breaking that group up into 2 groups of about 20 use cases followed by a small group with 4 example use cases is something I can comprehend better, implement better, therefore removing my concerns about the large scope. In addition, experience has taught me that when translating use cases to scenario tests, the last 2 to 3 translations are frequently show-stoppers or require major reworking to properly translate and get working. With a big group of 42 use cases, I know I would be expecting that behavior to happen, with a large amount of rework to do when it happened. After breaking down the problem into the 3 smaller groups, I was somewhat confident that if the same situation occurs, the amount of rework will be limited to approximately 20 scenario tests. For me, reducing that perceived effort helped me keep my confidence up instead of having it take a hit. Instead of \"when it happens\" with the 42 use cases, it became \"if it happens\" with the smaller groups of 20 use cases. Let the Implementation Begin! With a boost to my confidence in place, I was able to get a decent amount of work completed on the HTML blocks, wedged between shopping and work during the end of the holiday season. Despite my initial concerns about the size and complexity of this feature, the development went smoothly. Given how it went, I believe it lends support to my opinion that breaking down the use cases into the 3 groups was the right thing to do. For those not familiar with Markdown and HTML, there some basic rules for HTML blocks, and then the 3 categories of HTML blocks themselves: the meta tags, the special tags, and everything else. The basic rules are simple. HTML blocks are always started with tags that start at the beginning of a new line, and once the start condition is met for one of the 7 block types, only the matching end condition finishes off the HTML block. In some cases, the end conditions can be met on the same line, and in some cases, the end conditions make sense… and in some they do not. At least not without understanding the rules! Meta Tags Block type 1 contains what I refer to as the \"meta tags\", because those tags usually contain information that is at a higher level than normal tags, such as script information or style information. For anyone familiar with authoring HTML, the Markdown interpretation of these tags is almost the same as in a raw HTML document. The start condition is that one of the strings <script , <pre , or <style is present, followed by whitespace, the string > or the end of the line. The end condition is that one of the strings </script> , </pre> , or </style> are present, though the tags specified in the start condition and end condition do not need to match each other. As such, the following text is considered a complete HTML block: <style type= \"text/css\" > h1 { font-size: 140%; font-weight: bold; border-top: 1px solid gray; padding-top: 0.5em; } </style> as is: <script src= \"jquery.min.js\" ></script> and: <script src= \"jquery.min.js\" ></pre> Note that in the last example, while the Markdown specification considers it a complete HTML block, it is not a valid HTML snippet. The Markdown specification does not specify any validation of the produced output, so beware of garbage-in, garbage-out. This HTML block type was easy to figure out, hence it was easy to implement. Pretty straight forward: Look for one of the start strings, capture everything until we find one of the end strings. Quick and painless. Special Tags Block types 2 to 5 are what I refer to as the special tags. In order, the HTML specification refers to these as the comment tag, the processing instruction tag, the declaration tag, and the CDATA tag. Each of these tags is authored exactly as would be expected in a normal HTML document and has its own distinct purpose. In each case, the start condition is a simple string, and the end condition is the inversion of that string. While most of these tags have seldom used or esoteric purposes, the comment tag is used frequently in HTML code, and is common in HTML documents. Similar to block type 1 above, the following text is considered a complete HTML block: <!-- style type=\"text/css\"> h1 { font-size: 140%; font-weight: bold; border-top: 1px solid gray; padding-top: 0.5em; } </style> --> as is: <! -- this is a comment --> Like the previous HTML block type, these HTML block types were also easy to figure out and implement. Just like before: look for one of the start strings, capture everything until we find one of the end strings. Just as quick and just as painless. \"Everything Else\" Tags With block types 1 to 5 out of the way, the work focused in on the remaining block types 6 and 7. These two block types are different than the other blocks, with their most prominent difference being that their end condition is a simple blank line. Another difference is that there is a long list of tag names that are eligible for block type 6, while any other tag is relegated to block type 7. This becomes important as the start conditions of block type 6 are the string < or </ , followed by the tag name, and then followed by whitespace, the string > , the string /> or the end of the line. In contrast, the start conditions for block type 7 are that the HTML must either be a complete open tag or a complete close tag, followed by optional whitespace and the end of the line. As an additional requirement, a block type 7 HTML block cannot interrupt a paragraph. To me, these rules are confusing to anyone authoring even a small piece of HTML in Markdown, adding to the reasons for me to suggest to people not to use HTML in Markdown. While this confusion is not evident in the examples for the block types 1 to 5, consider this sample: <canvas class= \"my-canvas\" > <pre> **Hello**, _world_. </pre> </canvas> and this sample: <table class= \"column\" ><tr><td> <pre> **Hello**, _world_. </pre> </td></tr></table> Without looking at the information in the specification, how easy is it to tell what the output of each sample is? To be honest, I had to refer back to the HTML block definitions in the GFM specification twice when I was writing these samples and three times when I was verifying the samples before publishing this article. That does not bode well, does it? For the first example, the canvas tag name is not in the list for block type 6, and a block type 7 evaluation fails as the tag is neither a complete start tag nor a complete end tag. As such, the canvas start tag ends up being normal text, to be wrapped in a paragraph. The next tag, the pre start tag, gets identified as a block type 1 start, finishing at its own pre end tag, with the remaining canvas end tag going into its own paragraph. I know that was not what I expected at first glance. The second example has different issues. Because the table tag name is in the block type 6 list of allowable tag names, the start conditions only state that it needs to start with the first part of a start tag or end tag, which the string <table satisfies. However, as the end condition for block type 6 HTML blocks is a blank line, the HTML block ends after **Hello**, and before _world_. . At this point, the text _world_. is parsed as normal text, and the text </pre> is interpreted as a complete end tag by the block type 7 rules, carrying a block type 7 HTML block to the end of the sample. When reading a similar example as part of example 118 , it did take several tries to figure out what was going on. These block types provided a bit of complexity that was different than the previous blocks. As such, I hit a couple of roadblocks that I had to work through. It wasn't that the implementation was much more complicated than the previous HTML block types, they weren't. It is almost the same process: find one of the start conditions, and capture everything until a blank line. Sure, the start conditions were a bit meatier, but other than that, it was relatively simple. It was that they start conditions and end conditions were different for these 2 HTML block types that made me look back at the use cases and scenario tests with a couple of \"huh\"s until I that difference registered in my head. And that list separating HTML block type 6 from 7… sheesh. My Recommendation When it comes to HTML blocks, I implemented them as part of the parser because they are part of the specification. But because of the complexity in understanding HTML blocks, I whole heartedly recommend avoiding using HTML blocks if possible. What Was My Experience So Far? I took my time with the implementation for HTML blocks due to the complexities stated above. For the most part, the code I implemented worked on the first or second try, with few cases where it took more tries and debugging than that. I believe the key to the relatively easy implementation was breaking the groups and tasks down into multiple, smaller groups and smaller tasks. In retrospect, I believe this enabled me to more readily get my mind around the task to accomplish, and not get overwhelmed by the size of the problem. Implementing that thinking for the project, while not concrete, helped me see other things for the project in a better perspective. Most of the things I initially thought would be complex turned out to not be that complex. The long list of tag names for block type 6? Strings in a list object. The end conditions? Either looking for a blank line or one of a set of strings in one of the following lines. Getting the use cases right in the scenario tests? Really simple. I still contend that authoring HTML in Markdown is complex, but the implementation was easy. Another boost to my confidence was tackling the HTML blocks and getting them out of my \"technical debt column\". While I believe that I made the right decision to defer the HTML blocks for the right reasons, it still felt good to get them dealt with. Like my experience with translating the last 2-3 uses cases into scenario tests, thinking about revisiting any technical debt also triggers similar expectations of the reworking of existing code, if that revisiting is actually possible at all. Taking something out of technical debt and being able to remove that uncertainty helped my confidence towards the completion of the parser for this project. All in all, I believe things are still headed in the right direction! What is Next? During the implementation of the PyMarkdown parser, I have been using my PyScan script to great benefit. As such, I decided to take the time to polish it up a bit and document it in this article on Software Quality. While doing that, I took some time to refactor the PyMarkdown code to make it easier to work with, preparing it for the inline processing that was to come next. The next article will go over the refactoring that I did, and how it helped the project. The totals are as follows: paragraphs (9), tabs (11), indented code blocks (15), atx headings (18), thematic breaks (19), block quotes (22), lists (25),setext headings (27), and fenced code blocks (29). ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/02/10/markdown-linter-adding-html-blocks/","loc":"https://jackdewinter.github.io/2020/02/10/markdown-linter-adding-html-blocks/"},{"title":"Markdown Linter - Adding Block Quotes and Lists","text":"Introduction Having the leaf blocks mostly in place, as documented in the last article , the next items on the implementation list were the list blocks and the block quote blocks. These Markdown blocks, referred to as Container Blocks in the GitHub Flavored Markdown (GFM) Specification , are the more complicated blocks to deal with, as they are capable of containing other blocks. As there are specific suggestions on how to parse these blocks, my confidence took a hit when I started looking at this section. My viewpoint: if the specification writers thought it was difficult to implement that they wrote suggestions on how to handle it, it must not be as easy as the leaf blocks! What Is the Audience For This Article? While detailed more eloquently in this article , my goal for this technical article is to focus on the reasoning behind my solutions, rather that the solutions themselves. For a full record of the solutions presented in this article, please go to this project's GitHub repository and consult the commits between 08 December 2019 and 15 December 2019 . This work includes creating the scenario tests for all the Container Blocks as documented in the GFM specification and implementing the parsing to pass most those tests except for the nested cases. Container Blocks, Leaf Blocks, and Interactions (Oh My!) Before container blocks, parsing was easy. A block starts, and when the parser encounters the termination conditions, it ends. There are a few rules about when blocks can start and end, such as \"An indented code block cannot interrupt a paragraph.\" , but for the most part, there is little interaction between the leaf blocks. The leaf blocks are clean and tidy. Not so much with container blocks. Container blocks, by their very definition, contain other blocks, namely leaf blocks and container blocks. While this makes certain visual elements easier, this also means specific rules about what interactions are allowed between the blocks. On top of that, as container blocks can contain other container blocks, testing is required to ensure that an arbitrary number of nested containers is properly supported. A great example of nesting container blocks is the Markdown implementation of sublists. A list containing a list containing a list is simple in Markdown: - first level - second level - third level That example is not a single list, but 3 separate lists. The first level list is the first level list, containing the list second level , which contains the list third level . And while sublists are a simple case of container blocks, more complex cases are possible, such as this one: - first level - ``` text my text ``` This list is like the first list, except it contains a fenced code block as the contained block. Both examples are just a few of the possibilities of how container blocks can contain other blocks. Looking through the specification, I quickly lost count of the number of combinations possible. Enter Lazy Continuations If the interactions between container blocks and the blocks they contain was not a fun enough exercise in mental agility, enter lazy continuations. From the GitHub Flavored Markdown (GFM) Specification's block quotes section : Laziness . If a string of lines Ls constitute a block quote with contents Bs, then the result of deleting the initial block quote marker from one or more lines in which the next non-whitespace character after the block quote marker is paragraph continuation text is a block quote with Bs as its content. and from the list items section : Laziness . If a string of lines Ls constitute a list item with contents Bs, then the result of deleting some or all of the indentation from one or more lines in which the next non-whitespace character after the indentation is paragraph continuation text is a list item with the same contents and attributes. Basically, what they are both saying is that if a paragraph has been started with block quotes or within a list AND if a line is clearly a continuation of a paragraph, then it is valid to remove some or all of the container block markers. For a more concrete example, example 211 has the following Markdown: > bar baz > foo which is parsed the same as if the following Markdown were written as: > bar > baz > foo After reading those sections and letting them sink in, my confidence took a dip. This was not going to be an easy concept to get right. But the sooner I dealt with those scenarios, the sooner I could try and implement them the right way. Knowing this, I went forward with the implementation phase of the container blocks. Getting Down to Work - The Easy Scenarios I often recommend to friends and co-workers that taking a break and doing something unconnected to the \"chore\" helps your mind get things together. As such, before getting started on this work, I decided to walk our dog for a while and let some of these concepts mull around in my head. I am not sure if it was the exercise or the change in scenery, but it helped to clear the cobwebs from my head and helped me to see things about the project more clearly. The big thing that it accomplished was to help me cleanly separate out the easy tasks from the more difficult tasks. The easy tasks? Simple block quotes and simple lists, including sub-lists. The difficult tasks? Lazy continuations and mixed container types. I remember feeling that taking this time helped my confidence on the project, as I was taking simple steps to understand where the difficulties were most likely to show up. This process also allowed me to think about those hard issues a bit while implementing the easier features. While I was not devoting any serious time to the more complicated features, it was good to just have my mind aware of which sections of code that I was going to need to keep flexible going forward. Keeping this in mind, I started with block quotes, adding the block quote test cases to test_markdown_block_quotes.py , disabling any tests that I figured were not in the easy category. I then proceeded to implement the code, in the same way as detailed in the prior article on leaf blocks . Implementing the easy scenario tests for the block quotes was a decent sized task, mostly completed during two days on a weekend where I had some time. This also included fixing scenario tests in 6 other test files that has block quotes in their scenarios. Working on the basic list items over the next week, by the middle of the next weekend they were completed, in a similar fashion to how the block quotes were completed: new scenario tests were added, the easy ones were then tested, enabled, and verified for completion, and the more difficult ones were disabled. Similar to the block quotes, getting this right took roughly a week, and that work also had impact on scenario tests other than the ones I added. During this process, I believed I found the parsing of lists more difficult. Thinking about the implementation in hindsight, I believe it was mostly due to their parsing requirements. The fact is that block quotes have a single character > to consider for parsing, while the lists can be unordered and start with the - or * character or the lists can be ordered and start with a number and the ) or . or character. In addition, for ordered lists, there is also the parsing of the start number and how to interpret it. Looking at the two blocks that way, block quote blocks seem a lot easier to me. However, now that I have had a bit of time since that code was written, I believe that those two features were closer in difficulty that I initially thought. Having implemented both block quotes and lists, I think that they both had something that was difficult that needed overcoming. Since I have done a lot of parsers in my past, the number of variations in parsing the lists were immediately noticeable to me, while the block quotes were easy to parse. Balancing that out, once parsed the lists were easy to coordinate, while the block quotes took a bit more finessing to get right. In the end, I believe it was a pretty event effort to get both done properly. At least until nested mixed container blocks. Nested and Mixed Containers Nested container blocks, specifically mixed nested container blocks, is where things got messy. To be 100% honest, I am pretty sure I did not get everything right with the implementation, and I already have plans to rewrite this logic. More on that later. I started implementing these features knowing that they probably made up the remaining 10% of the scenarios. I also figured that to handle these specific scenarios properly would require as much time and effort as the prior 90% of the scenarios. This was not really a surprise, as in software development getting a project to the 70-90% finished mark is almost always the easy part. Over the next week's work, I reset my fork of the code back to its initial state 3 or 4 times. In each case, I just got to a point where I either hit a block in going forward, I wasn't happy and confident about the solution, or both. In one of those cases, the code was passing the scenario tests that I was trying to enable, but it just did not feel like I could extend it to the next scenario. I needed to be honest with myself and make an honest determination of how good the code I just wrote was. In the end, I completed some of the sublists and nested block quotes, requiring only 4 scenario tests to be disabled or skipped. The ones that were disabled were the 10% of the 10%, the cases where there were 3 or more levels of block quotes and lists mixed together. I was not happy with it, but after a week, I knew I needed to move on with the project. Grudgingly, I acknowledged that I would need to rewrite this later. Why Rewrite Already? I am very confident that I coded the easy level cases correctly, as I have solid scenario tests, and a decent volume of them, to test the various use cases. For the medium difficulty cases, such as a container within a container, I have a decent amount of confidence that the scenario tests are capturing most of the permutations. It is the more complicated cases that I really am not confident about. And when I say I am not confident; it is not that I am not sure if it is handling the test properly: that is a binary thing. The test is passing, or the test is failing, and thus disabled. I am not confident that all those tests work for all use cases like that the scenario tests represent. Part of any project is learning what works and what does not work. As I started looking at implementing example 237 , I read the following paragraph located right before the example: It is tempting to think of this in terms of columns: the continuation blocks must be indented at least to the column of the first non-whitespace character after the list marker. However, that is not quite right. The spaces after the list marker determine how much relative indentation is needed. It was then that I was pretty sure I had coded the container blocks in terms of columns instead of spaces. Add that to the list of rewrites needed. The other category where my confidence is not high is with multiple levels of mixed container blocks. Once I complete the rewrite above, I can properly evaluate how well I can nest the containers, but at that moment it was not high. At that point, example 237 will be a good scenario test to determine how well I have those set up. Having taken some time to really evaluate the code and the scenario tests, I just have a suspicion that there is at least 1-2 bugs in the code that I wrote. For now, that is on my list of possible rewrites, with a medium to high probability of being needed. The saving grace for both of these scenarios that I believe need rewrites? Their frequency. The scenarios for blocks, leaf blocks and container blocks, comprise about half of the specification, ending with example 306 . According to my test failure report, only 4 of the list block tests had to be marked as skipped, hence they were not passing. At approximately 1.3% of the total scenarios, it is not a big impact. In writing this block, I have used lists frequently, block quotes sporadically, and block quotes with lists even less. I am not sure if my writing is representative of everyone's writing, but at least for now, it is a good place to start. What Was My Experience So Far? All the leaf blocks were finished in about a week. The easy and medium cases for the container blocks were finished about a week. The hard cases for the container blocks… not finished after a week, but close. Was I disappointed? Sure. But in comparison to other issues I have had with projects, this was not even near the top 20 in terms of disappointment. To be honest, in terms of how projects have gone for me over the years, this has been a decent project to work on. Every project has its issues, and this was just the set of issues that happened to occur on this project. I know it may sound a bit silly, but me and my immediate family have a saying we like to repeat when things get tough: \"Stuff 1 happens, pick yourself up, dust yourself off, and figure out what to do next.\" The disabled tests happened, so I took some time to find my focus, and came up with a plan to deal with it. Not a great plan, but it meant I could go forward with the remaining scenarios and circle back once I accumulated more experience with the parser. Sure there already was some technical debt for this project, but other than that, I believe it is going well. At this point it was just before Christmas, and I had a Markdown parser that was coming along well. My confidence in the implemented leaf blocks was high, as was my confidence in the easy 90% of the container block implementation. The more difficult 10% of the container blocks was still undecided, but I had a plan to deal with it going forward. While not a sterling situation, it was definitely a good position for me to be in. What is Next? Before I took some time to improve my PyScan tool, I worked on adding HTML block support for the PyMarkdown project. As HTML in Markdown has some funny logic associated with it, the next article will be devoted entirely to the HTML blocks. When my kids were younger, I did indeed use the word \"stuff\". As my kids got older, we changed that word to another one that also starts with \"s\". The actual word that we now use should be easy to figure out! ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/02/03/markdown-linter-adding-block-quotes-and-lists/","loc":"https://jackdewinter.github.io/2020/02/03/markdown-linter-adding-block-quotes-and-lists/"},{"title":"Markdown Linter - Parsing Normal Markdown Blocks","text":"Introduction With the project requirements , the test framework , and the test strategy in place, it was time to start working on the most frequently used and easy-to-parse Markdown items. These Markdown blocks, referred to as Leaf Blocks in the GitHub Flavored Markdown (GFM) Specification , are the root of many Markdown documents and have the virtue of being easy to parse. With small exceptions, each of the Leaf Blocks is self-contained. For the most part, those exceptions arise in how the Leaf Blocks interact with each other. In all cases, this interaction is small and does not require complicated logic to understand. The full record of the work detailed in this article is documented in the project's GitHub repository in the commits that occurred between 30 November 2019 and 05 December 2019 . This work includes creating the scenario tests for all the Leaf Blocks as documented in the GFM specification and implementing the parsing to pass all of those tests except for the Link Reference Definitions, HTML Blocks, and Tables. While the documentation of what needed to be done (GFM Specification) and what was done (GitHub commits) is straightforward, the \"hows\" and \"whys\" of what I implemented is worth talking about. The process that I followed for the implementation of the Leaf Blocks did not uncover any development issues during implementation. However, without giving too much away, the same process applied to other block types (to be talked about in future articles) did uncover issues that were not so easy to resolve. As there were complications that arose with those feature implementations, I wanted to provide a consistent documentation of the process from the beginning, to provide a complete picture of how things progressed. I firmly believe that it is always good to show the complete story of what happened, and not only one side of the story. Let's go! Moving Forward with Implementation Even though the first commit for processing Markdown elements is on 30 November 2019, my work on implementing them started on 25 November 2019. Based on the test framework and strategy documented in previous articles, the first thing to do was to write the scenario tests cases, even if most of those tests were initially disabled or skipped. This was easily done by annotating each test function with @pytest.mark.skip . Once I implemented the code to satisfy a given test, I removed that skip annotation for that specific test. While I would made modifications on how I disabled tests later, this was a good point for me to start off at. What Was the Workflow? From the outset, the basic implementation workflow was as follows: figure out the next section to work on figure out the next section-feature to implement enable the relevant tests for that section-feature add or change the code in tokenized_markdown.py to implement that feature execute all enabled tests, with special attention to the feature added in item 4. if there were any test errors; debug, fix and go back to item 4. stage the changes in the project before if there are more features in the current section, go back to item 2. verify each test case's input and output against the specification if any verification errors are found; debug, fix and go back to item 4. if there are any leaf block sections left to work on, go back to item 1. It was not really glamourous, but it worked well. Looking closely at the list, it is easy for me to see why… I took an agile approach without really being aware of it. According to the Wikipedia article on Agile Software Development , there are a number of good practices that I was following. Because I was doing testing as I went, the is a good argument to be made that I was practicing Agile Testing and Test Driven Development . As the tests are also the acceptance criteria for this stage of the project, Acceptance Test Driven Development could also be tacked on to those two Agile practices. Finally, as the workflow is iterative by its very nature, the workflow also qualifies as Iterative and Incremental Development . All in all, I see a few solid agile patterns within the workflow. Agile aspirations aside, the real test of this workflow is that it works for me and works well. I was able to stick to the process easily. It very nicely compartmentalized my work into nice iterations that were easy for me to keep in my head. It was also simple enough that if I needed to refocus myself, I just had to figure out where I was in the workflow and where I was in the specification, and I was able to get back to work! In addition, I feel that if I had performed this development as part of a team, the frequent commits and complete with enabled tests would enable me to share my progress with the rest of the team, and solicit their feedback in a quick and iterative manner. More importantly, at no point in the development practice did I feel that I bit off more than I could handle. Of course, there were times where I was wondering how long it was going to take me and how I would handle some features… I am only human! But the agile nature of how the workflow is structured kept me grounded and focused on the feature that was in front of me. I just reminded myself to keep that focus, and feature by feature, the foundations of the parser came together. In the end, this workflow was not about being agile or taking easy to implement steps. It is about finding something that works well for the team… namely me. How Did Things Progress? The order in which things are tackled is important. Doing the big stuff at the start of the project sometimes pays off, but it can often be demoralizing. Doing the small stuff first can lay some great foundations but miss the larger target due to the smaller focus. To accomplish this for the PyMarkdown project, I broke this part of the project down into 4 groups of Markdown elements. Each group of Markdown elements that were handled added new information to the stream of tokens that were being generated by the parser, allowing for future examination. It was very important to me to ensure that the token stream was kept working, the implementation always moving forwards. Group 1: Foundational Elements The first group that I worked on were the rudimentary elements of blank lines, paragraphs, and thematic breaks. This was a good first group to work on, as these were all common Markdown elements that people use and are foundational to the rest of the work. As such, they were good confidence boosters for the tribulations that I expected that would occur later with the more complicated elements. The only real issue that I had with this first group was due to my lack of confidence about the Markdown specification itself. From my days on the Internet Engineering Task Force , I am used to clear grammar specifications written in Backus-Naur form . However, this specification has no such representation and is written mainly as a series of use cases and text to describe each use case. It took me a while to see that what I perceived initially as a downfall was a bonus. Instead of having to search for examples or to make them up myself, they were already provided. Once I got used to that concept, my confidence increased, and I started to implement each test more quickly than the last one. While it did not seem like much at the time, at this point the parser was capable of handling the following Markdown: This is captured in a paragraph . *** Group 1 Sidebar: Tabs I started to tackle the GFM specification decision that any tab character is replaced with exactly 4 space characters. For the most part, this had little bearing on the foundational elements, but the subject of tabs versus spaces has ignited programming holy wars that last to this day. I thought it was useful and prudent to deal with it and get it out of the way early. Smartly, Markdown avoids these arguments with a strong statement that 1 tab character equals 4 space characters, and a decent argument to reinforce that the decision is the right one. Except for the indented code block, every Markdown element is only recognized if it starts with less than 4 spaces. An indented code block line is only recognized if it starts with 4 spaces. Therefore, a shortcut for any indented code block is to start the line with 1 tab character, due to its 1:4 mapping. To be honest, I feel this is brilliant in its simplicity. Group 2: Headers The next group that I tackled were the header markers, referred to in the specification as the setext and atx elements. Weird names though they are, they are the up to 6 # characters at the start of the line, or the - or = characters underlining text from a previous paragraph. While the atx elements (the # characters) was straight forward, the ‘underlining' aspect of the setext element made it interesting. As that element essentially makes the last paragraph a heading, I had to search backwards in the list of generated tokens for the first time. It was also at this point that I decided to perform some refactoring to better handle string processing. The simple truth about any parser is that it requires gratuitous amounts of \"string fiddling\" 1 . Most efficient parsers work aggressively to parse their documents in a way that minimizes the number of actual strings created while parsing. A good example of efficient \"string fiddling\" can be seen in the following example of parsing the sentence I have a black dog . When parsing out the word black , the most optimal parsers will find the index of the b in black , then find the space character after the k , using the language's substring function and those two indexes to create a single string with black in it. Less optimal parsers will find the b append it to the end of an empty string (creating a new string with b ), then find the l character and appended it, etc. This can easily cause 6 strings to be created during the parsing of the word black , when only 1 is needed. As some of the Markdown documents that the parser will handle are large, it is important to remember optimizations like this as features are added. Keeping this in mind, I started looking for \"string fiddling\" patterns that looked ripe for refactoring. The most obvious one was the determine_whitespace_length function that took care of any tabs in the input data. While I would rip this out later, opting instead to do a simple search-and-replace for tabs at the start of parsing, the determine_whitespace_length function kept things manageable for tabs characters. There were also the extract_whitespace* functions for extracting whitespace and the collect_while_character function for collecting data for a string while the input was a given character. Taking a peek ahead in the specification, it was easy to see that moving the code into those functions was going to pay off. When it comes down to it, there were no real issues that I experienced with the headers. My confidence was still building from the foundational group above, but there was nothing weird or challenging that I did not handle with a bit of serious thought and planning. At this point, the parser could handle the following Markdown elements: # My Markdown This is captured in a paragraph . But this is also a header ------------------- *** Group 3: Indented and Fenced Code Blocks Marching right along, indented and fenced code blocks were next on the list. Both are used to denote sections of text that are to be represented literally, but one is easier and one is more flexible. The indented code blocks require 4 space characters (or a tab character) at the start of the line to denote the block, and text is presented plainly. However, the fenced code blocks start and end with an equal number of ` or ~ characters and include provisions for naming the type of text used within the code block. This naming allows processors to specify a given style to apply to the code block, allowing processors and style sheets to ‘colorize' the text according to the the specified type name. This grouping was easy to process, adding the extract_until_whitespace function to the growing list of helper functions. The interesting part to the code blocks was that I needed to add extra processing of normal text to handle the text within the code blocks. Prior to these code blocks, any text that did not fall into one of the other categories was simply wrapped in a paragraph. Both blocks have specific end conditions, and until those end conditions are met, the collection continues. This meant adding extra code at the start of line parsing to determine if it was within one of the code blocks. If the end condition was met, then the end block token was emitted, and if not, a text block would be emitted without further parsing. It was at this point that I started seeing the intertwining nature of some of the use cases. An indented code block cannot interrupt a paragraph, but a fenced code block can. So when looking for the indented code block, I had to explicitly disallow one from starting if the block currently being process was a paragraph. While this was only a small case, it became obvious to me from a quick scan over the specification that this type of pattern was going to repeat more than once. As such, I started moving the start and stop logic into their own functions, whether they required it or not. This improved the readability and enabled me to get a better view on what was being handled and where. At this point, the parser could handle the following Markdown elements: # My Markdown This is captured in a paragraph . ``` Python rt = \"1:\" + str ( 1 ) ``` But this is also a header ------------------- code block *** Please note that the fenced code block specifies python as it's type, allowing the colorization of the text with the assumption that the code block is Python code. Group 4: Stopping at a Good Place Sometimes it makes sense to march forward without much attention to the surroundings, and sometimes it makes sense to stop at a good place along the way. In taking a quick look at HTML blocks, I figured they were going to be tricky, and I had the same determination with the table element. Taking a look at the link reference definitions, I noticed that they required inline expansion of text within the blocks, something that I wasn't even remotely close to yet. These three leaf blocks were in the final group: the To Be Done Later group. To ensure that I had a good place to come back to when I was ready for the each of these blocks, I made sure to go through and implement, verify, and then disable each test for every leaf block. Depending on the leaf block, I handled the disabling of the tests differently. To properly deal with the link reference definitions, I needed the inline processing capabilities that I knew were many weeks away. As such, I kept those tests disabled in the previous documented way of using the @pytest.mark.skip annotation. This was a big shout out to me that these were going to need to be completed after almost everything else. In the case of any other of the leaf node tests, I captured the current tokens emitted for that case and placed them in the corresponding test. While it might seem weird, my belief was that by testing each test case this way, I would increase overall coverage and possibly hit edge cases not currently documented in a use case. It also meant that once I started implementing the HTML blocks and table blocks, those tests would just start failing in predictable fashion. What Was My Experience So Far? It is always easier to look back and see what worked and what did not work, than to observe it at the time. With only a few issues, I personally felt like I dodged a lot of pain due to the specification and planning. While BNF grammars are easy to implement, the general rule is to \"be strict in what you generate and lenient in what you accept\". As such, coming up with \"valid\" parse cases is a task that takes a long time to complete. By having the acceptable test cases as part of the core specification, the time that I would normally spend in the development and testing phase was greatly reduced. True, it took me a while to get used to it, but when I did, it just worked and worked well. One of the practices that I engaged in during the development of the parser is to liberally spread around print statements as I went. As I was adding these statements, my dominant thought was to collect enough information to determine which pieces of information were the most relevant for log messages to be added later. However, as I proceeded, that information also had the additional benefits of being immensely helpful to debug any parsing issues, and indispensable in the verification of the code itself. While I know I need to remove those statements or convert them before the project is completed, their presence is indeed beneficial. All in all, I think I had a great start to an interesting project and learned a bit in the process… and learning is always good! What is Next? Next up on the list is adding block quote and list support to the parser. Stay tuned! I remember this term being used all the way back to my university days. The closest I have been able to come to a definition is the Oxford dictionary's definition: touch or fidget with something in a restless or nervous way. Perhaps this is alluding to amount of work to get most string operations \"just right\"? ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/01/27/markdown-linter-parsing-normal-markdown-blocks/","loc":"https://jackdewinter.github.io/2020/01/27/markdown-linter-parsing-normal-markdown-blocks/"},{"title":"Clarity Through The Summarizing of Test Measurements","text":"Introduction As part of the process of creating a Markdown Linter to use with my personal website, I firmly believe that it is imperative that I have solid testing on the linter and the tools necessary to test the linter. In previous articles, I talked about the framework I use to scenario test Python scripts and how my current PyTest setup produces useful test reports , both human-readable and machine-readable. These two things allow me to properly test my Python scripts, to collect information on the tests used to verify those scripts, and to determine how well the collection of tests covers those scripts. While the human-readable reports are very useful for digging into issues, I often find that I need a simple and concise \"this is where you are now\" summary that gives me the most pertinent information from those reports. Enter the next tool in my toolbox, a Python script that summarizes information from the machine-readable reports, unimaginatively called PyScan . While it is simple tool, I constantly use this tool when writing new Python scripts and their tests to ensure the development is going in the direction that I want to. This article describes how I use the tool and how it provides a benefit to my development process. Why Not Discuss The Script Itself? When coming up with the idea for this article, I had two beneficial paths available: focus on the code behind the PyScan tool or focus on the usage of the PyScan tool. Both paths have merit and benefit, and both paths easily provide enough substance for a full article. After a lot of thought, I decided to focus on the usage of this tool instead of the code itself. I made this decision primarily due to my heavy use of the PyScan tool and it's significant benefit to my development process. I rely on the PyScan to give me an accurate summary of the tests used to verify any changes along with the impact on code coverage for each of those changes. While I can develop without PyScan, I find that using PyScan immediately increases my confidence in each change I make. When I make a given type of change to either the source code or the test code, I expect a related side-effect to appear in the test results report and the test coverage report. By having PyScan produce summaries of the test results and test coverage, each side-effect is more visible, therefore adding validation that the changes made are the right changes. In the end, the choice became an easy one: focus on the choice with the most positive impact. I felt that documenting how I use this tool satisfied that requirement with room to spare. I also felt that if any readers are still interested in looking at the code behind the script, it's easy enough to point them to the project's GitHub repository and make sure it is well documented. Setting Up PyScan For It's Own Project Based on the setup from the last article , the PyTest command line options --junitxml=report/tests.xml and --cov-report xml:report/coverage.xml place the tests.xml file and the coverage.xml file in the report directory. Based on observation, the tests.xml file is in a JUnit XML format and the coverage.xml file is in a Cobertura XML format. The format of the tests.xml is pretty obvious from the command line flag required to generate it. The format of the coverage.xml file took a bit more effort, but the following line of the file keyed me to it's format: <!-- Based on https://raw.githubusercontent.com/cobertura/web/master/htdocs/xml/coverage-04.dtd --> From within the project's root directory, the main script is located at ../main.py . Since the project uses pipenv , the command line to invoke the script is pipenv run python pyscan/main.py and invoking the script with the --help option gives us the options that we can use. Following the information from the help text, the command line that I use from the project's root directory is: pipenv run python pyscan/main.py --junit report/tests.xml --cobertura report/coverage.xml With everything set up properly, the output from that command looks like: Test Results Summary -------------------- Class Name Total Tests Failed Tests Skipped Tests ---------------------------- ------------ ------------- -------------- test.test_coverage_profiles 2 0 0 test.test_coverage_scenarios 12 0 0 test.test_publish_scenarios 9 0 0 test.test_results_scenarios 19 0 0 test.test_scenarios 1 0 0 --- -- - - TOTALS 43 0 0 Test Coverage Summary --------------------- Type Covered Measured Percentage ------------ -------- --------- ----------- Instructions --- --- ----- Lines 505 507 99.61 Branches 158 164 96.34 Complexity --- --- ----- Methods --- --- ----- Classes --- --- ----- Before We Continue… To complete my setup, there are two more things that are needed. The first thing is that I primarily execute the tests from a simple Windows script called ptest.cmd . While there is a lot of code in the ptest.cmd script to handle errors and options, when the script is boiled down to it's bare essence, the script runs tests and reports on those tests as follows: pipenv run pytest pipenv run python pyscan/main.py --only-changes --junit report/tests.xml --cobertura=report/coverage.xml Note I also have a Bash version called ptest.sh which I have experimented with locally, but is not checked in to the project. If you are interested in this script, please let me know in the comments below. Setting up a script like ptest keeps things simple and easy-to-use. One notable part of the script is that there is a little bit of logic in the script to not summarize any coverage if there are any issues running the tests under PyTest. Call me a purist, but if the tests fail to execute or are not passing, any measurements of how well the tests cover the code are moot. The other thing that I have setup is a small change to the command line for PyScan. In the \"bare essence\" text above, after the text pyscan/main.py , there is a new option used for PyScan: the --only-changes option. By adding the --only-changes option, PyScan restricts the output to only those items that show changes. If no changes are detected, it displays a simple line stating that no changes have been observed. In the case of the above output, the output with this new option is as follows: Test Results Summary -------------------- Test results have not changed since last published test results. Test Coverage Summary --------------------- Test coverage has not changed since last published test coverage. To me, this gives a very clear indication that things have not changed. In the following sections, I go through different cases and explain what changes I made and what effects I expect to see summarized. Introducing Changes and Observing Behavior For this section of the article, I temporarily added a \"phantom\" feature called \"nothing\" to PyScan. This feature is facilitated by two code changes. In the __parse_arguments function, I added the following code: parser . add_argument ( \"--nothing\" , dest = \"do_nothing\" , action = \"store_true\" , default = False , help = \"only_changes\" , ) and in the main function, I changed the code as follows: args = self . __parse_arguments () if args . do_nothing : print ( \"noop\" ) sys . exit ( 1 ) Note that this feature is only present for the sake of these examples, and is not in the project's code base. Adding New Code When I added the above code for the samples, the output that I got after running the tests was: Test Results Summary -------------------- Test results have not changed since last published test results. Test Coverage Summary --------------------- Type Covered Measured Percentage -------- -------- --------- ------------- Lines 507 (+2) 511 (+4) 99.22 (-0.39) Branches 159 (+1) 166 (+2) 95.78 (-0.56) Based on the introduced changes, this output was expected. In the Measured column, 4 new lines were added (1 in __parse_arguments and 3 in main ) and the if args.do_nothing: line added 2 branches (1 for True and one for False). In the Covered column, without any tests to exercise the new code, 2 lines are covered by default (1 in __parse_arguments and 1 in main ) and 1 branch is covered by default (the False case of if args.do_nothing: ). Adding a New Test Having added source code to the project, I added a test to address the new code. To start, I added this simple test function to the test_scenarios.py file: def test_nothing (): pass This change is just a stub for a test function, so the expected change is that the number of tests for that module increase and there is no change in coverage. This effect is born out by the output: Test Results Summary -------------------- Class Name Total Tests Failed Tests Skipped Tests ------------------- ------------ ------------- -------------- test.test_scenarios 2 (+1) 0 0 --- -- - - TOTALS 44 (+1) 0 0 Test Coverage Summary --------------------- Type Covered Measured Percentage -------- -------- --------- ------------- Lines 507 (+2) 511 (+4) 99.22 (-0.39) Branches 159 (+1) 166 (+2) 95.78 (-0.56) Populating the Test Function Now that a stub for the test is in place and registering, I added a real body to the test function as follows: def test_nothing (): # Arrange executor = MainlineExecutor () suppplied_arguments = [ \"--nothing\" ] expected_output = \"\"\"noop \"\"\" expected_error = \"\" expected_return_code = 1 # Act execute_results = executor . invoke_main ( arguments = suppplied_arguments , cwd = None ) # Assert execute_results . assert_results ( expected_output , expected_error , expected_return_code ) The code that I added at the start of this section is triggered by the command line argument --nothing , printing the simple response text noop , and returning a return code of 1 . This test code was crafted to trigger that code and to verify the expected output. Test Results Summary -------------------- Class Name Total Tests Failed Tests Skipped Tests ------------------- ------------ ------------- -------------- test.test_scenarios 2 (+1) 0 0 --- -- - - TOTALS 44 (+1) 0 0 Test Coverage Summary --------------------- Type Covered Measured Percentage -------- -------- --------- ------------- Lines 509 (+4) 511 (+4) 99.61 ( 0.00) Branches 160 (+2) 166 (+2) 96.39 (+0.04) Based on the output from the test results summary, the test does verify that once triggered, the code is working as expected. If there was any issue with the test, the summary would include the text 1 (+1) in the Failed Tests column to denote the failure. As that text is not present, it is safe to assume that both tests in the test.test_scenarios module succeeded. In addition, based on the output from the test coverage summary, the new code added 4 lines and 2 branches to the code base, and the new test code covered all of those changes. Establishing a New Baseline With the new source code and test code in place, I needed to publish the results and set a new baseline for the project. To do this with the ptest script, I invoked the following command line: ptest -p Within this ptest script, the -p option was translated into the following command: pipenv run python pyscan/main.py --publish When executed, the publish/coverage.json and publish/test-results.json files were updated with the current summaries. Following that point, when the script was run, it reverts back to the original output of: Test Results Summary -------------------- Test results have not changed since last published test results. Test Coverage Summary --------------------- Test coverage has not changed since last published test coverage. This process can be repeated at any time to establish a solid baseline that any new changes can be measured against. Refactoring Code - My Refactoring Process In practice, I frequently do \"cut-and-paste\" development during my normal development process. However, I do this with a strict rule that I follow: \"2 times on the fence, 3 times refactor, clean up later\". That rule break down as follows: if I cut-and-paste code once, I then have 2 copies, and I should consider refactoring unless I have a good reason to delay if I cut-and-paste that code again, I then have 3 copies, and that third copy must be into a function that the other 2 copies get merged into when I have solid tests in place and I am done with primary development, go back to all of the cases where I have 2 copies and condense them if beneficial My rationale for this rule is as follows. When you are creating code, you want the ideas to flow free and fast, completing a good attempt at meeting your current goal in the most efficient way possible. While cut-and-paste as a long term strategy is not good, I find that in the short term, it helps me in creating a new function, even if that function is a copy of something done before. To balance that, from experience, if I have pasted the same code twice (meeting the criteria for \"3 times refactor\"), there is a very good chance that I will use that code at least one more time, if not more. At that point, it makes more sense to refactor the code to encapsulate the functionality properly before the block of code becomes to unwieldly. Finally, once I have completed the creation of the new source code, I go back and actively look for cases where I cut-and-pasted code, and if it is worth it to refactor that code, with a decision to refactor if I am on the fence. At the very least, refactoring code into a function almost always makes the code more readable and maintainable. Basically, by following the above rule for refactoring, I almost always change the code in a positive manner. The summaries provided to me from PyScan help me with this refactoring in a big way. Most of the time, the main idea with refactoring is to change the code on the \"inside\" of the program or script without changing the \"outside\" of the program or script. If any changes are made to the \"outside\", they are usually small changes with very predictable impacts. The PyScan summaries assist me in ensuring that any changes to the outside of the script are kept small and manageable while also measuring the improvements made to the inside of the script. Essentially, seeing both summaries helps me keep the code refactor of the script very crisp and on course. Refactoring Code - Leveraging The Summaries A good function set of functions for me to look at for clean-up refactoring were the generate_test_report and generate_coverage_report functions. When I wrote those two functions, I wasn't sure how much difference I was going to have between those two functions, so did an initial cut-and-paste (see \"2 times on the fence\") and started making changes. As those parts of PyScan are now solid and tested, I went back (see \"clean up later\") and compared the two functions to see what was safe to refactor. The first refactor I performed was to extract the xml loading logic into a new __load_xml_docment function. While I admit I didn't get it right the first time, the tests kept me in check and made sure that, after a couple of tries, I got it right. And when I say \"tries\", I mean that I made a change, ran ptest , got some information, and diagnosed it… all within about 30-60 seconds per iteration. In the end, the summary looked like this: Test Results Summary -------------------- Test results have not changed since last published test results. Test Coverage Summary --------------------- Type Covered Measured Percentage -------- --------- --------- ------------- Lines 499 (-10) 501 (-10) 99.60 (-0.01) Branches 154 ( -6) 160 ( -6) 96.25 (-0.14) As expected, the refactor eliminated both lines of code and branches, with the measured values noted in the summary. The second refactor I made was to extract the summary file writing logic into a new __save_summary_file function. I followed a similar pattern to the refactor for __load_xml_docment , but there was a small difference. In this case, I observed that for a specific error case, one function specified test coverage and the other function specified test summary . Seeing as consistent names in output is always beneficial, I decided to change the error messages to be consistent with each other. The test coverage name for the first function remained the same, but the test summary name was changed to test report , with the text summary added in the refactored function. At this point, I knew that one test for each of the test results scenarios and test coverage scenarios was going to fail, but I knew that it would fail in a very specific manner. Based on the above changes, the text Project test summary file for the results scenario test should change to Project test report summary file and the text Project test coverage file for the coverage scenario test should change to Project test coverage summary file . When I ran the tests after these changes, there were indeed 2 errors, specifically in the tests I thought they would show up in. Once those 2 tests were changed to reflect the new consistent text, the tests were ran again and produced the following output: Test Results Summary -------------------- Test results have not changed since last published test results. Test Coverage Summary --------------------- Type Covered Measured Percentage -------- --------- --------- ------------- Lines 491 (-18) 493 (-18) 99.59 (-0.01) Branches 152 ( -8) 158 ( -8) 96.20 (-0.18) Once again, the output matched my expectations. While it wasn't a large number of code or branches, an additional 8 lines and 2 branches were refactored. Determining Additive Test Function Coverage There are times after I have written a series of tests where I wonder how much actual coverage a given test contributes to the overall test coverage percentage. As test coverage is a collaborative effort of all of the tests, a single number that identifies the amount of code covered by a single test is not meaningful. However, a meaningful piece of information is what unique coverage a given test contributes to the collection of tests as a whole. To demonstrate how I do this, I picked one of the tests that addresses one of the error conditions, the test_summarize_cobertura_report_with_bad_source function in the test_coverage_scenarios.py file. Before I changed anything, I made sure to publish the current state to use it as a baseline. To determine the additive coverage this test provides, I simply changed it's name to xtest_summarize_cobertura_report_with_bad_source . As the pytest program only matches on functions that start with test_ , the function was then excluded from the tests to be executed. Upon running the ptest script, I got the following output: Test Results Summary -------------------- Class Name Total Tests Failed Tests Skipped Tests ---------------------------- ------------ ------------- -------------- test.test_coverage_scenarios 11 (-1) 0 0 --- -- - - TOTALS 43 (-1) 0 0 Test Coverage Summary --------------------- Type Covered Measured Percentage -------- -------- --------- ------------- Lines 507 (-2) 511 99.22 (-0.39) Branches 159 (-1) 166 95.78 (-0.60) Interpreting this output, given what I documented earlier in this article, was pretty easy. As I \"disabled\" one of the coverage scenario tests in the test_coverage_scenarios.py file, the summary reports one less test in test.test_coverage_scenarios as expected. That disabled test added 2 lines of coverage and 1 branch of coverage to overall effort, coverage that was now being reported as missing. As this test was added specifically to test a single error case, this was expected. If instead I disable the xtest_junit_jacoco_profile test in the test_coverage_profiles.py file, I get a different result: Test Results Summary -------------------- Class Name Total Tests Failed Tests Skipped Tests --------------------------- ------------ ------------- -------------- test.test_coverage_profiles 1 (-1) 0 0 --- -- - - TOTALS 43 (-1) 0 0 Test Coverage Summary --------------------- Type Covered Measured Percentage -------- -------- --------- ------------- Lines 501 (-8) 511 98.04 (-1.57) Branches 152 (-8) 166 91.57 (-4.82) Like the previous output, the disabled test is showing up as being removed, but there is a lot more coverage that was removed. Strangely enough, this was also expected. As I also use PyScan to summarize test results from Java projects I work on, I used all 6 coverage measurements available from Jacoco 1 as a baseline for the 2 measurements generated by PyTest for Python coverage. With a quick look at the report/coverage/pyscan_model_py.html file, this was indeed the reason for the difference, with the test exercising 4 additional paths in each of the serialization and deserialization functions. Basically, four paths of one line each, times two (one for serialization and one for deserialization), and the 8 lines/branches covered is explained. Wrapping Up I believe that making my decision to talk about how I use my PyScan tool to summarize test results and test coverage was the right choice. It is difficult for me to quantize exactly how much benefit PyScan has provided to my development process, but it is easily in the very positive to indispensable category. By providing a quick summary on the test results file and the test coverage file, I can ensure that any changes I make are having the proper effects on those two files at each stage of the change that I am making. I hope that by walking through this process and how it helps me, it will inspire others to adopt something similar in their development processes. For an example Jacoco HTML report that shows all 6 coverage measurements, check out the report trunk coverage for Jacoco . ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/01/20/clarity-through-the-summarizing-of-test-measurements/","loc":"https://jackdewinter.github.io/2020/01/20/clarity-through-the-summarizing-of-test-measurements/"},{"title":"Measuring Testing in Python Scripts","text":"Introduction As part of the process of creating a Markdown Linter to use with my personal website, I firmly believe that it is imperative that I have solid testing on that linter and the tools necessary to test the linter. In my previous article on Scenario Testing Python Scripts , I described the in-process framework that I use for testing Python scripts from within PyTest. That framework ensures that I can properly test Python scripts from the start of the script, increasing my confidence that they are tested properly. To properly figure out how my tests are doing and what their impact is, I turned on a number of features that are available with PyTest. The features either make testing easier or measure the impact of those tests and relay that information. This article describes my PyTest configuration and how that configuration provides a benefit to my development process. Adding Needed Packages to PyTest There are four main Python packages that I use in conjunction with PyTest. The pytest-console-scripts package is the main one, allowing PyTest to be invoked from the command line. Since I am in favor of automating process where possible, this is a necessity. From a test execution point of view, the pytest-timeout is used to set a timeout on each test, ensuring that a single runaway test does not cause the set of tests to fail to complete. For reporting, the pytest-html package is useful for creating an HTML summary of the test results. The pytest-cov package adds coverage of the source code, with reporting of that coverage built in. I have found that all of these packages help me in my development of Python scripts, so I highly recommend these packages. Depending on the Python package manager and environment in use, there will be slightly different methods to install these packages. For plain Python this is usually: pip install pytest-console-scripts == 0 .20 pytest-cov == 2 .8.1 pytest-timeout == 1 .3.3 pytest-html == 2 .0.1 As I have used pipenv a lot in my professional Python development, all of my personal projects use it for setting up the environment and it's dependencies. Similar to the line above, to install these packages into pipenv requires executing the following line in the project's directory: pipenv install pytest-console-scripts == 0 .20 pytest-cov == 2 .8.1 pytest-timeout == 1 .3.3 pytest-html == 2 .0.1 Configuring PyTest For Those Packages Unless information is provided on the command line, PyTest will search for a configuration file to use. By default, setup.cfg is the name of the configuration file it uses. The following fragment of my setup.cfg file takes care of the configuration for those PyTest packages. [tool:pytest] testpaths=./test cache_dir=./build/test/.pytest_cache junit_family=xunit2 addopts=--timeout=10 --cov --cov-branch --cov-fail-under=90 --strict-markers -ra --cov-report xml:report/coverage.xml --cov-report html:report/coverage --junitxml=report/tests.xml --html=report/report.html While all configuration is important, the following sections are most important in the setting up of PyTest for measuring the effects of testing: testpaths=./test - relative path where PyTest will scan for tests addopts/--junitxml - creates a junit-xml style report file at given path addopts/--cov - record coverage information for everything addopts/--cov-branch - enables branch coverage addopts/--cov-report - types of report to generate and their destination paths default/--cov-config - configuration file for coverage, defaulting to .coveragerc In order, the first two configuration items tells PyTest where to look for tests to execute and where to place the JUnit-styled XML report with the results of each test. The next three configuration items turn on coverage collection, enable branch coverage, and specifies what types of coverage reports to produce and where to place them. Finally, because the --cov-config is not set, the default location for the coverage configuration file is set to .coveragerc . For all of my projects, the default .coveragerc that I use, with a small change to the source= line is: [run] source = pyscan [report] exclude_lines = # Have to re-enable the standard pragma pragma: no cover # Don't complain about missing debug-only code: def __repr__ if self\\.debug # Don't complain if tests don't hit defensive assertion code: raise AssertionError raise NotImplementedError # Don't complain if non-runnable code isn't run: if 0: if __name__ == .__main__.: To be honest, this .coveragerc template is something I picked up somewhere, but it works, and works well for my needs. The exclude lines work in all case that I have come across, so I haven't touched them in the 2+ years that I have been writing code in Python. Benefits Of This Configuration Given the setup from the last section, there are two main benefits that I get from this setup. The first benefit is machine readable XML information generated for the test results and the test coverage. While this is not immediately consumable in it's current form, that data can be harvested in the future to provide concise information about what has been tested. The second benefit is to provide human readable information about the tests that have been executed. The HTML file located at report/report.html relays the results of the last series of tests while the HTML file located at report/coverage/index.html relays the coverage information for the last series of tests. Both of these pieces of information are useful for different reasons. In the case of the test results HTML, the information presented on the test results page is mostly the same information as is displayed by PyTest when executed on the command line. Some useful changes are present, such as seeing all of the test information at once, instead of just a . for a successful test, a F for a failed test, and so on. I have found that having this information available on one page allows me to more quickly debug an issue that is affecting multiple tests, instead of scrolling through the command line output one test at a time. In the case of the test coverage HTML, the information presented on this page is invaluable. For each source file in the Python project being tested, there is a page that clearly shows which lines of each Python script are exercised by the tests, By using these pages as a guide, I can determine what tests I need to add to ensure that the scripts are properly covered. By using these two tools together, I can quickly determine what tests to add, and when tests fail, I can determine why they failed and look for patterns in the failures. This enables me to quickly figure out where the blind spots are in my testing, and to address them quickly. This in turn can help me to figure out the best way to improve the quality of the project I am working on. If this finds an issue with an existing requirement, that requirement can be adjusted or a new requirement added to fulfil the deficiency. If the requirements were all right and the code it was testing was incorrect, that code can be addressed. If the coverage page shows that code was written but not tested, a new test function can be introduced to cover that scenario. Each observation and its appropriate action work to improve the quality of the software project. What Was Accomplished This article showed how to setup PyTest using a configuration file. With that configuration file, it was set up to provide timeouts for tests, provide output on the test results, and provide a coverage report of how well the tests covered the scripts under test. This was all accomplished to better understand the impact of tests on a project and provide better information on how they succeed (test coverage) or fail (test results). By understanding this information, the quality of the software can be measured and improved on if needed. What Is Next? In the next article, I will briefly describe the PyScan tool I have written, and how it takes the XML information generate by the --junitxml=report/tests.xml option and the --cov-report xml:report/coverage.xml option and produces concise summaries of that information. I will also give a number of examples of how I use this information during my development of Python projects.","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/01/13/measuring-testing-in-python-scripts/","loc":"https://jackdewinter.github.io/2020/01/13/measuring-testing-in-python-scripts/"},{"title":"Scenario Testing Python Scripts","text":"Introduction As part of the process of creating a Markdown Linter to use with my personal website, I firmly believe that it is imperative that I have solid testing on that linter and the tools necessary to test the linter. This testing includes executing those Python tool scripts from start to finish and verifying that everything is working properly. From my experience, one of the most efficient ways to scenario test the project's Python scripts is to use an in-process framework for running Python scripts. Because of the way that Python works, it is very feasible to scenario test the Python scripts using the in-process framework which I describe in this article. To show how the framework works in practice, I reference my PyScan project to illustrate how I use this framework to test the scenarios in that project. Specifically, I talk about the pytest_execute.py file which contains the bulk of the code I use to write scenario tests with. 1 Determine the Requirements As with most of my projects, the first thing I do for any new project is to cleanly determine and document the requirements for the project. Even though this project is a single component used to test the tools and other components, I feel strongly that it is still important to follow those guidelines to ensure the right component is built in the right way. The basic requirements are pretty easy to define for this in-process test component: execute the Python script independently and capture all relevant information about it's execution, verifying that information against expected values. The devil is in the details however. I believe that a good definition of \"execute the Python script\" must include the ability to set the current working directory and arguments for the command line. For a good definition of \"capture all relevant information\", I believe the requirements must include capturing of the script's return code as well as any output to standard out (stdout) and standard error (stderr). As this component executes the script in-process, any attempts to exit the script prematurely must be properly captured, and the state of the test must be returned to what it was at the beginning of the test. Finally, to satisfy the \"verifying\" requirement, the component must have easy to use comparison functions, with informative output on any differences that arise during verification. Finding a balance between too many bulky requirements and too few lean requirements is a tough balance to achieve. In this case, I feel that I have achieved that balance by ensuring all of the major parts of the requirements are specified at a high enough level to be able to communicate clearly without ambiguity. Here's hoping I get the balance right! Capture Relevant Information The first thing to take care of is a class that will contain the information to satisfy the \"capture all relevant information\" requirement above. As the requirement specifies the 3 things that need to be captured, all that is left to do is to create a class to encapsulate these variables as follows: class InProcessResult : \"\"\" Class to provide for an encapsulation of the results of an execution. \"\"\" def __init__ ( self , return_code , std_out , std_err ): self . return_code = return_code self . std_out = std_out self . std_err = std_err Executing the Script Now that there is an object to collect the information about the script's execution, a simple function is needed to collect that information. In the InProcessExecution base class, the invoke_main function serves this purpose. def invoke_main ( self , arguments = None , cwd = None ): \"\"\" Invoke the mainline so that we can capture results. \"\"\" saved_state = SystemState () std_output = io . StringIO () std_error = io . StringIO () try : returncode = 0 sys . stdout = std_output sys . stderr = std_error if arguments : sys . argv = arguments . copy () else : sys . argv = [] sys . argv . insert ( 0 , self . get_main_name ()) if cwd : os . chdir ( cwd ) self . execute_main () except SystemExit as this_exception : returncode = self . handle_system_exit ( this_exception , std_error ) except Exception : returncode = self . handle_normal_exception () finally : saved_state . restore () return InProcessResult ( returncode , std_output , std_error ) Before changing any of the existing system values, changes that by their very nature are be made across the entire Python interpreter, the original values of those system values are kept safely in an instance of the the SystemState class in the saved_state variable. As I want to ensure that the saved system state is reverted back to regardless of what happens, a try-finally block is used to ensure that the saved_state.restore function is called to restore the system back to it's original state. Once the system state is safely stored away, changes to those system values can be made. Instances of the StringIo class are used to provide alternative streams for stdout and stderr. A new array is assigned to sys.argv , either an empty array if no arguments are provided or a copy of the provided array if provided. To the start of that array is inserted the name of the main script, to ensure that libraries expecting a properly formatted array of system arguments are happy. Finally, if an alternate working directory is provided to the function, the script changes to that directory. To reiterate, the reason it is acceptable to make all of these changes to the system state is that we have a safe copy of the system state stored away that we will revert to when this function completes. After the execute_main function is called to execute the script in the specified manner, there are three possibilities that the function needs to capture the information for. In the case of a normal fall-through execution, the returncode = 0 statement at the start of the try-finally block sets the return code. If a SystemExit exception is thrown, the handle_system_exit function does a bit of process to figure out the return code based on the contents of the exception. Finally, if the execution is terminated for any other exception, the handle_normal_exception makes sure to print out decent debug information and sets the return code to 1. In all three cases, the collected values for stdout and stderr are collected, combined with the return code determined earlier in this paragraph, and a new instance of the InProcessResult class is returned with these values. Verifying Actual Results Against Expected Results When I started with the assert_results function, it was only 3 statements in quick succession: 3 assert statements asserting that the actual values for stdout, stderr and the return code matched the expected values. However, as I started using that function, it was quickly apparent that when something did fail, there was a certain amount of repetitive debugging that I performed to determine why the assert was triggered. At first I added some extra information to the assert statements, and that worked for the return code. But there were still two issues. The first issue was that, in the case where all 3 expected values were different than the actual values, it took 3 iterations of cleaning up the test before it passed. Only when I cleared up the first failure did I see the second failure, and only after the second failure was dealt with did I see the third. While this was workable, it was far from efficient. The second issue was that if there were any differences with the contents of the stdout or stderr stream, the differences between the expected value and the actual value were hard to discern by just looking at them. To address the first issue, I changed the simple assert_results function to the following: def assert_results ( self , stdout = None , stderr = None , error_code = 0 ): \"\"\" Assert the results are as expected in the \"assert\" phase. \"\"\" stdout_error = self . assert_stream_contents ( \"stdout\" , self . std_out , stdout ) stderr_error = self . assert_stream_contents ( \"stderr\" , self . std_err , stderr ) return_code_error = self . assert_return_code ( self . return_code , error_code ) combined_error_msg = \"\" if stdout_error : combined_error_msg = combined_error_msg + \" \\n \" + str ( stdout_error ) if stderr_error : combined_error_msg = combined_error_msg + \" \\n \" + str ( stderr_error ) if return_code_error : combined_error_msg = combined_error_msg + \" \\n \" + str ( return_code_error ) assert not combined_error_msg , ( \"Either stdout, stderr, or the return code was not as expected. \\n \" + combined_error_msg ) The key to resolving the first issue is in capturing the information about all differences that occur, and then asserting only once if any differences are encountered. To accomplish this, several comparison functions are required that capture individual asserts and relay that information back to the assert_results function where they can be aggregated together. It is these comparison functions that are at the heart of the assert_results function. The easiest of these comparison functions is the assert_return_code function, which simply compares the actual return code and the expected return code. If there is any difference, the error message for the assert statement is descriptive enough to provide a clear indication of what the difference is. That raised AssertionError is then captured and returned from the function so the assert_results function can report on it. @classmethod def assert_return_code ( cls , actual_return_code , expected_return_code ): \"\"\" Assert that the actual return code is as expected. \"\"\" result = None try : assert actual_return_code == expected_return_code , ( \"Actual error code (\" + str ( actual_return_code ) + \") and expected error code (\" + str ( expected_return_code ) + \") differ.\" ) except AssertionError as ex : result = ex return result A slightly more complicated function is the assert_stream_contents comparison function. To ensure that helpful information is returned in the assert failure message, it checks to see if the expected_stream is set and calls compare_versus_expected if so. (More about that function in a minute.) If not set, the assert used clearly states that the stream was expected to be empty, and the actual stream is not empty. def assert_stream_contents ( self , stream_name , actual_stream , expected_stream ): \"\"\" Assert that the contents of the given stream are as expected. \"\"\" result = None try : if expected_stream : self . compare_versus_expected ( stream_name , actual_stream , expected_stream ) else : assert not actual_stream . getvalue (), ( \"Expected \" + stream_name + \" to be empty. Not: \\n --- \\n \" + actual_stream . getvalue () + \" \\n --- \\n \" ) except AssertionError as ex : result = ex finally : actual_stream . close () return result Addressing the second issue with the initial assert_results function, the differences between the two streams being difficult to discern, is the compare_versus_expected function. My first variation on this function simply used the statement assert actual_stream.getvalue() != expected_text , producing the same assert result, but lacking in the description of why the assert failed. The second variation of this function added a better assert failure message, but left the task of identifying the difference between the two strings on the reader of the failure message. The final variation of this function uses the difflib module and the difflib.ndiff function to provide a detailed line-by-line comparison between the actual stream contents and the expected stream contents. By using the difflib.ndiff function in this final variation, the assert failure message now contains a very easy to read list of the differences between the two streams. import difflib @classmethod def compare_versus_expected ( cls , stream_name , actual_stream , expected_text ): \"\"\" Do a thorough comparison of the actual stream against the expected text. \"\"\" if actual_stream . getvalue () != expected_text : diff = difflib . ndiff ( expected_text . splitlines (), actual_stream . getvalue () . splitlines () ) diff_values = \" \\n \" . join ( list ( diff )) assert False , ( stream_name + \" not as expected: \\n --- \\n \" + diff_values + \" \\n --- \\n \" ) Using it all together To start using the work that completed in the sections above, a proper subclass of the InProcessExecution class is required. Because that class is an abstract base class, a new class MainlineExecutor is required to resolve the execute_main function and the get_main_name function. class MainlineExecutor ( InProcessExecution ): def __init__ ( self ): super () . __init__ () resource_directory = os . path . join ( os . getcwd (), \"test\" , \"resources\" ) self . resource_directory = resource_directory def execute_main ( self ): PyScan () . main () def get_main_name ( self ): return \"main.py\" The MainlineExecutor class implements those two required functions. The get_main_name function returns the name of the module entry point for the project. This name is inserted into the array of arguments to ensure that any functions based off of the command line sys.argv array resolves properly. The execute_main function implements the actual code to invoke the main entry point for the script. In the case of the PyScan project, the entry point at the end of the main.py script is: if __name__ == \"__main__\" : PyScan () . main () Therefore, the contents of the execute_main function is PyScan().main() . In addition to those two required functions, there is some extra code in the constructor for the class. Instead of recomputing the resource directory in each test that requires it, the MainlineExecutor class computes it in the constructor to keep the test functions as clean as possible. While this is not required when subclassing from InProcessExecution , it has proven very useful in practice. To validate the use of the MainlineExecutor class with the project, I created a simple scenario test to verify that the version of the scanner is correct. This is very simple test, and verifying that the framework passes such a simple test increases the confidence in the framework itself. At the start of the scenario test, the executor variable is created and assigned an instance of our new class MainlineExecutor as well as specify that the arguments to use for the script as [\"--version\"] . in the array suppplied_arguments In keeping with the Arrange-Act-Assert pattern, I then specify the expected behaviors for stdout (in expected_output ), stderr (in expected_error ), and the return code from the script (in expected_return_code ). Having set everything up in the Assert section of the test, the Act section simply invokes the script using the executor.invoke_main function with the suppplied_arguments variable assigned previously, and collect the results. Once collected, the execute_results.assert_results function verifies those actual results against the expected results, asserting if there are differences. def test_get_summarizer_version (): \"\"\" Make sure that we can get information about the version of the summarizer. \"\"\" # Arrange executor = MainlineExecutor () suppplied_arguments = [ \"--version\" ] expected_output = \"\"\" \\ main.py 0.1.0 \"\"\" expected_error = \"\" expected_return_code = 0 # Act execute_results = executor . invoke_main ( arguments = suppplied_arguments , cwd = None ) # Assert execute_results . assert_results ( expected_output , expected_error , expected_return_code ) What Does Using This Look Like? In terms of writing scenario tests, the tests are usually as simple to write as the test_get_summarizer_version function in the last section. If there are parts of the output that have a non-constant value, such as the full path of the directory in which the test is executed in, the expected_output variable would have to be set to compensate for that variability, but that is an expected complexity. For the PyScan project, a quick scan of the PyScan test_scenarios.py file reveals that for this project, the non-constant values most often occur with failure messages, especially ones that relay path information in their failure messages. When that happens, such as with the test_summarize_junit_report_with_bad_source test function, that extra complexity is not overwhelming and does not make the test function unreadable. In terms of the test output for a passing test, there is no difference. If executing pipenv run pytest produced a . for a successful test before, it remains a . now. The big difference is in what is displayed when there is a difference in the test output. In the case where there is a single character difference in the test output, such as changing the expected output for the test_get_summarizer_version test to main.py 0.1.1 , the output below clearly shows where the actual output and expected output differ. Note that in these comparisons, the line that starts with the - character is the expected output and the line that starts with the + character is the actual output. E AssertionError: Either stdout, stderr, or the return code was not as expected. E E stdout not as expected: E --- E - main.py 0.1.1 E ? &#94; E E + main.py 0.1.0 E ? &#94; E E --- In the case where a line in the test output is completely different, such as changing the expected output to This is another line , the output below clearly reflects that difference: E AssertionError: Either stdout, stderr, or the return code was not as expected. E E stdout not as expected: E --- E - This is another line E + main.py 0.1.0 E --- Finally, in the case where the actual output contains either more lines or less lines that the expected output, such as adding the line This is another line to the expected output, the output below clearly shows that difference. In this example, as the first line is at the start of both the actual output and expected output, it is shown without any prefix to the line. E AssertionError: Either stdout, stderr, or the return code was not as expected. E E stdout not as expected: E --- E main.py 0.1.0 E - This is another line E --- Summary While the pytest_execute.py file that I use as the base for my scenario tests isn't rocket science, it is invaluable to me in creating simple, easy-to-read scenario tests. At the heart of the module is the base requirement (as stated above) to execute the Python script independently, capture all relevant information about it's execution, and then verifying that information against expected values. Based on my experience and evolution of this module, I believe that it handily satisfies the requirements with ease. To keep things simple for the article, the additional_error parameter from a number of the functions has been removed. This parameter is used in the PyMarkdown project and will be documented as part of my articles on that project. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/01/06/scenario-testing-python-scripts/","loc":"https://jackdewinter.github.io/2020/01/06/scenario-testing-python-scripts/"},{"title":"Have a Happy Winter Holiday 2019","text":"I just wanted to take a quick minute and wish everyone a happy winter holiday season as 2019 winds to a close. When I resume posts in the new year, I will be trying to publish weekly posts on Mondays instead of Sundays, and see how that goes. Safe travels, and well wishes.","tags":"Software Quality","url":"https://jackdewinter.github.io/2019/12/29/have-a-happy-winter-holiday-2019/","loc":"https://jackdewinter.github.io/2019/12/29/have-a-happy-winter-holiday-2019/"},{"title":"Markdown Linter - Parser Testing Strategy","text":"Introduction In the previous articles in this series, I discussed the requirements for the Markdown linter that I am writing. From a development point of view, the main requirement is the need for an accurate stream of tokens emitted by the parser. Due to the absence of any Markdown-to-token parsers out there, I need to write a new parser that outputs an accurate stream of tokens instead of a stream of HTML text. With the last article showing the patterns I am using to test the parser, it is now time to figure out a set of good strategies for the project, to ensure I can complete it without losing my confidence (and sanity). Why Is Strategy Important When Testing? When my son was younger, like most boys in his age group, he loved playing with LEGO and he loved the idea of robots. I mean, come on! I am a lot older than him and I still like LEGO and the idea of robots! Anyhow, at his school they advertised for 5th grade students that were interested in participating in a local FIRST Lego League robotics team. From the first mention of it, he was hooked. As they needed some parents to help, I participated with him as a coach. That position was a very rewarding, very humbling, and very frustrating experience. Rewarding because I got to help 5th graders learn a little taste of what I did every day at work. Humbling because the look in the kid's eyes when they really understood something reminded me of the benefits of being a coach. Frustrating because of almost all the rest of the time between those two types of moments. I am not sure which parent, coach, or teacher helped me with a little gem of wisdom, but I remember it as clear as day: People have problems moving boulders, people have success moving pebbles. The idea behind that phrase is that if a team is confronted with a problem, it is like encountering a boulder that you need to move out of the way. Upon seeing a big boulder, many people looking at it and say something similar to \"Wow! That is too big to move!\" But if you take that boulder and break it down into smaller rocks, such as pebbles, many people will just laugh with ease at moving those rocks, even if they must do it one at a time. In a similar fashion, breaking down a big problem into smaller problems is a necessity in problem solving a situation. The boulders-to-pebbles phrase is a phrase I still use to this day when coaching people in both my professional and personal lives. Writing a parser that handles anything more significant than a single line of text is definitely \"a boulder\". I have been writing parsers for the better part of 25 years, and those parsers are still boulders to me. However, I know from experience that breaking down that \"boulder-sized\" task into more \"pebble-sized\" tasks is something that works and works well. So here are the various items of my strategy for this project. Strategy 0: Define and Execute Testing, Linting, and Formatting For me this is a strategy that I bring to almost every project, with very few exceptions. I always start with a workflow template that I apply to the project that performs formatting of the source code, linting of the source code, and executes the testing framework. Since I am a stickler for this approach, the setup for this workflow usually takes 5 minutes or less, as I usually have at least one example project lying around. By consistently executing this workflow before committing any changes, I keep the quality reasonably high as I go. Knowing that I had this framework in place for the Markdown parser was a godsend. My preference is to find frequent small break points during the implementation of a feature, and to use those points to run the workflow. For me, it increases my confidence that I am either establishing a new \"last known good point\" or that I need to retrace my steps to the last known good point to address an issue. That confidence helps me go forward with a positive attitude. Strategy 0A: Suppress Major Issues Until Later This may seem like somewhat of a counter to Strategy 0, but I see it more of allowing the project to grow but being reminded that there is work to do. Minor issues such as stylistics and documentation are handled right away, as they have a direct impact on the maintainability of the code as it moves forward. Major issues usually involve a larger amount of code and changing that much code usually has a fair amount of side effects unless you work to prevent those side effects. Major issues are usually of the \"too many/much\" type, such as \"too much complexity\", \"too many statements\", or \"too many boolean statements\". When I get to a good and stable point in the project, I know I will deal with these. If I deal with the issues before I get to such a point, I am taking a chance that I won't have the stability to make the change, while limiting and dealing with any potential side effects in a clean and efficient manner. What is a good and stable point? For me, such a point must have two dominant characteristics. The first is that I need to have a solid collection of tests in place that I can execute. These tests make sure that any refactoring does not negatively affect the quality of the code. The second characteristic is that the source code for the project is at a point where there is a large degree of confidence that the code in the section that I want to refactor is very solid and very well defined. This ensures that I can start looking for commonalities and efficiencies for refactoring that will enhance the source code, but not prematurely. Strategy 1: Break Tests and Development into Task Groups Following the principle of keeping things at a good size, don't plan the entire project out ahead of time, but make sure to break things down into the groups of tasks that are needed as you need them. Following an agile approach, make sure you have a good idea of what needs to be done for a given task group, and do not worry about any more details of it until you need to. And when you reach that point, reverify the tasks before going forward and flushing out the details. For this parser, the GitHub Flavored Markdown specification delineates it's groups by the features in Markdown that are implemented. Aligning the groups specified in that document with the groups for tests and development was a solid choice from a tracking point of view. One of the reasons that I feel this worked well is because these feature groups have anywhere between 1 and 50 examples in each group. While some of the larger ones were a tiny bit too big, for the most part it was a manageable number of scenarios to handle in each group. Strategy 2: Organize Those Task Groups Themselves Once the task groups have been identified, take a step back and organize those task groups themselves. There are almost always going to be task groups that have a natural affinity to be with similar task groups, so do so. By doing similar tasks in groups, it will help identify refactorings that can be accomplished later, as well as the efficiency benefits from repeating similar processes. Especially with a larger project, those little efficiency benefits can add up quickly. As with the previous strategy, the GitHub Flavored Markdown specification comes to the rescue again. There are some implementation notes near the end of the specification that provide some guidance on grouping. The groups that I recognized were container blocks, normal blocks, and inline parsing. Normal blocks are the foundation of the parsing, so it made sense to schedule those first. Container blocks (lists and block quotes) add nesting requirements, so I scheduled those second. Finally, once all the block level tasks are done, inline parsing (such as for emphasis) can be performed on text blocks derived at after the processing of the normal and container blocks. After re-reading the end of the specification, the example that they gave seemed to indicate that as well, so I was probably on a decent path. Strategy 3: K.I.S.S. As I mentioned in the last article, I am a big proponent of the K.I.S.S principle . While I usually arrive at an end project that has lots of nice classes and functions, worrying about that at an early stage can often be counter-productive. Even if it means doing ugly string manipulations with variable names that you know you will change, that approach can often lead to cleaner code faster. Worry about getting the logic and the algorithms right first, and then worry about making it \"look pretty\". A good example of this is my traditional development practice of giving variables and functions \"garbage names\" until I am finished with a set of functions. Yes, that means during development I have variable names like \"foobar\", \"abc\", \"sdf\", and \"ghi\", just to name a few of them. When I am creating the function, I maintain a good understanding of what the variables are doing, and I want to concentrate on the logic. Once the logic is solid, I can then rename the variables to a descriptive name that accurately reflects its purpose and use. I am not sure if this process works for everyone, but for me, not focusing on the names helps me focus on the logic itself. I also find that having a \"naming pass\" at the function when I am done with the work helps me to give each variable a more meaningful name before I commit the changes. Once again, this is one of my development practices that helps boost my productivity, and I acknowledge it might not work for everyone. For the parser, I employed this strategy whole-heartedly. The first couple of groups of work on the parser were performed by dealing with strings, with the only class for the parser being the single class containing the parsing logic. Once I got to a good point (see above), I moved a few of the parsing functions and html functions into their own static helper modules. Up until that point, it was just simpler to be creative with the logic in a raw form. After that point, it made more sense to identify and solidify the logic that encapsulated some obvious patterns, moving those algorithms into their own classes for easy identification. As with many things, finding the right points to perform changes like this are difficult to describe. I can only say that \"it felt like the right time for that change\". And as I commit and stage code frequently, if I made a mistake, I could easily rewind and either retry the change, or abandon it altogether. Strategy 4: Use Lots of Debug Output There is a phrase that we use at work called \"TTR\" or Time-To-Resolution. This is usually measured as the time taken from knowing that you have a problem until the time that the problem is resolved and its solution is published. Added during development and debugging, spurious debug output can help provide a journal or log of what is happening in the project, allowing for a more comprehensive comparison of the output of a passing test with the output of a failing test at the same time. To be clear, using a debugger to load the code and step through it as it executes is another way to debug the code. In fact, in a decent number of situations I recommend that. However, I find that the downside is that I do not get to see the flow through the code in the same way as with lots of debug statements. As with a lot of things, determining the balance between debug output and using a debugger will differ for individual developers and for individual projects. Another benefit of the debug output approach is the transition from debug output to logging. Once the project has been sufficiently stabilized and completed, one of the tasks that arises is usually to output useful log messages at various points throughout the code. I personally find that a certain percentage of the debug output that was good enough to emit during development can become quality log messages with only small changes. The parser development benefitted from this strategy. Within a given task group, there were often two Markdown patterns that were almost the same. Sometimes it looked like they should be parsed differently and sometimes I couldn't figure out why they weren't parsed differently. By examining the debug output for both cases, I was able to verify whether the correct paths were followed, and if not, where the divergences occurred. Sure, the debug was cryptic and most of it never made it in the final version of the parser. But when I needed to debug or verify during development, it was invaluable. Strategy 5: Run Tests Frequently Don't only run tests when a set of changes is ready to commit, run those tests frequently during the development of each task. If done properly, most tests are there to verify things are as they should be, and to warn of changes or situations that fall outside of the requirements. If something is wrong, it is better to look through the last feature added to determine what the problem is, rather than trying to determine which of the last 5 features introduced that bad behavior. Therefore, by executing the tests frequently, either the confidence that the project is working properly increases or there are early and frequent indications that something is wrong. During the development of the parser, the tests were instrumental in making sure that I knew what features were \"locked down\" and which features needed work. By keeping track of that when adding a new feature, I could easily see when work on a new feature caused a previously completed feature to fail its tests. At that point, I knew I did not have the right solution, but I also had confidence that the changes were small enough to handle. Also, as the specification is large, there were often cases that were present but not always spelled out in the documentation as well as they could have been. However, time and time again, the saving grace for the specification were the examples, now scenarios and scenario tests in my project, sterling examples of what to expect. And as I took care to make sure they ran quickly; I was able to run all of the scenario tests in less than 10 seconds. For me, taking 10 seconds to ensure things were not broken was well worth the cost. Strategy 6: Do Small Refactors Only at Good Points While this strategy may look like a repeat of Strategy 0A: Suppress Major Issues Until Later , the scope for this strategy is on a smaller, more local level. Where Strategy 0A talks about refactoring major issues later, there are often obvious minor refactors that can be done at a local level. These changes are often done right after a function is written to fulfil a feature and rarely includes more than one function. A good example of this is taking a function that performs a given action twice with small variations and rewriting that function by encapsulating that repeated action into its own well-named function. While such refactors almost always improve the code, care must be taken to strike a good balance between making each method more readable and trying to optimize the function ahead of time. For myself, it is often more efficient for me to see the raw code to recognize patterns from rather than already refactored code. Unless I am the author of the refactored code, I find that I do not see the same patterns as with the raw code. As with many things, \"Your Mileage May Vary\". When implementing the parser, this strategy was effectively applied at the local level to improve readability and maintainability. There were quite a few cases where the logic to detect a given case and the processing of that case were complicated. By assigning the detection of a given case to one function and the processing of that case to another function, the border between the two concepts was enhanced, making the calling function more readable. As this kind of refactoring occurred at the local level, it employed this strategy quite effectively. How Did This Help? For one, I had a plan and a strategy to deal with things. As always, something would happen during development which would require me to re-assess something. Given the above strategy, I had confidence that I would be able to deal with it, adjusting the different parts of the project as I went. Basically, I took a boulder (writing a parser) and not only broke it down into pebbles (tasks needed to write the parser), but came up with a set of rules (strategy) on what to do if I found some rocks that were previously unknown or larger than a pebble. As I mentioned at the start of the article, it is a fairly simple bit of wisdom that I was taught, but what a gem it is! What Comes Next? In the next article, I take the requirements, scenarios, and strategies and put them together to start writing the parser. As one of the test groups that I came up with was normal Markdown blocks, I will describe how I implemented those blocks as well as the issues I had in doing so cleanly.","tags":"Software Quality","url":"https://jackdewinter.github.io/2019/12/22/markdown-linter-parser-testing-strategy/","loc":"https://jackdewinter.github.io/2019/12/22/markdown-linter-parser-testing-strategy/"},{"title":"Markdown Linter - Setting Up Parser Tests","text":"Sidebar My apologies for this being a day or two later than usual. My son brought home a cold that knocked the stuffing out of me, I needed to take some personal time to ensure I was feeling better before writing. Thanks for your patience. Introduction As a reminder of the requirements from the last article , the big bullet-point items are: command line driven, GitHub Flavored Markdown (for now), and preserving all tokens. To make sure I have a solid set of goals to work towards, setting these requirements as part of the project was pivotal. Now that I have that as a touchstone, I need to move forward with defining how to progress with the testing of the parser at the core of the linter. Why Write a Parser? In looking at the kind of rules that linters support, I have observed that there are typically two categories of rules: general rules and grammar rules. For general rules such as \"tabs should not be used\", it is easy to look at any line in the document being scanned and look for a tab character. For grammatical rules such as \"headings should always be properly capitalized\", that scan is more difficult. The most difficult part of that rule is identifying whether any given piece of text is considered part of a header, thus engaging the rest of the rule. From experience, to properly determine which part of grammar maps to which part of text requires a capable parser, written to the specifications of the language to be parsed. Based on my research from the last article , all of the parsers that I found only translated Markdown into HTML, not any intermediate form. Since I need a clean stream of tokens before translation to HTML, the only option is to write my own parser which will output a clean stream of parsed Markdown tokens. As I am writing my own parser, I need to have a good set of tests to ensure that the parser works properly. But where to start? Where to Start With The Tests? Referring back to my article on software reliability , the 2 main types of tests that I need to decide on are scenario tests and unit tests. In a nutshell, the purpose of a scenario test is to test the input and outputs of the project and the purpose of a unit test is to test a specific function of a specific components of the project. Getting a hold of how to balance the quantity of tests that I need to write between the two of these types is my priority. As one of the initial requirements is to support the GitHub Flavored Markdown specification , it is useful to note that the specification itself has 637 individual examples. Each example provides for the input, in Markdown, and the output, in HTML. While the output is not at the token level needed to satisfy my project's third requirement, it should be close enough. In looking at each of these examples, I need a solid set of rules that I can apply to the tokens to get them from my desired token-based output to a HTML-based output that matches the examples. It is reasonable to collect these rules as I go when I develop the various types of elements to be parsed. If I tried to do them too far ahead of time, it would invariably lead to a lot of rework. Just in time is the way to go for these rules. Taking another looking at the types of tests that I need to write, I realized that this project's test viewpoint was inverted from the usual ratio of scenario tests to unit tests. In most cases, if I have anything more than 20-30 scenario tests, I usually think that I have not properly scoped the project. However, with 637 scenarios already defined for me, it would be foolish not to write at least one scenario test for each of those scenarios, adding extra scenario tests and supportive unit tests where needed. In this case, it makes more sense to focus on the scenario tests as the major set of tests to write. The balance of scenario tests to unit tests? Given 637 scenarios ready to go, I need to create at least 637 scenario tests. For those scenario tests, experimenting with the first couple of scenario tests to find a process that worked seemed to be the most efficient way forward. Given a simple and solid template for every scenario test, I had a lot of confidence to then use that template for each scenario test that I tackled. And the unit tests? In implementing any parsing code, I knew that I needed helper functions that parsed a specific type of foundational thing, like a tag in an HTML block or skipping ahead over any whitespace. The unit tests are used to verify that those kinds of foundational functions are operating properly, ensuring that the rest of the code can depend on those foundations with confidence. As a bonus, more combinations of the various sequences to parse could be tested without inflating the number of scenario tests. Ground rules set? Check. On to the first scenario test. Starting With the First Scenario Test While it might not seem correct, starting with example number 189, the first test I did write was for GitHub Flavored Markdown example 189 , the first example included in the specification for the paragraph blocks. After solidly reading the specification, the general rule seemed to be that if it does not fit into any other category, it is a paragraph. If everything is going to be a paragraph until the other features are written, I felt that starting with the default case was the right choice. After a few passes at cleaning up the test for this first case, it boiled down to the following Python code. \"\"\" https://github.github.com/gfm/#paragraphs \"\"\" from pymarkdown.tokenized_markdown import TokenizedMarkdown from .utils import assert_if_lists_different def test_paragraph_blocks_189 (): \"\"\" Test case 189: simple case of paragraphs \"\"\" # Arrange tokenizer = TokenizedMarkdown () source_markdown = \"\"\"aaa bbb\"\"\" expected_tokens = [ \"[para:]\" , \"[text:aaa:]\" , \"[end-para]\" , \"[BLANK:]\" , \"[para:]\" , \"[text:bbb:]\" , \"[end-para]\" , ] # Act actual_tokens = tokenizer . transform ( source_markdown ) # Assert assert_if_lists_different ( expected_tokens , actual_tokens ) Breaking Down the Scenario Test It might be a lot to take in all at once, so let us break it down step by step. Start of the Module The start of the module needs to perform two important tasks: provide useful documentation to someone examining the tests and import any libraries needed. \"\"\" https://github.github.com/gfm/#paragraphs \"\"\" from pymarkdown.tokenized_markdown import TokenizedMarkdown from .utils import assert_if_lists_different The most useful and relevant information about the module that I was able to think of was the actual source for the test cases themselves. That being the case, I felt that including the URI to the specific section in the GitHub Flavored Markdown specification was the right choice for the module documentation. For anyone reading the tests, it provides a solid reference point that answers most of the questions about why the tests are there and whether the tests are relevant. Next are the import statements. The first one statement imports the TokenizedMarkdown class, a class that I set up to handle the parsing. Initially this class was a quick and simple skeleton class, especially for the first paragraph case. However, it provided the framework for me to support more use cases while maintaining a uniform interface. The second import statement is used to include a function that provides a good comparison of the contents of the list returned from the transform function of the TokenizedMarkdown class and a simple text list of the expected tokens. Arrange the Data For The Test From all the useful pieces of information that I have learned about testing, the most useful bits about actually writing tests are the K.I.S.S. principle and the use of the Arrange-Act-Assert pattern. The K.I.S.S principle constantly reminds me to not overcomplicate things, reducing the tests to what is really relevant for that thing or task. The Arrange-Act-Assert pattern reminds me that when writing tests, each test I write breaks down into setup, action, and verification (with cleanup occasionally being added if needed). As such, I always start writing my tests by adding a comment for each of those sections, with the rest of the function blank. Once there, it is easy to remember which parts of the tests go where! def test_paragraph_blocks_189 (): \"\"\" Test case 189: simple case of paragraphs \"\"\" # Arrange tokenizer = TokenizedMarkdown () source_markdown = \"\"\"aaa bbb\"\"\" expected_tokens = [ \"[para:]\" , \"[text:aaa:]\" , \"[end-para]\" , \"[BLANK:]\" , \"[para:]\" , \"[text:bbb:]\" , \"[end-para]\" , ] The Arrange part of this test is simple, consisting mostly of easy-to-read assignments. The object to test needs to be setup in a way that it is completely enclosed within the test function. The tokenizer object with no options is assigned to the tokenizer , so a simple assignment takes care of its setup. The source_markdown variable is setup within Python's triple-quotes to preserve newlines and provide an accurate look at the string being fed to the tokenizer. This string is copied verbatim from the example represented by the function, in this case example 189 . The final setup, the array assigned to the expected_tokens variable, takes a bit more work. When I wrote these, I sometimes wrote the expect tokens ahead of time, but often used a known \"bad\" set of tokens and adjusted the tokens as I went. Act (Tokenize) and Assert (Verify Results) With all the work on the setup of the tests, the Act and Assert parts of the test are very anticlimactic. # Act actual_tokens = tokenizer . transform ( source_markdown ) # Assert assert_if_lists_different ( expected_tokens , actual_tokens ) Using the information that was established in the Arrange section of the test, the Act section simply applies the input ( source_markdown ) to the object to test ( tokenizer ) and collects the output in actual_tokens . The Assert section then takes the output tokens and compares them against the expected list of tokens in expected_tokens . Why Not Use Pure Test Driven Development? In a normal project, I usually follow Test Driven Development practices quite diligently, either writing the tests first and code second, or writing both tests and code at the same time. As this was my first version of my first Markdown parser, I was aware that I was going to be adapting the tokens and token formats as I went, eventually arriving at a set of tokens that worked for all scenarios. Knowing that this churn was part of the development process for this project, I decided that a true Test Driven Development process would not be optimal. For this project, it was very useful to adjust the process. The balance that I struck with myself was to make sure that as I coded the parser to respond to a given scenario, I adjusted the tokens assigned to the expected_tokens variable based on the example's HTML output for the equivalent scenario test. This process gave me the confidence to know that as I made tests pass by enabling the code behind the scenario, each individual passing test was both moving towards a fully functioning parser and protecting the work that I had already done in that direction. To be clear, as I copied the template over, I adjusted the function name, the function's doc-string, and the Markdown source text based on the scenario test that I was implementing. The list of tokens in expected_tokens were then populated with a \"best guess\" before I started working on the code to make that scenario pass. In a microscopic sense, as I updated the test and the test tokens before starting on the code, I was still adhering to Test Driven Development on a scenario-by-scenario level. To me, this was a good balance to strike, evaluating the correct tokens as I went instead of trying to work out all 637 sets of tokens ahead of time. How Did This Help? Getting a good process to deal with the large bulk of scenario tests was a welcome relief. While I still needed to create a strategy to deal with that bulk of scenario tests I would need to write (see the next article for details on that), I had a solid template that was simple (see K.I.S.S. principle), easy to follow (see Arrange-Act-Assert pattern), and would scale. This was indeed something that I was able to work with. What About the Unit Tests? Compared to the scenario tests, writing unit tests for the parser's foundation functions was easy. In each case, there is a function to test with a very cleanly specified interface, providing for a clean definition of expected input and output. What Comes Next? In the next article , I look at the work that needs to be done and come up with general strategies that I use to implement the parser required for the linter. With the specification's 637 examples as a base for the scenario tests, good planning is needed to ensure the work can progress forward.","tags":"Software Quality","url":"https://jackdewinter.github.io/2019/12/16/markdown-linter-setting-up-parser-tests/","loc":"https://jackdewinter.github.io/2019/12/16/markdown-linter-setting-up-parser-tests/"},{"title":"Markdown Linter - Collecting Requirements","text":"Introduction My website is now up and running, even though in my mind it took forever. To make sure everything was \"just so\", I went through each article with a fine-toothed comb multiple times, each with a slightly different thing I was looking for. In the end, it worked out, but I wished I could have automated at least some of that work and reduced the time it took to do it. And I also have a lingering question of whether I got everything, or did I miss something out? What Is a Linter? A long time ago, when I first heard the term \"lint\", I thought someone was referring to the stuff that you find in the clothes dryer trap that you need to clean out. According to Wikipedia , my guess was close. Like the \"undesirable bits of fiber and fluff found in sheep's wool\" from the Wikipedia article, software linters are used to detect undesirable practices and patterns in the objects they scan. Once pointed out, the development team can then decide whether to address the issue or ignore the issue. Doing My Research I started looking around, and even though there are a few Markdown-to-HTML command line programs out there, finding a solid Markdown linter was another story. I did find a couple of starts at making one, but not a finished one that I could use. The only exception was the NPM-based Markdownlint by David Anson. This VSCode plugin is pretty much a standard for anyone creating content in Markdown using VSCode, with well over 1.3 million downloads as of the writing of this article. By default, as you save articles, this linter executes and produces a list of issues in the Problems section at the bottom of the VSCode editor. This tool is indeed handy while writing an article, but the act of verifying multiple articles becomes a bit of chore. My general process was to open a document I wanted to inspect, make a small whitespace changes, save the file, and examine the Problems section to see what the linter came up with. Two things were more annoying about this process that others. The first issue is that any issue for any file that is open is displayed in that section. If I wanted to be efficient, it meant closing every other file and just working on a single file at a time. The second issue is that other plugins write their problems there as well. As a lot of my content is technical, there are a fair number of spelling issues that arise that I need to ignore. Once again, neither one of these issues is a bad thing, just annoying. What Are the Requirements? Doing some thinking about this during the couple of weeks that I worked on the website, a decent set of requirements crystalized: must be able to see an accurate tokenization of the markdown document before translating to HTML working with an accurate tokenization remedies any translation problems instead of translating from HTML all whitespace must be encoded in that token stream as-is for consistency, want an exact interpretation initial tokenization for GitHub Flavored Markdown only, add others later initial tests against the GitHub Flavored Markdown specs plans to later add other flavors of parser must be able to provide a consistent lexical scan of the Markdown document from the command line clean feedback on violations extending the base linting rules should require very little effort clear support for adding custom linting rules. written in Python good cross-platform support same language as Pelican, used as the Static Site Generator for my website While there are only 5 requirements, they are important. The first two requirements speak to reliability: the parsed Markdown tokens should be complete. The third requirement is for stability: write against one specification with a solid set of test cases before moving on to others. The fourth requirement is all about usability: the linter can be run from any appropriate command line. Finally, the fifth requirement is about extensibility: add any needed custom rules. From my point of view, these requirements help me visualize a project that will help me maintain my website by ensuring that any articles that I write conform to a simple set of rules. Those rules can be checked by a script before I commit them, without having to load up a text editor. Simple. Why Is This Important to Me? Writing this section, it took me a couple of tries to word this properly. In the end, I settled on a basic phrase: It is a tool that I can use to make a software project better. In other parts of my professional life, I take a look at things such as a Java project and try and improve the quality of that project. The input is mainly Java source code and the output is mainly JAR files that are executed by a JVM. My website is no different. Remove Java source code and insert Markdown documents. Remove JAR files executed by a JVM and insert HTML files presented by a browser. There are a few differences between the two types of projects, but in all the important ways, they are the same. I took the time to manually scan each article for my website multiple times before I did my website's soft release. To me, it just makes sense that there should be an easier way to perform that process. Easier in terms of time, and easier in terms of consistency. Unless I am missing something out there on the Internet, the only project that came close to fulfilling my requirements was Markdownlint , and it still had some things missing. I came to the realization that to be able to lint a Markdown file against a set of rules, I was going to have to write my own Markdown parser. In the last couple of decades of professional life, I have written many parsers, so that part of the project does not scare me. Due to the great works of the people at the GFM site , there we a solid number of test cases that I can test the parser against. The extensibility issue would make me look at different ways to integrate code into my tool, so a plus there. All in all, a decent number of things I must get right, but nothing too far out of my field of experience. Sure, it would be hard in places… but also a challenge! Just the kind of thing I like! What Comes Next? In the next article , I start breaking down the requirements for the Markdown parser and document how I will setup the tests for it. As I am parsing a well-known format with varying implementations already available, it is important to stay focused on one implementation and have a solid set of tests to ensure I don't go backwards in my development.","tags":"Software Quality","url":"https://jackdewinter.github.io/2019/12/08/markdown-linter-collecting-requirements/","loc":"https://jackdewinter.github.io/2019/12/08/markdown-linter-collecting-requirements/"},{"title":"Website Soft Launch!","text":"Well, it took a lot of work, but the website is finally tuned up the way I want it to be! I still have a number of small things to do to get the website up and running at 100%. However, I am doing this soft-launch to start publishing articles on a regular basis instead of batching them up. In the crawl-walk-run model , this is me moving my website from the crawl stage to the walk stage. The website itself was mostly ready about 2-3 weeks ago, near the start of November. But as this is something public with my name attached to it, I wanted this soft-launch to be a good representation of what I can do and what I can say. As such, I went over the website with a keen eye for detail during the period. There were a number of days where I just went through each individual article, one at a time, looking for something specific. On one day, it was making sure the categories were good. On another, it was making sure the tags I had selected for each article made sense. On others, it was just scanning the articles and making sure they were reflecting my voice. As this is a learning experience for me, please visit the pages on the site and leave constructive comments about the content or the style! Remember that everyone has had help learning things in their live, and pay it forward!","tags":"Website","url":"https://jackdewinter.github.io/2019/12/01/website-soft-launch/","loc":"https://jackdewinter.github.io/2019/12/01/website-soft-launch/"},{"title":"The Inspiration For Jack's Digital Workbench","text":"Growing up in the late 70s and early 80s, the only computers that were available to me at the time were the Commodore line of computers locked up in schools. To feed my growing and inquiring mind, that left electronics and mechanics to experiment with, and expendable examples of both were hard to come by. My only choice was to scrounge around for things in various stages of disrepair, and upon getting one of those uncommon finds, take it downstairs to our unheated garage and trying to \"fix\" it. Far from a stellar place to work, it was not heated, not ventilated and rarely keep clean. And at the head of that garage, on the wall adjoining the basement, was a solitary, ugly workbench fastened to the wall. That workbench was often the birthplace of many far-fetched dreams of what I could do if only I could properly figure the thing laid out before me. I almost never had the right tools, so to be totally honest, more things got \"more broken\" than ever got fixed or reassembled into something else. Even with the right tools, while I believe my ability to make it work again would have increased, the drive and knowledge to do so was probably not there. But even with a veritable graveyard of things that were not completed, I chose not to dwell on the failures or make excuses for why I didn't finish them. Instead, every time I approached that bench, I chose to dream and move forward. Looking back, I fondly remember the times I stood at that bench with the single, bright work light in my eyes. Back then, it was always the journey and the creative imagination that was important. The only pressure to produce something concrete was when my mom asked \"So, what have you been working on?\" However, as I grew older, I realized that my mom's question was equal parts sincere interest on her part and her desire to have me grow past that graveyard of unfinished projects. I believe that she wanted me to be able to channel that creativity into the something tangible. Not so she could show off my work to others, but in a sincere effort to help me grow. Things are a lot different some 40 years later. Even though I longer use that old workbench, I try and capture a lot of that creativity and wonder on a daily basis, both in my professional life and home life. The projects I undertake are no longer far-fetched, but grounded in reality. As a result, I am better at selecting the projects that I want to work on and I am more successful in carrying them to completion. When a project fails, I embrace one of the Mythbuster's idioms, \"Failure is always an option\" and try and learn what failed and how to deal with it next time. While a small number of projects till end up in my project graveyard, but I keep the larger number of successful projects on proud display in my workplace and my house. So why start a blog and call it \"Jack's Digital Workbench\"? The simple answer is that I started blogging to help me keep a digital form of that garage workbench alive and renewed. I want to inspire people in the same way that my mother has done in the past and continues to do so today. I want to inspire people to dream big, following each dream up with a grounding that enables them to work on that project and gain confidence in their skills along the way. I want to inspire people to try and take something on, taking any failures along the way as teaching or learning moments and not as dream killers. Basically, the reason for Jack's Digital Workbench is to communicate with others and to inspire them in a positive manner to allow them to, in turn, inspire the people around them.","tags":"Personal","url":"https://jackdewinter.github.io/2019/11/15/the-inspiration-for-jacks-digital-workbench/","loc":"https://jackdewinter.github.io/2019/11/15/the-inspiration-for-jacks-digital-workbench/"},{"title":"Software Quality: Reliability","text":"In the main article titled What is Software Quality? , I took a high level look at what I believe are the 4 pillars of software quality. This article will focus specifically on the Reliability pillar, with suggestions on how to measure Reliability and how to write good requirements for this pillar. Introduction From the main article on What is Software Quality? , the essence of this pillar can be broken down into two questions: Does the software do the task that it is supposed to do? Does the software execute that task in a consistent manner? This article will take an in-depth look at common types of tests, discussing how those tests can help us gather the information necessary to answer those questions. At the end of this article, the section How To Measure Reliability will use that information to provide a cohesive answer to those questions. How Does Testing Help Measure Reliability? As discussed in the main article's section on Reliability , many articles on testing and reliability refer to a test pyramid that defines the 4 basic types of reliability tests: unit tests, functional/integration tests, scenario tests, and end-to-end tests. While those articles often have slightly different takes on what the pyramid represents, a general reading of most of those articles leaves me with the opinion that each test in each section of the pyramid must pass every time. With tests and reliability being closely related, it is easy for me to draw the conclusion that if tests must pass every time, then reliability is a binary choice: they all pass and the project is reliable, or one or more fail and the project is not reliable. As such, my main question is: Does it have to be a binary choice? Are the only two choices that either all tests did pass or all tests did not pass? If the answer to that question is a binary answer, then the answer is simple: it is either 100% reliable or 0% reliable. More likely, there are other answers that will give use a better understanding of how to measure reliability and how to interpret those measurements. Can We Identify Groups of Tests? Before determining whether or not reliability is a binary choice, I feel that it is important to make some foundational decisions on how to measure reliability based on the types of tests that are already identified. To aid in making those decisions, it helps to examine the four categories of tests, looking for groupings between them. Using the definitions established in the main article, unit tests are used to test the reliability of individual software components and functional tests are used to test the reliability of more than one of those components working together. Both of these categories are used to determine the reliability of the components themselves, and not their objectives. As such, they make for a good grouping as they have a common responsibility: technical reliability. Observing the scenario tests and end-to-end tests through a similar lens, those tests are used to determine whether or not the software project meets its business requirements. The end-to-end tests are often a set of tests that are very narrow and deep of purpose. At a slightly lower level, the scenario tests provide extra support to those end-to-end tests by breaking those \"bulky\" end-to-end tests into more discrete actions matched to the overall business use cases for the project. A good grouping for these tests is by what they: business reliability. Another way to think about it is to view the groups of tests in terms of whether or not they are inside or outside of the black box that is the software project. The first group of tests verify the inside of that black box, ensuring that all of the technical requirements or \"what needs to be done to meet expectations\" are met. The second group of tests verify the outside of that black box, ensuring that all of the business requirements or \"what is expected of the project\" are met. [Add picture of pyramid showing inside and outside?] Give Me an Example For the follow sections, I use the example of a simple project that uses a data store to keep track of contact information. By providing a simple example that most developers have encountered before, my hope is that it will make it easier for the reader to picture the different types of tests and how they will interact with their team's project. As I examine each type of tests, I try and explain my thinking on what I write and how I write it for that group of tests, hoping to guide others on making better decisions for their testing strategy. Note that I do not believe that the definition of \"data store\" is relevant to the example, therefore the definition of \"data store\" is left up to the reader's imagination and experience. End-To-End Tests Starting at the top of test pyramid, each end-to-end test needs to be a solid, representative test of the main focus of the project itself. These tests are usually a small set of tests meant as a solid litmus test on whether the software project is reliably meeting the requirements of the project. In forming the initial end-to-end tests, my intent is to start with a focus on positive cases which occur more than 60% of the time. For the example project, I started with a test to successfully add a new contact. As a nice bonus, starting with that test allowed me to add the remove, list, and update end-to-end tests, as they all need to add a new contact as a foundation of each of those 3 individual tests. Given my experience measuring quality, I believe that all of those tests together provide that check with confidence for the example project. If I had found out that the number of end-to-end tests I needed was more than a handful of tests, I would have then examined the requirements and try to determine if the project had too many responsibilities. Doing this exercise with a new project often helps me figure out if the project is properly scoped and designed, or if it requires further refinement. Having identified the end-to-end tests for a project and assuming that no further refinement is necessary, I rarely write source code for these tests right away. Most of the time I just add some simple documentation to the project outlined in pseudocode to capture that information. I find that the main benefit of doing this in the early stages is to provide a well-defined high level goal that myself and my team can work towards. Even having rough notes on what the test will eventually look like can help the team work towards that goal of a properly reliable project. Scenario Tests Still on the outside of the box, I then add a number of scenario tests to expand on the scope of each of end-to-end tests. For these tests, I focus on use cases that the user of the project will experience in typical scenarios. The intent here is to identify the scenario tests that collectively satisfy 90% or more of the projected business use cases for a given slice of the project. For the example project, adding a test to verify that I can successfully add a contact was the first scenario test that I added. I then added a scenario for the negative use case of adding a contact and being told there are invalid fields in my request and a third for a contact name that already existed. Together, these scenarios met my bar for the \"add a contact\" slice of the scenarios for the project. It is important to remember that these are tests that are facing the user and systems they interact with. Unless there is a very strong reason to, I try and avoid scenario tests that depend on any specific state of the project unless the test explicitly sets that state up. From my experience, such a dependency on external setup of state is very fragile and hard to maintain. It also raises the question on whether or not it is a realistic or valuable test if that setup is not something that the project itself sets up. Why only those 3 scenario tests? Here is a simple table on what types of scenario tests to add that I quickly put together for that project. The estimates are just that, examples, but helped me determine if I hit the 90% mark I was aiming for. Category Percentage Scenario Success 60% Add a contact successfully to the project. Bad/Invalid Data 25% Add an invalid contact name and validate that a ValidateError response is returned. Processing Error 10% Add an contact name for an already existing contact and validate that a ProcessingError response is returned. I sincerely believe that between those 3 scenario tests, I can easily defend that they represent 90%+ of the expected usage of the project for the specific task of adding a contact. While the percentages in the table are swags that seem to be \"plucked out of thing air\", I believe they can be reasonably defended 1 . This defense only needs to be reasonable enough to get the project going. Once the project is going, real data can be obtained by monitoring and more data-driven percentages can be used, if desired. How did I get there? From experience, there are typically 4 groups of action results, and therefore, scenarios: the action succeeded, the action failed due to bad data, the action failed due to a processing error, or the action failed due to a system error. The first scenario test represents the first category. Unless there was a good reason to show another successful \"add\" use case, I will typically stick with a single \"add\" test. As the goal is to achieve 90% of the typical use cases for the project, unless a variant of that success case is justified by it's potential contribution towards the 90% total, it can be better performed by other tests. In addition, tests on variations of the input data are better performed by unit tests and functional tests, where executing those tests have lower setup costs and lower execution costs. The second scenario test is used to satisfy the second group of tests where the data is found to be bad or invalid. In general, I use these to test that there is consistent error handling 2 on the boundary between the user and the project. At this level, I ideally need only one or two tests to verify that any reporting of bad or invalid data is being done consistently. By leaving the bulk of the invalid testing to unit testing and/or functional testing, I can simulate many error conditions and check them for consistent output at a low execution cost. To be clear, if possible I try and verify the general ability that consistent error handling is in place and not that a specific instance of error is being reported properly. The third scenario test is used to verify the third group of tests where data is valid but fails during processing. Similar to the second group of tests, there is an assumption that the reporting of processing errors should be done consistently. However, as most processing errors result due to a sequence of actions originating from the user, representative types of processing errors should be tested individually. The key to this type of scenario tests is to represent processing errors that will help the group of scenario tests hit that 90% mark. Relating this to the example project, getting a \"already add a record with that name\" response from the project is something that would occur with enough frequency to qualify in my books. From experience, the fourth group of tests, testing for system errors, rarely makes it to the level of a scenario test. In this example, unless a system error is so consistent that it was estimated to occur more than 10% of the time, a higher priority is placed on the other types of responses. One of the exceptions to these generic rules are when a business requirement exists to provide extra focus on a given portion of the interface. These requirements are often added to a project based on a past event, either in the project or in a related project. As the business owners have taken the time to add the business requirement due to its perceived priority, it should have a scenario test to verify that requirement is met. In the contact manager example, I made a big assumption that unless there were requirements that stated otherwise, the data store is local and easy to reach. If instead we are talking about a project where the data is being collected on a mobile device and relayed to a server, then a test in this last group of system errors would increase in value. The difference that this context introduces is that it is expected that project will fail to reach the data store on a frequent basis, and hence, providing a scenario for that happening helps us reach that 90% goal. Commonalities between End-to-end tests and scenario tests While I took the long way around describing end-to-end tests and scenario tests, I believe the journey was worth it. These two types of tests test against the external surface of the project, together painting a solid picture of what that external surface will look like once the project is done. For both of those tests, the project needs clear business requirements on what benefit it provides to the user, which will be highlighted by translating the requirements into the various tests. By including either actual data (for existing projects) or projected data (for new projects) on the usage patterns for that project, the requirements can be prioritized to ensure the most frequently used requirements are more fully tested. For each of those requirements and goals, the team can then set goals for the project based on those documented requirements. By codifying those goals and requirements with end-to-end and scenario tests, you firm up those goals into something concrete. Those actions allow the team to present a set of tests or test outlines to the authors of the requirements, validating that things are going in the right direction before writing too much source code or setting up of interfaces with the user. That communication and changing the course before writing code can save a team hours, days, or weeks, depending on any course changes discovered. What happens if the requirements change? The project has a set of tests that explicitly test against the outside of the box, and informs the team on what changes will be needed if that requirement change is applied to the project. At the very least, it starts a conversation with the author of the requirement about what the external surface of the project will look like before and after the change. With that conversation started, the team can have a good understanding of how things will change, with some level of confidence that the change is the change specified by the requirements author. Unit Tests and Functional Tests Transitioning to inside of the black box, unit tests and functional tests are more understood by developers and more frequently used than end-to-end tests or scenario tests. The unit tests isolate a single component (usually a class) and attempt to test that each interface of that component and is functioning properly. The functional tests do the same thing, but with a single group of components that work together as a single component rather than a single component itself. From an implementation point of view, the main difference is in how these tests are created. Unit tests, as they are testing a single component, should only contain a project reference to the one component being tested. If the components are created properly and have a good separation from the rest of the project, this should be achievable for a good number of components for the project, especially the support components. Therefore, the degree to which these tests are successful is determined by the amount of clean division of responsibilities the project has between it's components. Functional tests complete the rest of the inside-of-the-box testing by testing individual components with related components, in the way they are used in a production environment. With these tests, the degree to which these tests are successful is the ability to inject the project dependencies into one or more of the components being tested, coupled with the clean division of responsibilities needed for good unit tests. While using a concept such as the interface concept from Java and C# is not required, it does allow the injection of dependencies to be performed cleanly and with purpose. To enable groups of functional tests to be as independent of the components outside of their group as possible, mock objects are often used to replace concrete classes that are part of your project. If interfaces are used in your project to allow for better dependency injection , your functional tests can create mock objects that reside with your tests. This provides more control and reliability on what changes you are making from the live instance of the interfaces, for the sake of testing. If interfaces are not supplied for better dependency injection, a mocking library such as the Java Mockito are required to replace test dependencies with reliable objects. Back to our example Using the example project as a template, we know from the section on scenario tests that we need to test for valid inputs when adding a new contact. To add coverage for the component containing the \"add a contact\" logic as a unit test, it's success is determined by how much of the handling the external interface is in the one component. If that component contains all of the code needed to handle that external request in one method, it is extremely hard to test that component without bringing in the other components. That is definition of a functional test, not a unit test. As an alternative, if the validation of the input can be condensed into it's own component and removed from that method, that validation component can be unit tested very effectively. Applying that refactoring pattern a couple of more times in the right ways, the project's ability to be functionally tested increases. As an added bonus, depending on how the refactoring is accomplished, new unit tests can be added based on the refactoring, gaining measurable confidence on each additional component tested. Using the adding a contact example again, having refactored the input validation to a validation class could be followed by the following changes: create a new component for the handling of \"add a contact\" and decouple it from logic of the handling of the external interface move the user authentication and authorization logic into it's own component move the persisting of the new contact logic into it's own component From a functional test point of view, each of these refactorings makes it easier to test. For the first refactoring, instead of having to rely on all functional testing going through the external interface, which may include costly setup, we can create a local instance of the new component and test against that. If interfaces are used for the remaining two refactorings, then test objects can be used instead of the \"live\" objects, otherwise a mocking library can be used to replace those objects with more predictable objects. How is each group of Tests Measured? On this winding journey to determine how to measure reliability, I explored the relevant elements of the four main types of tests. I believe that I was successful in showing a clear delineation between the two groups of tests and the benefits each group provides. To recap, the outside-of-the-box group validates the expectations to be met, matched against the requirements set out for the project. The inside-of-the-box group validates how those exceptions are met, matched against the external interfaces for the project. These two distinct foundations are important, as the two distinct groups of tests require two distinct groups of measurements. The first group, scenario tests and end-to-end tests, are measured by scenario coverage. Scenario coverage measures the number of tests that successfully pass against the total number of scenario tests and end-to-end tests for that project. As this group of tests is measuring the business expectations of the project, this measurement is a simple fraction: the number of passing tests as the numerator and the number of defined tests as the denominator. The second group, unit tests and functional tests, are measured by source code coverage, or code coverage for short. Code coverage can be specified along 6 different axes: class, method, line, complexity, blocks, and lines. Different measurement tools will provide different subsets of those measurements, but in the end they are all relaying the same thing: the points in the project's source code that are not properly tested. Back to the original question Does it (the measuring of reliability) have to be a binary choice? It depends. In an ideal world, the answer to that question is yes, but we do not live in an ideal world. In the real world, we have a decision to make for either group of tests on what is good enough for the project and that group of tests. If the suggestions of this article are followed, then a condition of releasing the project to a production state is 100% scenario coverage. Anything less than 100% means that critical use cases for the project are not complete, hence the project itself is not complete. To achieve the 100% coverage without adding new project code, updated requirements are needed from the requirements author, say a project manager, to change the composition of the scenario tests and end-to-end tests. This may include removing some of these tests as the release goals for the project are changed. While changing and removing goals and their tests, may seem like cheating to some people, the other option is very risky. It should be evident that if a project is released without all scenario tests and end-to-end tests passing, that team is taking a gamble with their reputation and the reputation of the project. It is better to adjust the tests and goals, and communicate those changes, than to take a risk on releasing something before it meets those goals. Following the suggestions of this article for code coverage is a more nuanced goal, and really does depend on the project and the situation. If architected and designed to support proper testing from the beginning, I would argue that 95%+ code coverage is easy and desirable. If you are adding testing to an already existing project or do not have the full support of the developers on the project, this number is going to be lower. Another factor is the type of project that is being tested and who will use it. If you are creating this project to support people inside of your company, it is possible that one of the requirements is to have a lower initial code coverage target to allow the project to be used right away and alleviate some internal company pressure. If the project is something that will represent you and your company on the international stage, you will have to balance the time and effort needed to meet a higher bar for code coverage with the need to get the project out where it can be used. As with many things, it is a matter of negotiation and balance between the various requirements. What Is Really Important I want to stress that I believe that the important thing is that each project measures where they are against whatever goals they set for their project. The team doesn't need to always maintain a near-100% code coverage measure, but that team needs to know where they stand. This will influence and inform the people that author the requirements and adjust the priorities for the team. Any negotiations within the team can then cite this information and use it to help with the balancing act of adding new features, fixing existing bugs, and enhancing code quality (in this case, increasing code coverage). How To Measure Reliability To answer the question \"Does the software do the task that it is supposed to do?\", scenario coverage is measured. Scenario coverage for end-to-end tests and scenario tests should always be at 100% when a production release of the project is performed. This measurement is binary. Until that release (or next production release) is performed, adding or changing these tests based on the requirements for the next release will inform the team and any stakeholders of how close the team is to satisfying those requirements for that release. To answer the question \"Does the software execute that task in a consistent manner?\", code coverage is measured. Code coverage for unit tests and functional tests should strive for 95% code coverage along all 6 axes with all active tests completing successfully 100% of the time. The test completion percentage must be non-negotiable, but the code coverage percentage must take into account the maturity of the project and the usage of the project. This measurement is non-binary. However, it is important to know your project's code coverage measurement, and how it trends over time. While the measurement is non-binary, it is suggested to create a binary rule that specifies what the minimum percentage is for each axis, failing the rule if that specific metric falls below the goal percentage. Wrapping It Up By breaking down the types of tests that are expected for a given project, the two different types of measurements of reliably become more evident. Scenario coverage is determined by outlining the major scenarios for using a project and writing end-to-end tests and scenario tests against them. Scenario coverage must be a binary measurement at release time. Code coverage is determined by using tools to measure which parts of the code are executed when running functional tests and unit tests. Code coverage is a non-binary metric that must have a minimum bar for coverage that is met for the project, and determined on the merits of the project itself. By using these two measurements, I hope that I have shown that it is possible to provide a way to empirically measure reliability. By having a project be transparent about how it is reaching those measurements and what they are, any team can provide meaningful and understandable measurements of reliability. If asked, I could easily defend the percentages. For the success case, I would assume that half the 60% number will come from first try successes and half the number will come from success that occurred after people fixed errors returned from the other two tests and resubmitted the data. While the other two categories are somewhat guesswork, from my experience validation errors are 2-3 times more common than an \"existing contact\" processing error. Note that in the absence of real data, these are estimates that do not have to be perfect, just reasonable. ↩ In designing any type of project, you should seek to have clear and consistent interfaces between your project and the users of the project. An extension of that statement is that any responses you return to your user should be grouped with related responses and returned in a common data structure or UI element to avoid confusion. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2019/11/10/software-quality-reliability/","loc":"https://jackdewinter.github.io/2019/11/10/software-quality-reliability/"},{"title":"Fine Tuning Pelican: Getting Ready For a Soft-Launch","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction Through the previous 11 articles, I have detailed the various fine tuning that I have done on my website as I get it ready to publish. Having completed and tested most of the preparations to get the website ready for a soft-launch, it was time to think long and hard about what categories and tags to use, and the rules going forward for both. For me, having those two concepts right is going to help me shape the voice of my website and really dial in the content I want to deliver. This article details the steps I took to take those final steps towards publishing. Write Some Starter Articles To make sure that I was not presenting a blank website to any prospective readers, I made sure to write a number of articles to have preloaded for the website's soft launch. I am not sure if others would make the same decision, but my reasoning was this: how can I invite readers to my website if there is nothing to read? I am aware that my writing skill is passable and I am frequently reminded that I am somewhat stubborn. Between these two concepts, it took me a long while to find my voice and come up with some ideas on what I wanted to write. For each of the articles published on this website before this date, there were probably 2-3 times as many articles that \"died\" before they were finished, and another 2-3 times that died before the first sentence was written. At first I thought this was a bit of a failure, but then I remembered some words from a talk on photography that encouraged people to take 100 pictures, just to find the 1 or 2 pictures that were good. The speaker went on to mention that as he gained more experience, he still took 100 pictures, but the number of good pictures increased to the point where his \"good picture percentage\" is now about 20%… and he is happy with that. He went on to say that the most important things he learned were to take better pictures and to not waste time on the picture that were not going to turn out. How Does That Relate To Article Writing? Relating that back to my article writing, I believe that I am getting better in writing the articles for three main reasons. And when I say three main reasons, I am not really talking about first, second, and third place – I am talking about three reasons that I feel are almost equal in importance. The first reason is that like the photographer, I needed experience to grow as a writer and gain confidence with my writing. If I waited until after the soft-launch of the website, then all of my learning through articles would be laid out for everyone to see. From my experience, you only get one chance to make a good first impression… after that, the good impression have less and less impact. By gaining that experience before the launch, it enabled me to make all of the failures I wanted to without worrying about any first impressions being lost. The second reason is that I noticed that the more passionate I was about the subject matter, the more passionate I was about writing the article. By reducing the scope of categories and subjects to the ones that I was most passionate about, my drive to write a good article, start to finish, was more intense. While that passion also caused me to take more time to write the articles (and fuss over them like crazy), when each article was done, I sincerely feel like I have done my best. No regrets, no what-ifs, but a strong feeling that I did my best in writing the article the way I did. Properly Scoping My Categories Stemming in part from the second reason above, the third reason that I think I got better with the articles was that I narrowed the scope of the article topics from \"anything I want to say\" to a small number of categories. By focusing on a smaller and more focused number of categories, I can afford to be more passionate about them. I know that these categories may change over time, but I am going to keep it to 5 categories or less. This part was not as easy as I thought, but also not as difficult. As I wrote the articles, I started noticing trends on which articles were making it further along and kept notes and ideas for new articles. If the category was a good one, I was picking up speed in writing the articles. Another good indicator was how easy it was to come up with new articles for that category, along with a 3-4 sentence \"sketch\" on what the article should be about. It did take me a bit of time to recognize it, but in retrospect, it was obvious what the categories should have been from the start. Determining Good Rules for Tags The last thing I needed to figure out before a soft-launch was the tags. Unlike the categories where I have a number of preset categories to create articles in, the tags are present to allow for better identification of what topics or concepts are in an article. This in turn allows a reader to click on one of the tags displayed for an article and see what other articles are listed there. Whereas for my website the categories are more or less fixed, the first rule I came up with is that the tags should be fluid and truly represent the article. If I want readers to be able to find similar articles, I want to build trust that if I say an article is tagged with a given word or phrase, it will directly reference that tag. On my travels through the internet, there is nothing more disappointing that following a link about something you are researching, only to find that that the data you were promised is only 1 sentence in an article. I don't want that for my website. The second rule that I wrote for myself might seem stupid or silly, but a tag should not be the same or similar to an existing category. Before going through my articles for my soft-launch, there were a number of times where that duplication existed. Now, often that was due to me not having the first rule in place when authoring those articles, but it still happened. This should be an easy one to follow. Finally, the last rule that I came up in preparation for the soft-launch was that tags will be descriptive without being too verbose. When I picture this rule in my mind, what I usually think of is 2 words, where one is an adjective and the other is a noun. While this can sometimes get to 3 words, I don't want it to extend past there as that is descending quickly into the verbose category. As an example, \"website\" is a bad tag as it does not describe what kind of website I am talking about. On the other side, \"how to write a better website\" is too verbose, practically being it's own sentence. In between is \"writing better websites\" which seems to strike a good balance between the two for me. What Was Accomplished Using the other articles in this series as a jumping off point, this article focuses on getting the categories and tags right across the articles. I talked briefly about my philosophy about preloading the site with articles, both to gain experience in writing articles and to determine which articles were the right articles for me to write about. As a natural result of that work, I was able to determine a good initial set of categories for my website, encompassing the topics that I am most passionate about, hopefully ensuring that my best work will be put forward. Finally, I came up with a number of initial rules about tags to help readers of my website find related content on my website with a minimum of effort.","tags":"Website","url":"https://jackdewinter.github.io/2019/11/03/fine-tuning-pelican-getting-ready-for-a-soft-launch/","loc":"https://jackdewinter.github.io/2019/11/03/fine-tuning-pelican-getting-ready-for-a-soft-launch/"},{"title":"Fine Tuning Pelican: Enabling Website Crawling","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction Many forms of website crawling are fraught with copyright issues and may be considered unethical, as is discussed in this article on Data Crawling and Ethics . In contrast, there are legal and ethical uses for web crawling, such as providing the data to search engines such as Google and Bing . While Search Engine Registration and Optimization is covered in another article, it is worthwhile to ensure that the website is properly set up to regulate any web crawling that does occur. This article details the setup required to enable this regulation. Why Use A Robots.Txt File? The Robots Exclusion Protocol has been around for almost as long as webservers. As described in the protocol, it manifests itself as a specially formatted robots.txt file located in the base directory of the webserver. While this protocol is not enforceable and remains a suggestion for web crawlers, it does provide for the rules that you have for \"proper\" crawlers accessing your website. For my website, this file exists in the content directory at /extras/robots.txt and has the following configuration: User-agent: * Disallow: Sitemap: https://jackdewinter.github.io/sitemap.xml This instructs any web crawler that is behaving properly of 3 important facts about the website. The first instruction is that the website is okay with crawlers representing any user agents are allowed to access the site. The second instruction is that there are no paths in the webserver that web crawlers are not allowed to access. Finally, the third instruction provides the web crawler with the location of the website's sitemap, detailing the location of each page on the website. These pieces of information are important for different reasons. The first two pieces of information are meant to restrict web crawlers from accessing the site, if so informed. In the case of this configuration, the * value for the user-agent field means that all user agents are allowed, and the empty value for the disallow field means that no parts of the website are disallowed. Between these two instructions, a web crawler can correctly determine that it is allowed to access any webpage on the website, appearing as any type of web browser or web crawler. How To Publish The Robots.txt File Publishing the robots.txt file requires two separate bits of configuration to ensure it is done properly. The first bit of configuration modifies the existing STATIC_PATHS value to add the path extra/robots.txt to the list of directories and files to publish without modification. The second bit of configuration specifies that the file at the path extra/robots.txt , when published without any modifications, will be located at the path /robots.txt at the webserver's root. STATIC_PATHS = [ 'extra/robots.txt' ] EXTRA_PATH_METADATA = { 'extra/robots.txt' : { 'path' : '/robots.txt' } } Publishing the Sitemap Generating a sitemap for Pelican is accomplished by adding the sitemap plugin to the PLUGINS configuration variable as follows: PLUGINS = [ 'sitemap' ] As detailed in the sitemap plugin documentation , while there are defaults for the sitemap, it is always better to specify actual values for each specific website. The values used for my website are as follows: SITEMAP = { 'format' : 'xml' , 'priorities' : { 'articles' : 0.6 , 'indexes' : 0.5 , 'pages' : 0.4 }, 'changefreqs' : { 'articles' : 'weekly' , 'indexes' : 'weekly' , 'pages' : 'monthly' } } In short, the configuration specifies that the format is xml , producing a /sitemap.xml file. The priorities of scanning are articles, then indexes, then pages, with change frequencies roughly backing up the priorities. For my website, the thought behind the values is that articles, and the indices they are part of, will be updated on a weekly frequency while pages will vary rarely changed. What Was Accomplished The purpose of this article was to detail the configuration for my website that supports crawling of the site for information. The first part of this configuration enabled the creation of a robots.txt file and publishing that file as part of the website. The second part of the configuration added the sitemap plugin and tailored the sitemap configuration for the specific balances for my website. Together, this configuration makes me feel confident that the website is well configured for web crawlers, specifically search engines.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/30/fine-tuning-pelican-enabling-website-crawling/","loc":"https://jackdewinter.github.io/2019/10/30/fine-tuning-pelican-enabling-website-crawling/"},{"title":"Fine Tuning Pelican: Connecting with Readers","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction One of the most important things for a website to focus on is to connect to the people that are viewing the website. That focus is usually geared towards the readers that are attracted to the website's content and retaining that focus. To retain this connection, a website needs to employ a number of tools to engage the readers and maintain their attention. Without employing these tools, any readers of the site will be transient and unlikely to return. This article discusses the tools that my website uses to engage and attempt to retain readers. Social Media As social media is so pervasive in today's society, it is pivotal to add links to your social media accounts, as detailed in the Elegant article on displaying social media profiles . For my website, that configuration boiled down to the following configuration: SOCIAL_PROFILE_LABEL = u 'Stay in Touch' SOCIAL = ( ( 'GitHub' , 'https://github.com/jackdewinter' , 'github-alt' ), ( 'linkedin' , 'https://www.linkedin.com/in/jackdewinter/' , \"linkedin\" ), ( 'RSS' , 'https://jackdewinter.github.io/feeds/all.atom.xml' ), ) Together, these configuration settings provide for the Stay in Touch section at the bottom of the right sidebar, containing icons that will take you to social media associated with the website. The one outlier to this is the RSS icon that appears at the end of the other icons. While technically not a form of social media, by following the Elegant steps outlined here , this useful icon is added and placed in a location that is easily accessible for any readers to use in their automation. Allow Readers To Comment To establish a sense of community and utility with the website, it is important to allow readers to make comments on your articles. While some of those comments may not always seem nice, it is important to try and engage with each reader's comment, and figure out a positive manner in which to respond to it. While Pelican does not support any commenting system itself, the Elegant theme has supported Disqus for a long time, and recently added support for Utterances . Going through the obvious features of each of these platforms, I quickly constructed the following table: Disqus Utterances paid tiers free (backed by GitHub) register with Disqus register with GitHub savvy readers may already have GitHub account Based on that information, choosing Utterances was an easy choice for me. Registering with Utterances is also easy, with Elegant having taken care of a number of the steps for us. Enabling Utterances For The Repository Going to the Utterances website , my first pass at trying to figure out how to register was an utter bust, and I confess that I was somewhat confused. Taking a step back, I was able to figure out the following flow: made sure I have a GitHub account went to the Utterance application page clicked on the Configure button at the top right of the page picked the User or Group containing the project where the target repository for the comments is my repository for the website is jackdewinter\\jackdewinter.github.io , so I selected jackdewinter verified my access to setup the account by providing the password this verification may not occur if you have verified your access within the last 10-15 minutes selected the repository jackdewinter.github.io from the drop-down list pressed the Save button to save these changes While I was able to get this flow right on the first try, it did take me an additional time or two to get the flow documented correctly. Other than that, it was really easy to setup, with no misleading steps along the way. Enabling Utterances In Elegant Once I had the GitHub portion of Utterances set up, it was time to setup Elegant to make use of that setup. Following Elegant's article on Comments - Enabling Utterances , I quickly came up with the following configuration changes: UTTERANCES_REPO = \"jackdewinter/jackdewinter.github.io\" UTTERANCES_LABEL = \"Comments\" UTTERANCES_FILTER = False UTTERANCES_THEME = \"github-light\" In order of appearance in the above code block, the comments will appear as issues in the GitHub repository jackdewinter/jackdewinter.github.io tagged with the label Comments . At the bottom of each article, the Comments section is not be filtered out by default, shown to the reader using the github-light theme. With these configuration settings in place, a quick publishing and hosting of the website showed that comments were now enabled for articles! Testing Utterances With the configuration in place, I performed a couple of quick tests of the newly installed comment system. Picking an article at random, I scrolled down to the bottom where a Comments section awaited me. Signing in through GitHub, I was then able to leave a comment that was persisted after a couple of page refreshes. Opening a new tab in the same browser, I navigated over to my jackdewinter.github.io repository and clicked on the Issues button to open the Issues page for the repository. There under the Issues section was the URL of article that I selected with a Comments label on it. Opening up the issue, I was greeted with the test comments that I had entered previously. Tuning Utterances After reading the other articles in that section of the Elegant documentation, two other articles leapt out as being useful. During testing, I had noticed that there weren't any text or images that separated the article from the comments, and I wanted to change that. The article on Comments - Invite Visitors To Comment provided a nice way to do this, so I followed their suggestions and made the following change to the configuration: COMMENTS_INTRO = \"So what do you think? Did I miss something? Is any part unclear? Leave your comments below.\" Another thing that I noticed during testing was that the comments were filed under an issue with the URL of the article. I want to be able to make small changes to the article title if need be, therefore basing the issue on the article's URL is less than optimal. Luckily, the authors of Elegant thought about this problems and have an article on Comments - Thread Id that deals with it. Without any changes, each issue will be attributed to the URL of the article. In the case of this article, that URL ends with: /2019/10/01/fine-tuning-pelican-connecting-with-readers/ By adding the following line to this article's metadata: comment_id : fine-tuning-pelican--connecting-with-readers the issue that is created for the article is instead fine-tuning-pelican--connecting-with-readers . What Was Accomplished At the start of this article, I stressed that a big part about a successful website is the ability to engage readers and maintain their attention. The first tool is to use the social media section of the sidebar that is available with Elegant. The second tool is to have an active comment system for each article that is easy to use. With two small fine-tunings to the Elegant configuration, both of these tools were configured for the website, and working great! While this was one of the aspects of running a website that I was worried about, having great documentation from Elegant ( Comments - Invite Visitors To Comment ) and Utterances made this a snap to setup.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/27/fine-tuning-pelican-connecting-with-readers/","loc":"https://jackdewinter.github.io/2019/10/27/fine-tuning-pelican-connecting-with-readers/"},{"title":"Fine Tuning Pelican: Producing RSS Feeds","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction While it may seem counter-intuitive, the mostly text RSS Feeds have experienced a comeback in the last 5-10 years. Due in large part to a wave of automation, these feeds are mostly set up for computers to read and process new and existing articles, rather than human readers. As such, I felt it was important to provide RSS feeds for my website. This article describes how to set up RSS feeds for a Pelican website. Strictly Speaking… For any purists reading this, note that the output that Pelican provides is actually in the Atom format not pure RSS. This is not a major issue as most RSS readers will accept RSS format or Atom format for their readers. A good article on the differences between the two is presented here . Only Generate Feeds When Publishing During the normal course of writing content, I will often do a simple publish against the pelicanconf.py configuration to see how that content looks when rendered. In my pelicanconf.py configuration, the following lines are present: # Feed generation is usually not desired when developing FEED_ALL_ATOM = None CATEGORY_FEED_ATOM = None TRANSLATION_FEED_ATOM = None AUTHOR_FEED_ATOM = None AUTHOR_FEED_RSS = None Unless you are actively debugging something to do with RSS feeds, there is no need to generate these feeds during development. From a look and feel point of view, each RSS feed contains the same text as the normally viewed article, with all of the theme styling and extras removed. As such, there is usually no benefit to generating the RSS feed until the final publish step. That is why the publishconf.py configuration includes configuration to override the pelicanconf.py which enables RSS feed generation. Generating the Right Types of RSS Feeds To provide the right types of RSS feeds for my website, I provided the following configuration in the publishconf.py files: # Feed Items FEED_MAX_ITEMS = 15 FEED_ALL_ATOM = 'feeds/all.atom.xml' CATEGORY_FEED_ATOM = 'feeds/{slug}.atom.xml' The first line of configuration specifies that none of the feeds should not contain more than 15 items. Without this setting, a website with 200 articles would have all 200 of those articles included in the feed each time the feed was generated. In addition, when each feed was downloaded, it would download all 200 articles. For me, this setting presents a good balance between presenting a decent amount of content and sending too much data. It is very unlikely that I will publish more than 15 articles at a time, so it just seems right. The next two lines of configuration enable the \"all\" feed and the \"category\" feeds. The FEED_ALL_ATOM configuration enables the all.atom.xml feed to be established at the location feeds/all.atom.xml . This feed contains every article is published, in reverse order of publication. The CATEGORY_FEED_ATOM configuration enables the individual category feeds, one for each category that exists. Each on of those feeds is located at feeds/{slug}.atom.xml where {slug} is the category for which the feed is being generated. Based on the above configuration publishconf.py , when this article was written, the feeds produced were: /feeds/all.atom.xml /feeds/github.atom.xml /feeds/markdown.atom.xml /feeds/quality.atom.xml /feeds/technology.atom.xml What Was Accomplished I started with a quick description of why an older protocol such as RSS and Atom are still good things to have in today's website world. I then covered why to not generate RSS feeds until publish time, followed by how to setup and configure the RSS feeds when it was publish time. This effort allowed me to add RSS feeds to my website in a pretty painless manner, and should allow a reader to perform that same task.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/23/fine-tuning-pelican-producing-rss-feeds/","loc":"https://jackdewinter.github.io/2019/10/23/fine-tuning-pelican-producing-rss-feeds/"},{"title":"Fine Tuning Pelican: Custom Error Pages","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction When a webpage is requested that does not exist, it is common for websites to react to the request by displaying a custom error page. While Pelican does not natively ship with this feature, Elegant adds a themed page that does. This article details the changes needed to mark Elegant's custom error page is properly used as \"the\" error page. Configuring Elegant and GitHub Pages Following the instructions from the Elegant Documentation , Elegant can be instructed to generate a properly themed 404 page by adding the 404 string to the DIRECT_TEMPLATES variable in the pelicanconf.py file. After following those instructions, the next time the website is generated, a 404.html file will be generated in the base directory. This file will have all of the trappings of the Elegant theme, and it will display an error page that includes a box to search for what the reader was looking for. The website hosting service that is being used will dictate if there are any extra steps needed to enable the custom 404 page. For GitHub Pages, as long as the file is named 404.html and is in the root directory that is being hosted, GitHub will automatically pick it up and use it as the 404 error page. Note that it seems like the file must exist on the master branch of the GitHub Pages directory in order for that action to take effect. Using Other Themes If you are using a theme other than Elegant, you can replicate some of the behavior that Elegant provides out of the box. In particular, you can define a page called markdown.md somewhere in your contents, and add the following text to it: --- Title : My Custom Page permalink : / 404 . html --- This is my custom error page . The key to this being used as an error page is the permalink: /404.html part of the markdown header. This informs Pelican to always publish the page with output file of /404.html , placing it in the root directory where it will be picked up properly by many site publishers, such as GitHub Pages. What Was Accomplished In this article, I provided some quick information on how to set up a custom 404 page using Elegant, and noted how it will be picked up by GitHub Pages. I also provided some basic information on how to set up a custom page for themes other then Elegant. By using this approach, I was able to have a custom error page that had the theme of my website, allowing the reader to recover in case of a bad URL.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/20/fine-tuning-pelican-custom-error-pages/","loc":"https://jackdewinter.github.io/2019/10/20/fine-tuning-pelican-custom-error-pages/"},{"title":"Fine Tuning Pelican: Markdown Configuration","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction When choosing authoring tools for a website, a primary concern is that the tools are either already used by the authors or easy enough to learn by the authors that it does not allow them to write freely. For me, I regularly author documentation in Markdown using Visual Studio Code for my professional job, so using the same tools for my website was an easy choice. However, unlike the documents I write at work, the Pelican generator utilizes the Python-Markdown generator library which has a few more options than standard Markdown. This article details the Markdown configuration that I have enabled for my website, and why I have enabled the specified configuration. Markdown Configuration The following configuration is the Markdown configuration for my website: MARKDOWN = { 'extension_configs' : { 'markdown.extensions.extra' : {}, 'markdown.extensions.admonition' : {}, 'markdown.extensions.codehilite' : { 'css_class' : 'highlight' }, 'markdown.extensions.meta' : {}, 'smarty' : { 'smart_angled_quotes' : 'true' }, 'markdown.extensions.toc' : { 'permalink' : 'true' , }, } } markdown.extensions.meta This configuration is perhaps the most important extension that I use. This setting enables the Python-Markdown - Metadata feature which enables the processing of the header of Markdown files with metadata about the Markdown that is being authored. While the Python-Markdown processor does not use this metadata itself, the metadata is passed to Pelican and Elegant to allow for configuration of the articles and pages on a one-by-one basis. This choice is actually a requirement for Pelican to work, being provided as one of the defaults for the MARKDOWN configuration element, as documented here . markdown.extensions.extra This configuration enables the Python-Markdown - Extra features which includes support for: abbreviations, attribute lists, definition lists, fenced code blocks, footnotes, and tables. This choice is actually a requirement for Pelican to work, being provided as one of the defaults for the MARKDOWN configuration element, as documented here . markdown.extensions.codehilite This configuration enables the Python-Markdown - Code Hilites feature to provide for special displaying of marked text within given areas of the document. These sections or specially marked text are typically used to display text that represents code for programs or scripts, with more stringent rules on how to display the text. If no specific text format is specified with the text block, such as many of the code blocks in the article on Static Websites: Posting My First Article . If a text format is specified, this feature will try it's best to match it to known highlighters, using color to indicate different parts of the specified text format. This can be seen in a later section of the above article where a code block is used for a sample Markdown article and later in the series where Python configuration is referenced. In both of these examples, the highlighting done to the text is able to be changed according to the type of text being highlighted. This choice is actually a requirement for Pelican to work, being provided as one of the defaults for the MARKDOWN configuration element, as documented here . markdown.extensions.toc This configuration is present as part of the setup for Elegant's Table Of Contents support. This specific value instructs the Python-Markdown - Table of Contents feature to generate permanent links at the end of each header. These links provide the destination URLs that Elegant's Table of Content support use to go directly to a given item in the Table of Contents. I subscribe to Elegant's philosophy on providing a clean reading experience with minimal distractions. By moving the table of contents to the left sidebar and out of the article, I believe the reader can focus more on the article. markdown.extensions.admonition This configuration enables the Python-Markdown - Admonition feature to provide a highlighted box around the indicated text content. These highlighted boxes are themed by Elegant to provide for a good, quick communication of important information to the reader without being too disruptive. An example of admonitions is available in this article on Glanceable Displays . I find that using admonitions in articles allows me to include more useful and descriptive information to the reader. The options of sections of tests in various parentheses and braces, or placing the text in footnotes, doesn't feel right to me, while admonitions, with their colored call outs do. This is a personal preference. smarty This configuration enables the Python-Markdown - SmartyPants feature to provide for more clear representation of various characters and character sequences used in articles and pages. With this feature enabled, the following substitutions are made: the apostrophe character ( ' ) is translated into a left and right single quotes around the words or phrases they surround: ‘this is my phrase' the quotation character ( ' ) is translated into a left and right double quotes around the words or phrases they surround: \"this is my phrase\" double greater than signs ( >> ) and less than signs ( << ) are translated into angled quotes: « and » three period characters in a row ( ... ) are translated into ellipses: and so they said… two consecutive dashes ( -- ) and three consecutive dashes ( --- ) are turned into lengthened dash characters: – and — While I am not 100% sold on this one, I like the effects it has, even though they are small. It just seems to add a bit of a finished touch to the articles. What Was Accomplished This article was created to share the Markdown configuration that I use for my website For each feature that I use I specify what benefit it provides to the articles, along with the reasons that have for using that Markdown feature.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/16/fine-tuning-pelican-markdown-configuration/","loc":"https://jackdewinter.github.io/2019/10/16/fine-tuning-pelican-markdown-configuration/"},{"title":"Fine Tuning Pelican: Pelican Plugins","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction In setting up the website to reflect my decisions on how things should be displayed, I surveyed a number of plugins to use. Some of these plugins were chosen as they are supported by the Elegant theme, and some by Pelican itself. This article details the list of the plugins that I use on my own website and why I decided to use them. Plugins I Use Pelican plugins help shape the look and feel of a website, but they can also get in the way of the content creation. For me, it is important to experiment with plugins to determine whether or not the plugin and the services it presents enhances either the content creation or content display in a manner that I will find positive for my website. If it doesn't meet that metric with an acceptable cost for meeting that metric, then the plugin gets discarded quickly. For the initial setup of my website, I just went for the standard plugins that work well with Elegant, keeping it simple for now. Search The tipue_search plugin provides for a simple yet effect method to catalog the text on the website and to provide for manner in which to search that catalog. Elegant exposes this capability using a search box in the top right corner of each webpage. Instructions on how to configure this plugin are located here . Simply, a website without search would look vary basic, and I wanted to show a finished product. For me, Elegant makes it look nice, while being easy to use. Navigable Table Of Contents Between the extract_toc plugin and some Markdown configuration, Pelican can take a single Markdown tag and generate an accurate table of contents for the article in it's place. Elegant can then take that information out of the main body of the article and display it on the left sidebar in a manner that does not disrupt the reading of the article. Instructions on how to configure this plugin are located here . I subscribe to Elegant's philosophy on providing a clean reading experience with minimal distractions. By moving the table of contents to the left sidebar and out of the article, I believe the reader can focus more on the article. Series Indicators The series plugin provides extra information to Pelican's template engine, to allow themes to show that an article is part of a series. Elegant takes that information and displays the title of the series on the right sidebar, followed by an ordered list of links to each article in the series. Instructions on how to configure this plugin are located here . In a similar vein to how I feel about Elegant displaying the table of contents, having a series navigation on the right sidebar allows for a ready to have a clean reading experience while allowing the reader the freedom to navigate within a series of articles. Previous And Next Article Links The neighbors plugin provides the necessary information to determine the neighbors of the current article, with respect to the time it was written. Elegant displays this information as the links are the bottom of the article, to assist in navigation. Instructions on how to configure this plugin are located here . Quite simply, having the previous and next article links allow a reader to navigate backward or forward through articles, without getting in the way of normal reading of the article. Sharing Article Links The share_post plugin provides information that Elegant than uses to display simple text links at the end of articles. These links allow the reader to share this on their Twitter accounts, through Facebook, or via there email accounts. The main benefit of these links are that they allow the reader to share these posts, hopefully attracting more readers to the website without being tracked. Many of the other \"share\" buttons on other blogs are implement tracking on each link from one website to another, a practice that doesn't sit well with every reader. Instructions on how to configure this plugin are located here . The benefit here is easy for me to quantify. If it is simple and safe to share articles with their friends, they will naturally share the articles. If readers share articles, I will get more readers. Improved Asset Downloads The assets plugin provides for a way for Pelican to take supporting files for the website, such as CSS files and Javascript files, and combine them into a smaller number of files. By performing this ‘minification', the number of downloads for each page or article is reduced, and therefore the pages and articles load time is smaller. Instructions on how to configure this plugin are located here . The benefit here is also easy for me to quantify. From a reader's point of view, I don't want a page taking forever to download… the quicker the better. From a provider's point of view, fewer requests containing fewer bytes means less load on the infrastructure. Reading Time Estimate The post_stats plugin calculates an estimate of the time required to read the article. Elegant displays this estimate at the top of the right sidebar. Instructions on how to configure this plugin are located here . The benefit of this plugin is harder for me to justify, but it falls into my concept of look-and-feel. This plugin provides a good estimate as to how long a reader can expect to take in reading the article, and hence budget enough time for them to read the article without feeling hurried. SiteMap As the sitemap plugin requires a significant amount of configuration, it is detailed along with the configuration for the robots.txt file in the article on Fine Tuning Pelican: Enabling Website Crawling . What Was Accomplished This article walked through the plugins that I currently use for my website, what they are for, and how to install them. To add extra context to each plugin, I also detailed some of the reasons why I selected to use a given plugin, with it's benefits to me.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/13/fine-tuning-pelican-pelican-plugins/","loc":"https://jackdewinter.github.io/2019/10/13/fine-tuning-pelican-pelican-plugins/"},{"title":"Fine Tuning Pelican: Copyright and Creative Commons Notice","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction For anything that you publish, you want to make sure that you clearly state your rights as the author of the publication. In addition, if you want people to use that information in various ways, you want to make sure that you clearly state how they can use that information for their benefit. This article outlines the steps needed to ensure that the website's copyright and any licensing is prominently displayed. Clearly Stating The Copyright and Licensing For copyright and licensing to be effective, you need it to be clearly stated in a place that is visible. One of the reasons I chose the Elegant theme is that it had a solid place for this at the bottom of the page. To add the necessary information for Elegant to display the copyright and licensing, add the following to the pelicancpnf.py file, with your own name instead of mine: # Legal SITE_LICENSE = \"\"\" &copy; Copyright 2019 by Jack De Winter and licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"> <img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/80x15.png\" /> Creative Commons Attribution 4.0 International License</a>. \"\"\" This configuration accomplished three things: Clearly states that the copyright for any information presented in the article, except where I denote that I am referencing someone else's work, is mine. Clearly states the license under which any information presented can be used, using both the Creative Commons icon and text. Provides a link to the Creative Commons page that clearly states what the licensing agreement is. Selecting the Right Licensing While there are various licenses out there, the Creative Commons licenses have the right balance for me between maintaining my ownership of my content and allowing others to use the information cleanly. The Creative Commons Licenses Page gives a good breakdown of what the particulars of each of their licenses allows and does not allow, with easy to use pictures to allow for quick dissemination of the information. While there are other licenses, such as the MIT License , the GNU General License , and others , for me the Creative Commons licenses are very dry and clean cut on what you can and cannot do. As I want people to use the information I am placing on my site, keeping the license simple and clean is one of my priorities. As one of my goals is to help people, educate people, and inspire people, the most simplistic license that allows people to use the information on the website while protecting my copyright was the best solution. Based on those justifications, the CC-BY license was the right choice for me. This version allows for the sharing and adapting of the information on the website as long as any use of the information is given proper credit, or attribution. Basically, feel free to use it, but give credit where it is due. What Was Accomplished In this article, I outlined a discussion of why you want to make sure that you have copyright and licensing information on your website. I then provided a simple way of adding the copyright and licensing information to each page of a Pelican website themed with Elegant. Finally, I had a quick discussion on what you should think about when deciding on the licensing on your website, and why I chose the CC-BY license for my website.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/09/fine-tuning-pelican-copyright-and-creative-commons-notice/","loc":"https://jackdewinter.github.io/2019/10/09/fine-tuning-pelican-copyright-and-creative-commons-notice/"},{"title":"Fine Tuning Pelican: Publishing, Drafts, and Document Status Defaults","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction When creating content for the website, it is often desired to group articles together that are in various stages of readiness for publication. Prior to publication, many content authors want to see how their content will actually look by preparing a draft of the content that only they can see. This article discusses settings that address both of those concerns. What Does the Document Status do? As part of the standard Pelican feature set, an article or page may have a status of hidden , draft or published . If the status is hidden , then it is ignored by Pelican, and no further action is taken. If the status is draft , then it is published under the drafts directory and not registered with any of the navigation maps. Finally, if the status is published , then it is published using the standard Pelican article and page mapping, and it is registered with the standard navigation maps. While there isn't much information on the status metadata tag, it is included in the main pelican documentation on File Metadata . Without any relevant DEFAULT_METADATA settings being changed (more on that in a minute), the default value for status is published . As such, as long as the article is in the right place to be picked up, it will be published as part of the website. If a draft version of the article is desired, then the status metadata must be set to draft as shown in this example markdown article: --- Title : Some Markdown Article Category : Quality Status : draft --- My markdown article . Unlike the normal publishing process, the rendered version of this file will be placed in the website's /drafts directory, most likely with a name such as some-markdown-article.html . While it is obvious that the page was created by looking at the output from the publishing of the website, this page will not appear on any summaries or searches on the website. This can all be changed by changing the status metadata to publish and re-publishing the website. Setting a Document Status Default This information is largely taken from Pelican's Publishing Drafts section. As is mentioned in that section, to change the metadata default for articles and pages from having a default status of published to a default of draft , the following text much appear in the pelicanconf.py file: DEFAULT_METADATA = { 'status' : 'draft' , } While the Publishing Drafts article does mention that to publish articles and pages, their metadata must now include a Status: published metadata line, I feel it does not stress this enough. When writing articles from that point forward, it may be easy to remember to add that metadata tag to each article. However, to ensure that any previously published page or article is still published, each previous article and page must be revisited and that metadata line must be added to those article's metadata section. What Was Accomplished When authoring content, it is often desired to group articles in a series together, often in different states of readiness. This article started by looking at the document status and how it works for Pelican, and then moving on to how to set a new default for a Pelican-based website. Finally, a note was added to Pelican's own documentation on document status to help any readers avoid \"losing\" any published articles if the default document status is changed.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/06/fine-tuning-pelican-publishing-drafts-and-document-status-defaults/","loc":"https://jackdewinter.github.io/2019/10/06/fine-tuning-pelican-publishing-drafts-and-document-status-defaults/"},{"title":"Fine Tuning Pelican: Article And Page Settings","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction When creating content for a website, it is important to be able to create the content in a way that works for you, changing the configuration to suit your needs. For different reasons, when you publish that content, control of how that content is displayed is important to the character of your website. These settings control these aspects of the content publishing for the website. This article goes over some of the more useful settings for pages and articles and how they affect the website. Where Is the Content Located? I prefer to keep different forms of content in different locations, just to make it easier to keep where things are in my head. As such, I have separate content directories for each type of content: ARTICLE_PATHS = [ 'articles' ] PAGE_PATHS = [ 'pages' ] STATIC_PATHS = [ 'images' , 'theme/images' , 'extra/robots.txt' ] In my website's content directory, I have 4 main directories: articles, pages, images, and extra. The first two are easy to explain, and I set the Pelican configuration values ARTICLE_PATHS and PAGE_PATHS to point to those directories. The static paths specified by the Pelican configuration values for STATIC_PATHS are for folders containing anything that does not need transformation but is part of the website. The most important directories in that category are any images that either I provide or are provided by the theme. The robots.txt file will be covered in a future article, but provides guidance for any robots that are crawling the website. As I intend to keep publishing to this blog for a long time, it was important to me to make sure that I have an organization that can scale with me. By keeping each type of content separate from each other, it allows me to find articles quickly. I take this a step further and create a directory for any series that I am writing, keeping both the draft articles and published articles in that directory. An advantage of this is it allows me to ensure a group of articles has a consistent look and feel within that group with little effort. How is the Content Published? For the publishing of website pages, there are very few options for controlling their location, as Pelican assumes that the collection of pages is pretty constant, while the collection of articles will change frequently. As such, there are two main configuration values that control where the article is published. By default, both of these values are set to the same value {slug}.html , publishing all articles in the root output directory. That didn't seem right to me, so I changed these values to the following: ARTICLE_URL = '{date:%Y}/{date:%m}/{date: %d }/{slug}/' ARTICLE_SAVE_AS = '{date:%Y}/{date:%m}/{date: %d }/{slug}/index.html' When the article is published, it will be published to a directory mostly dictated by the publish date of the article, with the article itself written as an index.html file within that directory. For my tastes, this organizes the articles on the website in a manner in which I can quickly see what I published and when. The default values for ARTICLE_URL and ARTICLE_SAVE_AS just published everything in the root output directory, and that just seemed to cluttered for me. How Many Articles To Show For Lists? One of the other Pelican configuration values that I needed to set was the DEFAULT_PAGINATION value. On the main page, where the list of the most recent articles is displayed, I want to show a decent number of articles, but I don't want them to fall off the bottom too much. As such, I set the following configuration value to provide a default pagination of 10 items: DEFAULT_PAGINATION = 10 I am not sure if this is the right value to start with, but it seems like a decent place to start. I expect to make changes to this as I get more feedback from readers, so just giving it a decent starting value was enough. What Was Accomplished When writing content for a website, it is important to be able to create the content and display content in a manner that works for you, the content author. I described the settings that I have for my website which control where the content is stored as well as where the rendered content is published. In addition, I cover a configuration value that dictates the default for the maximum number of articles to show in a list.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/02/fine-tuning-pelican-article-and-page-settings/","loc":"https://jackdewinter.github.io/2019/10/02/fine-tuning-pelican-article-and-page-settings/"},{"title":"Fine Tuning Pelican: Setting Up The Landing Page","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the Disclaimer section below is referred to by link in the other articles in this series instead of replicating it in each article. Disclaimer I have taken efforts to be clear about whether a given fine tuning was made to the Pelican configuration or the Elegant configuration for my website. Any Pelican configuration that I am documenting should be broadly applicable to any Pelican based site, and any Elegant configuration to a Elegant themed site. If you are using a theme other than Elegant, please consult the documentation for that theme to determine if that theme supports something similar. In addition, I want to be clear that I am a contributor to the Elegant theme that I use for my website, helping with the documentation of the theme. While I decided that it is the best choice for a theme for my website, I leave it to any reader to make their own determination as to which theme is best for their own website. I merely reference Elegant in these notes on Fine Tuning as part of the configuration that I had to learn and set up for my own website. Your mileage may vary. Introduction It is important to engage readers of the website and to give them multiple reasons to come back to the website time and time again. The creation of a good landing page for the website is a large part in making that engagement with the readers happen. This article walks through a number of things that help a landing page increase it's engagement with the readers. One of the most important parts of that is to give the website a good name that will stick in their head and display it prominently on the website. Following that, a well thought out introduction to the website will then give the reader a solid idea of what to expect, allowing them to determine for themselves if it is worth their time and effort to return to the website. Finally, adding a list of projects that are related to the website can be a useful tool in demonstrating to the reader why the website is useful by documenting how the website and it's authors benefit other websites and projects. Giving the Website a good name When a reader first comes to the landing page, it is pivotal to have a name for the website that is front and center on the landing page, easy to identify, and one that will stick in their mind. The Elegant theme partially helps with this by putting the site name at the top left corner of the webpage, with no other text or images right next to it. This Pelican configuration is set with the following configuration in the website's pelicanconf.py file: SITENAME = \"Jack's Digital Workbench\" While the title placement helps, it is up to the website owners to determine the name to be used with the website. I put a lot of thought into the name of my website, as detailed in my article The Inspiration For Jack's Digital Workbench . However, prior to me getting the finishing touches on the website for a soft-launch, the name was Jack's Web Site for a couple of months. I wanted to give it a name that reflected what I wanted to communicate, and I believe that waiting to give it the proper name allowed me to really figure out the right title, and not just a placeholder title. Introducing myself to Readers Now that the website's name was configured, it was time to introduce myself to the people who would hopefully become readers of my blog. The first part of this was to follow Elegant's Home Page - Write Welcome Message article and set the title of the landing page to reflect the new name of the site: LANDING_PAGE_TITLE = \"Welcome to \" + SITENAME With the title of the introduction taken care of, it was time to further follow Elegant's advice, this time from the article titled Home Page — Write About Me . Creating a new document in my content directory called /pages/landing-page-about-hidden.md , I copied their sample document and initially trimmed it down to the following: --- Title : What is Jack ' s Digital Workbench? slug : landing - page - about - hidden status : hidden --- This blog is a place for me to ... After building the website and serving it up locally, to make sure that page was being included properly, I started working on the content. Due to the importance of this page, I started working on it in mid-August and didn't finish it until mid-November. While this may seem a bit of a long time and somewhat silly, I wanted to make sure that this introduction was done right. In the end, after 5-6 drafts, I made notes on what I liked and disliked about each draft, then starting from scratch to write the final version in a couple of hours. During all of those drafts, what I was looking for in an introduction was something that was descriptive, yet not too wordy. In most of my drafts, I nailed one section, but the other two failed this test, so the final version took the best parts of a number of the drafts and wrapped them up as one. I believe it took a long time because I was trying to find the right balance for what I wanted to say, without writing what seemed to be an entire article. In the end, I ended up taking one of the more verbose sections of the introduction and made it into it's own article , replacing it with just two sentences. For me, this struck a good balance between communicating what I wanted in that section and keeping the introduction descriptive, but brief. Adding in related Projects I believe that in any website, it is important to show how that website and it's authors contribute or are related to other projects. Elegant supports this configuration out of the box with the PROJECTS configuration value, documented with the Elegant article on Home Page — Projects List . For my website, this was set up as the following: PROJECTS = [ { 'name' : 'Creating and Maintaining This Website' , 'url' : 'https://jackdewinter.github.io/categories#website-ref' , 'description' : 'Notes and articles about the creation and maintenance of this website.' }, { 'name' : 'Elegant Documentation' , 'url' : 'https://elegant.oncrashreboot.com/' , 'description' : 'Documentation repository for Pelican-Elegant theme.' } ] From my research on different authors and their blogs, the ones that ended up grabbing my attention were the ones that not only solved a problem I had, but showed interest in related projects. Those related projects didn't always have to be related to what I was looking for, but they added \"color\" to the website. Without any extra references to other related websites, I found that I viewed that website as a reference resource rather than a website that I bookmarked and frequented. One thing I thought about for a while was whether or not to include the Creating and Maintaining This Website in the projects list. For the longest time, I was stuck between calling it a category (it is a distinct group of articles) and calling it a project (it is stored as one in GitHub). In the end, the balance I achieved with this was to create a category for it that appears after pressing the Categories button on the title bar and not mentioning it in my introduction where I talk about the other categories. I am not sure if this makes sense to any readers, but I felt that the equilibrium achieved by those two choices allow it to be a category but elevate it a bit to something that is a bit bigger than just another category. As the website is somewhat fluid, I'll see how that goes for now, and perhaps change it later. What Was Accomplished The landing page of any website is almost certainly the most important page or article on that entire website. I documented the process I used to capture a reader's attention by selecting a good title for the website, followed by a good introduction to what the website and it's authors are about. As part of this process, I also talked about how I approached these decisions, and how I figured out that I had accomplished my goal for both of these items. Finally, I talked about how I configured a Projects section to detail some of the other projects I am working on, and why having a section like that is important to a website. The landing page of any website can capture the attention of a reader or lose that attention just as easily. It is important to take your time when developing these parts of your website to ensure that these pieces of your website are an accurate reflection of yourself and what you intend to communicate with the website.","tags":"Website","url":"https://jackdewinter.github.io/2019/09/29/fine-tuning-pelican-setting-up-the-landing-page/","loc":"https://jackdewinter.github.io/2019/09/29/fine-tuning-pelican-setting-up-the-landing-page/"},{"title":"Static Websites: Publishing To GitHub Pages","text":"This is the fifth article in a series about setting up my own website using a Static Site Generator. For other articles in the series, click on the title of the article under the heading \"Static Site Generators\" on the right side of the webpage. Introduction In the previous articles, I used Pelican as my Static Site Generator, generating a nicely themed site with some test articles in it. To make sure I can speak with my own voice, I took my time determining what theming I wanted for the website. By taking these deliberate steps, I was able to arrive at my choice for the site's theme with confidence and clarity. This entire journey has been geared towards generating an externally hosted website that is primarily a blog. This article talks about the final step on that journey: publishing my website to the free GitHub Pages platform. Before We Start Please read Operating System Paths from the second article in this series for my view on operating system pathing. (i.e. /mount/my/path vs. c:\\my\\path). To understand the term base project directory and the script text %%%MY_DIRECTORY%%% , please read the explanation under Step 2: Create a Project Directory For The Site , also from the second article in this series. Why GitHub Pages? In looking at a number of sites built on various Static Site Generators (SSGs), it became obvious that a majority of the pages were hosted on GitHub Pages . With that in mind, I looked into GitHub pages to figure out if it was best solution for me. The article What is GitHub Pages? goes into a decent amount of detail, but the summary boils down to: it's 100% free only static content can be hosted don't do anything illegal don't create an excessively large site (over 1 GB in size) don't create a site that is incredibly popular (over 100GB per month in bandwidth) For cases where the last two items occur, their website even mentions that they will send \"a polite email from GitHub Support or GitHub Premium Support suggesting strategies for reducing your site's impact on our servers\". To me, this seemed like a good place to start. I already use Git for source management, so familiarity with the website and tooling is already there. Their documentation is good, and it looks relatively easy to implement. Another plus. Most importantly, there are no fees for upload or serving the content, so I can experiment with various things and not worry about incurring extra charges. Branches on GitHub Pages After doing my research on GitHub, specifically about publishing on GitHub pages , I was confused about one point. From my experience with Git, most people and companies do either repository based development or branch based development. Even less frequent is something called monolith based development. The approach for GitHub pages is not one of those. Repository based development uses Git's distributed nature to create changes on your own repository, only merging the changes into the \"main\" repository when you are sure of your changes. Branched based development is similar, except the branching feature of Git is used on a single remote repository, only merging changes into the \"main\" branch when you are sure of your changes. Monolith development is more dangerous, with committing all changes to a single repository with a single branch. For all three type of development, there is one thread going through all of them: you are keeping versions of a single type of thing in your repository. In a number of sites that I researched, it appeared that they were using a tool called ghp-import . This tool allows for the content for the site to be stored in the content branch of the repository, while the produced website is pushed to the master branch of the same repository. While I can wrap my mind around it, to me it didn't seem like a good idea. As this is outside of my normal workflows, I was pretty sure that at some point I would forget and push the wrong thing to the wrong branch. To keep things simple, I wanted my website content in one repository, and my website content in another repository. That itself raised some issues with my current setup, having the output directory at the same level as the content directory. During my research, I came across the statement that Git repositories cannot contain other repositories. If you do need to have this kind of format, a concept called submodules was recommended. The plugins and themes repositories for Pelican make heavy use of submodules, so I knew it could be done. But after some experimentation with some sample repositories, I was unable to make this work reliably. Also, while I can learn to wrap my mind around it, it seemed like a lot of extra work to go through. In the end, I decided that it was best to keep things simple, keeping 2 repositories that were 100% separate from each other. If I do more research and figure out how to make submodules work reliably, I am confident that I can condense these distinct repositories into one physical repository. With that decision made, I needed to create a new output directory outside of the blog-content directory. I decided to call this new directory blog-output and have it at the same level as blog-content . To make sure it was initialized properly with a local repository, I entered the following commands: mkdir ..\\blog-ouptut cd ..\\blog-ouptut git init Once that was complete, I had to ensure that the pelican-* scripts were changed to point to the new location, taking a simple search and replace over all of the script files. That being completed, I executed each of my pelican-* scripts, to verify the changes were correct, with no problems. To further ensure things looked good, I performed a git status -s on both repositories to be sure I didn't miss anything. While this approach wasn't as elegant as the other solution, in my mind it was simpler, and therefore more maintainable. Adding Remotes Repositories Now that I had two local repositories, one for content and one for output, it was time to make remote repositories on GitHub for each of them. I already had a GitHub account for some other projects I was looking at, so no worry there. Even if I didn't have one set up, GitHub makes it simple to set up a new account on their home page . From there, it was a simple matter of clicking the plus icon at the top right of the main window, and selecting New Repository from the drop down list. The first repository I created was for the content, and I simply called it blog-content , which I entered in the edit box under the Repository Name label. As I wanted my content to be private, I changed the selection from Public to Private and clicked on the Create Repository button. For the other repository, I followed the same instructions with two exceptions. The first exception is that, as the output of Pelican needs to be public to be seen, I kept the selection on Public . The second exception was the name of the repository. According to the User Pages page, to publish any committed pages you need to use a site of the form user-name .github.io and push any changes to the master branch. As my user name on GitHub is jackdewinter , this made my repository name jackdewinter.github.io . If you are using this article as a guide, please note that you will need to change the repository name to match your own GitHub user name. Securing The GitHub Access The first time that I added my remote repositories to their local counterparts, I encountered a problem almost right away. When I went to interact with the remotes, I was asked to enter my user id and password for GitHub each time. This was more than annoying. Having faced this issue before on other systems, I knew there were solutions, so back to the research! Now, keep in mind that my main machine is a Windows machine, so of course this is a bit more complicated than when I am working on a Linux machine. If I was on a Linux machine, I would follow the instructions at Connecting to GitHub with SSH and things would probably work with no changes. To start with, I want to make sure that GitHub has it's own private/public key pair, so I would follow the instructions under Generating a New SSH Key and adding it to the ssh-agent . I would then follow the instructions under Adding a new SSH key to your GitHub account to make sure GitHub had the right half of the key. A couple of Git commands later, and it would be tested. In this case, I needed to get it running on windows, and the Win10 instance of SSH takes a bit more finessing. To make sure the service was installed and operational, I followed the instructions on Starting the SSH-Agent . Once that was performed, I was able to execute ssh-agent , and only then could I use ssh-add to add the newly created private key to ssh-agent . In a nutshell, I needed to execute these commands to setup the key on my local machine: ssh-agent ssh-keygen -f %USERPROFILE%\\.ssh\\blog-publish-key -C \"jack.de.winter@outlook.com\" ssh-add %USERPROFILE%\\.ssh\\blog-publish-key Attaching Remote Repositories to Local Repositories This was the real point where I would see if things flowed together properly. First, I needed to specify the remote for the blog-content repository. Looking at my GitHub account, I browsed over to my blog-content repository, and clicked on the clone or download button. Making sure the link began with ssh , I pressed the clipboard icon to copy the link into the clipboard. Back in my shell, and I change directory to blog-content and entered the following: git remote add origin %%%PASTE HERE%%% where %%%PASTE HERE%%% was the text I copied into the clipboard. As my user id is jackdewinter and the repository is blog-content , the actual text was: git remote add origin https://github.com/jackdewinter/blog-content.git This process was then copied for the blog-output directory and the jackdewinter.github.io repository. Publish the Content to Output Until this point, when I wanted to look at the website, I would make sure to have the windows from the pelican-devserver.bat script up and running. Behind the scenes, the pelican-autobuild.bat script and the pelican-server.bat scripts were being run in their own windows, the first script building the site on any changes and the second script serving the newly changed content. As long as I am developing the site or an article, that workflow is a good and efficient workflow. When generating the output for the actual website, I felt that I needed a different workflow. As that act of publishing is a very deliberate act, my feeling is that it should be more controlled than automatically building the entire site on each change. Ideally, I want to be able to proof a group of changes to the website before making those changes public. One of the major reasons for the deliberate workflow is that, from experience, the generation of anything production grade relies on some form of configuration that is specific to the thing you are producing. For my website, this needs extra testing specifically around that production configuration in order for my confidence in those changes to be high enough that I am confident in publishing it. The most immediate example of such configuration is the SITE_URL configuration variable. While it was not obvious in the examples that I researched, this variable must be set to the actual base URL of the hosting site. Using the Elegant theme, if you click on the Categories button in the header, and then the Home button, it will stay on the Categories page. Looking more closely at the source for the base.html page, the Home button contains an url is defaulted to '‘ . Digging into the template for the base.html page, the value being set for the anchor of that button is href=\"{{ SITEURL }}\" . Hence, for the Home button to work properly, SITE_URL needs a proper value. The default configuration in pythonconf.py for SITE_URL is '‘ , so that needed to be changed. For the developer website to work properly, SITE_URL must be set to ‘http://localhost:8000' in pythonconf.py . This however introduces a new issue: how do I make sure this variable is set properly when we publish the output? Luckily, the Pelican developers thought of situations like this. Back in the second article of this series, Step 4: Create a Generic Web Site , I mentioned a file called publishconf.py . This file was generated as part of the output of pelican-quickstart and has not been mentioned since. This file is intended to be used as part of a publish workflow, allowing the variables from publishconf.py to be overridden. Specifically, in that file, the following code imports the settings from publishconf.py before defining alternate values for them: sys . path . append ( os . path . abspath ( os . curdir )) from website.pelicanconf import * Below this part of the configuration, in the same manner as in pythonconf.py , the SITEURL variable in publishconf.py is set to '‘ . Therefore, when I publish the website with the publish configuration, it will use '‘ for the SITE_URL . To make sure the website publishes properly, I needed to change the SITE_URL variable in publishconf.py to reflect the website where we are publishing to, namely https://jackdewinter.github.io . Now that I took care of that, I just needed to come up with a batch script that makes use of publishconf.py . To accomplish that, I simply copied the pelican-build.bat script to pelican-publish.bat , and edited the file removing the –debug flag and referring to publishconf.py instead of pelicanconf.py : pelican --verbose --output ..\\blog-output --settings website\\publishconf.py website\\content To test this out, I stopped the pelican-autobuild.bat script and executed the pelican-publish.bat script. By leaving the pelican-server.bat script running, I was able to double check the published links, verifying that they were based on the jackdewinter.github.io site where I wanted to publish them. Pushing the Content To The Remote At this point, I had two local repositories, one with commits and one without, and two remote repositories with no information. While I wanted to see the results and work on the blog-output repositories first, it was more important to make sure my work was safe in the blog-content repositories. So that one would be first. Changing into the blog-content directory and doing a git status -s , I noticed a couple of changes that were not committed. A quick git add –all and a git commit later, all of the changes were committed to the local repository. At this point, the changes are present in the local repository, but not in the remote repository. The following command will push those changes up to the remote repository's master branch. git push --set-upstream origin master At this point, I did a quick check on the blog-content repository in GitHub and made sure that all of the repository was up there. Now, in the future, I knew I would be more selective than using git add –all most of the time, but for now it was a good start. So I carefully went through the files that GitHub listed and verified them manually against what was in the directory. I didn't expect any issues, but a quick check helped with my confidence that I had set up the repository correctly. Pushing the Output To The Remote Once that was verified, I carefully repeated the same actions with the blog-output directory but with one small change. In the blog-content directory, I want to save any changes. However, with the blog-output directory, I want to commit everything, ever if there are conflicts. This is something that is done with quite a few static sites, so the workflow is decently documented. As this is an action that I am going to repeat every time I publish, I placed in a script file called pelican-upload.bat : pushd ..\\blog-output git add --all . git commit -m \"new files\" ssh-agent git push origin master --force popd In order: switch to the blog-output directory, add all of the files, commit them with a simple reason, ensure the ssh-agent is up and running, push the committed files to remote repository, and go back to our original directory. If that last git push looks weird, it is. It is so weird and destructive that there are a number of posts like git push –force and how to deal with it and GIT: To force-push or not to force-push . However, even after I looked at the manual page for git push , I was still trying to figure it out. It wasn't until I came across The Dark Side of the Force Push , and specifically the Force Push Pitfalls section of that article, that things made sense. Under new script run pelican-upload.bat Viewing the Webpage To make sure things looked right, I wanted to do a side by side comparison of what I could see in my browser both locally and on the new website. To do that, I opened up one tab of my browser and pointed it to http://localhost:8000/ , and another tab beside it and pointed it to https://jackdewinter.github.io/ . To be honest, while I was hoping there would be no issues, I was expecting at least 1-2 items to be different. However, as I went through the comparison, there was 100% parity between the two versions of the website. What Was Accomplished At the beginning of this article, I had most of what I needed to start selecting a theme. It took some small updates to the configuration to make sure I had a good test site available. This was critical to allowing me to go through each theme I was interested in and see if it was for what I was looking for. While one of the themes proved to be a handful, the experience was good in advising me of possible issues I might have in customizing my own site. In the end, I had a strong choice of the elegant theme, which as benefits, is actively being developed and has great documentation. What's Next?","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/09/22/static-websites-publishing-to-github-pages/","loc":"https://jackdewinter.github.io/2019/09/22/static-websites-publishing-to-github-pages/"},{"title":"What is Software Quality?","text":"When introducing myself to someone professionally, I usually start with the normal \"Hi, my name is …\" that is boiler-plated on nametags the world over. Getting past that initial point, if the person is so inclined, they ask that always fun lead off question \"So, what do you?\" For me, I always respond with \"I am an SDET\" 1 , to which anyone not in the software industry replies back with \"Um.... What is that?\" Spewing out \"It means I am a Software Development Engineer in Test.\", I wait for the response that most people use: \"Oh, so you are a tester.\" Often with gritted teeth, I try and explain that testing is only a small part of what I do. If I think they are still listening, I given them my quick elevator pitch that emphasizes that I focus on helping to produce good quality software by helping to increase the quality of the teams, the projects, and the processes that I am tasked to assist with. Approximately 60-70% the time I win people over with the elevator pitch, and a pleasant conversation continues. The next 20-30% of the time, usually with people not in the software field, I get blank stares and they fixate on the \"test\" in the title rather than the \"quality\" in my description. The remaining people are usually Software Development Engineers or SDEs 2 that for one reason or another, start to tune out. For the percentage of people that I win over, they seem to understand that I focus on quality, but the follow up question is almost always: \"What does quality software mean to you?\" Where do we start? For me, I almost always start at the beginning with requirements. Whether they are spoken or written down, each project has a set of requirements. It could be the requirements are to \"explore my ability to use X\" or \"fix the X in the Y project\" or \"create a project that can help me X\", but every project has requirements. In the software development industry, requirements are often presented to teams that are hard to deal with or are left to the developers to write themselves. This practice is so prolific that Scott Adam's Dilbert site has pages and pages of instance where requirements are talked about . One example is when a manager talks to their team and informs them that some process needs to be faster by 5%. Do they have enough information from that manager to understand the context of the requirement? Do they expect that increase by a specific time to meet their own goals? What does that requirement look like? How do they know when they have achieved it? Is it achievable? If it is achievable, how do they measure progress towards that goal? These are some of the core questions that I believe need answering. As those questions are at the front of my mind, when someone asks me how I define software quality, the first thing I immediately think back to is a course that I once took on setting S.M.A.R.T. requirements . In that class, the main focus was on taking unrefined requirements and curating them to a point where they could be more readily be acted upon. The instructor made a very good argument that each requirement must be Specific, Measurable, Assignable, Realistic, and Time-Related. When it comes to software quality, I believe those same questions needs to be asked with regards to any of the requirements teams put on their software. But to ask those questions properly, we need to have some context in which to ask those questions. To establish that context, it is helpful to have some guidelines to provide a framework for the requirements. Establishing Some Guidelines: The Four Pillars A good general article for anyone interested in software quality is the Wikipedia article on Software Quality . In fact, when asked by people where to get started in the software quality area, I often refer them to this article solely because of the excellent diagram in the Measurements section on the right side of the page. 3 The diagram in the Measurements section correlates very closely to what I believe are the four pillars of software quality: Reliability, Maintainability, Efficiency, and Security. The diagram then shows how their pillars relate to other attributes: Application Architecture Standards, Coding Practices, Complexity, Documentation, Portability, and Technical/Functional Volumes. From there, it provides more lists of how to break things down, with many references to other articles. In short, it is a great place to start from. Measuring Software Quality Before proceeding to talk about the pillars themselves, I feel strongly that we need to discuss the categories that I use for measuring the metrics talked about in the Wikipedia article. My intention is that by talking about the metrics before discussing each of the pillars, you can start building a mental model of how to apply them to your projects as you are reading about them. From my point of view, making that mental transition from something abstract that you read about to something concrete that applies to your work is essential to serious forward momentum on software quality. These metrics typically fall into two categories: seldom violated metrics and positive momentum metrics. The seldom violated metrics category contains rules that define rules that are pivotal to the quality of your project. Each rule are a combination of a given metric and a maximum or minimum weighed against that metric. As a guideline, teams should only ignore these rules on a case by case basis after providing a reason that is good, defensible, and documented. Examples of such metrics are Service Level Agreements (SLAs), Static Code Analysis (SCA) results, and Test Failure Rates. Examples of rules are \"the TP99 for the X API is Y millisecond\" or \"all PMD warnings (Java SCA tool) must be following with a minimal of suppressions\". Furthermore, to make these rules useful and to keep your team honest, your team needs to publish the selected metrics, with a description of what the metrics are, how your team measures those metrics, and why your team is measuring them. The positive momentum metrics category is usually reserved for metrics that are being introduced to an already existing project. When introducing software quality metrics into an already existing project, it is not realistic to expect those metrics to be adhered to in an instant. It is more realistic to expect positive momentum towards the goal until the point when your team achieves it, at which point is moves to the desired seldom violated metrics category. As such, a measure of the momentum of these metrics is used, and is hopefully in a positive direction. Similar to the previous category, your team should publish information about the selected metrics, with the added information on when your team feels they will translate it from the positive momentum category to the seldom violated category. Being consistent on these chosen metrics is very important. While dropping a metric looks better on any reporting in the short term, it usually negatively impacts the software quality, perhaps in a way that is not obvious until later. Adding a new metric will show lower the measured quality in the short term, but increases the measured quality in the long term. Your team can negate the short term impact by paying the immediate cost of making the new metric a seldom violated metric, but that has to be weighed against the other priorities for your project. As with everything, it is a balancing act that needs to be negotiated with your team. Exploring The Four Pillars Having established that S.M.A.R.T. requirements and the two categories for metrics from the previous sections are useful in measuring software quality, the focus of the article can return to the guidelines: the four pillars. Each one of these pillars will look at your software project from a different angle, with the goal of providing a set of data points to formulate a coherent measurement of software quality for that project. In the following sections, I strive to describe each of the four pillars, providing a jumping off point to another article that describes that pillar in a more comprehensive manner. I firmly believe that by providing metrics for each pillar that are specific to your project, with each of those metrics properly categorized into the two measurement categories documented above, that your team will take a decent step forward in clearly defining software quality for your project. Reliability The essence of this pillar can be broken down into two questions: Does the software do the task that it is supposed to do? Does the software execute that task in a consistent manner? Reliability is one of the areas in which \"pure\" testing shines. A lot of the tests that SDEs, SDETs, and testers are asked to write specifically verify if a given object does what it is supposed to do. Unit tests determine whether an individual software unit, such as a class, performs they way it is supposed to. Functional tests or integration tests take that a step higher, determining whether a group of related software units do what they are supposed to do. Another step higher are the scenario tests, which determine whether the software project, as a whole, responds properly to various use cases or scenarios that are considered critical to its operation. Finally, end-to-end tests or acceptance tests determine whether or not a group of projects respond properly from an end user's perspective. This pattern is so widely used, any search for test pyramid , will find many variations of the same theme. Different articles on the subject will stress different points about the pyramid, but they will all generally look like this: This pyramid, or other similar pyramids, are interpreted by authors to indicate a specific things about the tests, to highlight the position of their article. Some of these interpretations are: An article on test volume will typically stress that ~70-80% of the tests should be at the unit test level, ~10-15% at the functional test level, ~5-10% at the scenario level, and ~1-3% at the end-to-end level. An article on test frequency will typically stress that tests near the bottom of the pyramid should complete within 60 seconds and be executed every time the source code is checked in. Tests near the top of the pyramid may take minutes or hours and should be executed once a week. An article on test fragility will typically stress that tests near the bottom of the pyramid are closer to their components, the expectation is that they will not fail. Tests near the top of the pyramid require more orchestration between projects and teams, and therefore, are more likely to failure do to environmental or other reasons. While all of these interpretations have merit, the critical point for me is the issue of boiling down that information to a small number of bite sized observations that can be easily measured and communicated. In the upcoming article Software Quality: Reliability , I will delve more into breaking the Reliability pillar into S.M.A.R.T. requirements and I provide suggestions on how it can be measured. Maintainability The essence of this pillar can be broken down into one question: If you are asked to change the software to fix a bug or introduce a new feature, how easy is it to change the software, how many surprises do you expect to encounter, and how confident will you be about the change afterwards? The best, and most comedic, form of asking this question is captured by this cartoon from OSNews : Maintainability is a single pillar that encompasses the most diverse types of processes and measurements of any of the pillars. The reason for this is that maintainability is often a word that is used without a lot of provided context. For me, a good way to think about maintainability is that it is the cleanliness of your project. Different people will have different experiences, and after asking different people about how \"clean\" the project is, the collected answers will almost certainly by different. Try this in your office with your colleagues. Point to a given area of your office and ask 2-5 people how clean a given area, such as your desk is. Instead of accepting a single answer, dig in a bit as to why they answered the way they did. Most likely, you will get as many distinct answers as people that you talk to. This exercise illustrates how hard it is to give a good answer to how maintainable software a given piece of software is. The best way to provide metrics for maintainability is usually with various Static Code Analysis tools. Almost every mature language has at least one tool to do this, and each tool usually measures a fair number of metrics. These metrics will use established (and sometimes experimental) industry practices to look at the source code of your project and determine if there are issues that can be addressed. In addition to those metrics, those same tools often look for \"problematic\" and \"sloppy\" code. Problematic code is usually some manner of pattern that a fair number of experts have agreed is a bad thing, such as appending to a string within a loop. Sloppy code is usually things like having a variable or a parameter that is not being used, or a forgotten comment on a public method. In addition to Static Code Analysis, teams must continue to strive to have a good set of documentation on what the project is doing, and regularly maintain that documentation. While the \"correctness\" of the documentation is harder to measure than source code, it is pivotal for a project. How much of the information on the various projects that your team supports is in the head of one or two individuals? What is going to happen if they leave the team or leave the company. Your team should not need volumes of information on every decision that was made, but as a team, it is imperative to document the major decisions that affect the flow of the project. It is also a good idea to have solid documentation on building, deploying, and executing the project. Imagine yourself as a new team member looking at the software project and any documentation, and honestly ask yourself \"How much would I want to run away from that project?\" If the honest answer from each member of the team is something similar to \"I'm good\", you probably have a decent level of documentation. A Note On Static Code Analysis Before delving deeper into maintainability, I want to take a minute to talk about Static Code Analysis. Typically, Static Code Analysis is used as a gatekeeper for maintainability, and as such, any suggestions should be strictly followed. However, Static Code Analysis tends to be an outlier to the gatekeeper rule in that the metrics need to be \"bent\" every so often. This \"bending\" is accomplished using some form of suppression specified by the Analyzer itself. Static Code Analyzers tend to fall into two main categories: style and correctness. Any warnings that are generated by a style analyzer should be addressed without fail. In terms of stylistics, there are very few times where deviating from a common style are beneficial, and as such should be avoided. As stylistics can vary from person to person when writing code, it is useful to supplement the style analyzer with an IDE plugin that will reformat the source code to meet the team's stylistics, with the Static Code Analyzer acting as a backstop in case the IDE formatting fails. Warnings generated by correctness analyzers are more likely to require bending. Most correctness analyzers are based on rules that are normally correct, but do have exceptions. As such, your team should deal with these exception by having a follow up rule on when it is acceptable to suppress the exceptions, and specifically on a case-by-case basis. It is also acceptable to suppress the exception after generating a future requirement to address the exception, if your team is diligent on following up with these requests. In both cases, it is important to remember that SCAs are used to help your team keep the project's maintainability at a healthy level. Back to Maintainability In the upcoming article Software Quality: Maintainability , I will delve more into breaking the Maintainability pillar into S.M.A.R.T. requirements and I provide suggestions on how it can be measured. I will do this by presenting the 4-5 metrics that I consider to be useful as well as both patterns and anti-patterns to avoid. [ED: Need to rephrase that last sentence.] Efficiency The essence of this pillar can be broken down into one question: Does the software execute that task in a timely manner? Similar to my analogy of maintainability being the cleanliness of your software, efficiency is whether or not your software is executing \"fast enough\". Coming up with an answer to a question on whether or not something is \"fast enough\" is usually pretty easy. But when you ask for a definition of what \"fast enough\" means, that is when people start to have issues coming up with a solid answer. In my experience, a large part of the reason for that vagueness is usually not having a good set of requirements. As an example, let's figure out what \"fast enough\" means for two different video games that my family plays: Civilization and Rocket League. For the game Civilization (in multiplayer mode), the big delays in the game are the human interactions and decisions required before a player ends their turn. It is also very important that all of the information get conveyed between turns so that the multiplayer server can accurately record actions in a fair and just manner. For this game, \"fast enough\" for the software is largely dwarfed by the delays that the players introduce. However, if we have a game with 12 players, 2 of them human and the other 10 using the game's AI players, then we can start to formulate what \"fast enough\" is for the AI players. It really depends on the context. Rocket League is a different story. Rocket League is a sequel to the game \"Supersonic Acrobatic Rocket-Powered Battle-Cars\" released in 2008. In this game, you play a game of arena soccer using rocket powered cars, each match consisting of a series of games between teams of 1-3 players. Unless there is a LAN tournament between professional teams, it is very rare for more than one player to be in the immediate vicinity of their teammates, and often players are from different states/provinces and even countries. For the client software on the player's computers, \"fast enough\" is measured by latency and packet loss. With each player's action being relayed to the server and then back out to the other players, any packet loss or increase in latency will impact the server's reaction to various inputs from the player's controllers. For this type of game, \"fast enough\" depends on a good network connection and a server that is able to process many actions per second. As you can see from the video game example, efficiency greatly depends on what the requirements of the software are. In the upcoming article Software Quality: Efficiency , I will delve more into breaking the Efficiency pillar into S.M.A.R.T. requirements and I provide suggestions on how it can be measured. Security The essence of this pillar can be broken down into one question: How easy is it for a third party to perform malicious actions with your software? That is one dramatic question. \"Perform malicious actions.\" Wow! I have read all sorts of articles on various news sites about those, but surely they cannot affect my software? That is usually one of the first reactions of a lot of software developers. Just 10 minutes with a security researcher can open your eyes to what is possible. To understand this better, pretend that your software project is on a slide, being viewed through a microscope. If you look at the slide without the microscope, you just see your software on the slide, pretty much the same as any other slide. However, if you increase your magnification by one order of magnitude, you see that your project includes your source code and components developed by other people. You may be following proper security practices, but did they? Another order of magnitude down, and you are looking at the low level instructions for your project and any included components. Once the component was assembled, could a third party have added some malicious code to that component, executing normally until they activate it? Was that malicious code in their from the beginning? Or maybe it is a vulnerability at the source code, machine code, or machine levels? Someone can make a small change to a component to utilize that vulnerability with little effort if they know what they are doing. Reversing our direction, if we expand outwards instead of inwards, we have containerization. Containerization solutions, such as Docker , provides a complete computing environment to execute your software within. Popular with back end development, you encapsulate your software with it's intended operating system platform, reducing the number of platform's you need to design your software for to 1. But with containerization, we also have to ask the same questions of the platform as we did with the software. How secure is the operating system that the container uses as it's base? In today's world of software development, where componentization is key, the software you write is not the only place where security issues can be introduced. However, there are proactive steps you can take to reduce the vectors than users can follow to use your software maliciously. In the upcoming article Software Quality: Security , I will delve more into breaking the Security pillar into S.M.A.R.T. requirements and I provide suggestions on how they it be measured. Back To Requirements Having explored the 4 pillars, it is important to bring the discussion back to the definition of good requirements. Using the information from each of the individual pillar articles in concert with the information on S.M.A.R.T. terminology, your team can request requirements that are more focused. As any focused requirements will be Specific (the S. in S.M.A.R.T.), it is reasonable to expect that any impact on our 4 pillars will be noted. Asking for this change will almost guarantee some negotiations with the team's stakeholders. In my experience, when your team asks for more focused goals from your stakeholders, there will typically be some pushback from those stakeholders at the beginning. If your team has had some requirements mishaps in the past, highlight each mishap and how the ensuing loss of time and focus could have been avoided usually sways stakeholders. Don't point fingers, but simply point out something like: Hey, when we did the X requirement, we all had a different idea on what to fix, and as such,it took X hours of meeting and Y hours of coding and testing to figure out it was the wrong thing. We just want to help tune the requirements process a bit to help everyone try and avoid that waste.\" Most stakeholders are being asked to have their teams do the maximum amount of work possible in the shortest amount of time. By asking that question in such simple manner, you are asking if you can spend a small amount of time up front to hopefully eliminate any such missteps. Most stakeholders will grab on to that as a way for them to look good and for the team to look good, a win-win. What will these requirements look like? The requirements will typically come in two main categories. The first category, requirements focused on fixing bugs or adding features, will typically be the bulk of the requirements. Each requirement should outline any negative impact it will have on any of the metrics. If nothing is added on negative impacts, the assumption is that the impact will be neutral or positive. A good example of this is a requirement to add a new feature to the project. The requirement should be clearly stated using S.M.A.R.T. terminology, because it will remove any ambiguity in the requirements. As any source code added without tests would impact any reliability metrics, reliability tests should be added to meet any seldom violated metrics for your project. In similar ways for the other 3 pillars, it is assumed that any source code added will be a step forward or neutral in terms of quality, not backward. At some point in your project, you should expect that at least a few of the requirements will appear in the the second category: requirements specifically targeted at one or more of the pillars. These requirements allow your team to focus on some aspect of your project where your team feels that the quality can be improved. The big caveat with these requirements is to be mindful of the Achievable and Time-Related aspects of S.M.A.R.T. requirements. Make sure that whatever the goal of these requirements are, they are things that won't go on forever and are not pipe dreams. A good example of this is wanting to improve the efficiency of your project or processes. Without a good requirements that is Specific, Achievable and Time-Related, this can go on forever. A bad requirement would state something like \"Make the project build faster\". A good requirement might state something like \"Reduce the unit test time from 20 seconds to under 15 seconds\", timeboxed to 4 hours. The good requirement has good guard rails on it to keep it from exploding on someone who picks up that work. Publishing Software Quality Having gone through the previous sections and any related articles, you should have a better idea on: how to write better requirements to ask for software quality to be improved what metrics I recommend to use for each of the four pillars how to measure those metrics and integrate them into your projects Using this information as tools, your team can improve the quality of the project at it's own pace, be that either an immediate focus or a long term focus for your project. For any metrics that are in the seldom violated category, the best way to approach them is to make them gatekeeper metrics for your project. It should be possible to execute a great many of the gatekeeper metrics before a commit happens, which is optimal. For the remaining metrics in the seldom violated category and metrics in the the positive momentum category, your team should publish those metrics with every commit or push, giving the submitter that needed feedback. In addition, publishing the metrics to some kind of data store allows your team to determine how the project quality is trending over time, allowing any stakeholders or project members to observe any potential software quality issues and take steps to deal with them. Even for certain seldom violated metrics, it can be useful to track how they are trending, even if they are trending above the gatekeeper lines set for the project. If your team does not publish those metrics in some form, the only data point they have for the project is a binary one: it passes or it does not. From my experience, that binary metric is often a false positive that burns teams due to a lack of information. What Does Software Quality Mean To Me? Software quality means each software project has a plan. When requirements come in to the project, they are detailed using the S.M.A.R.T. terminology. If not specifically geared towards a given software quality pillar, each requirement may specify what kind of impact it has on one or more of the pillars. If not specified, it is assumed that it has a neutral or positive effect on all of the software quality pillars. The goals are also specific, not overly broad, and realistically achieved within a given time frame. Software quality means that metrics are well thought out for each project. Each metric is both defensible and reasonable for that project and that team. Any metrics that are not being used as gatekeepers are published so they can be tracked over time. For additional benefit, non-binary gatekeeper metrics are also published, to further improve the project and the quality of the project. Software quality means ensuring that software projects are reliable. Projects have well thought out tests that are performed at many levels to ensure that the project's components work together to meet the project requirements as well as verify the correctness of the components themselves. These tests are executed frequently, and a large number of them are used as gatekeepers, trying to ensure that only reliable changes are made to the project. When a project is released, the scenario coverage is 100% and the code coverage is either at 100% or whatever percentage the team has negotiated and documented for their project. Software quality means ensuring that software projects are maintainable. This entails sufficient documentation of project goals, architecture, design, and current state. The documentation is coupled with Static Code Analysis to measure a number of maintainability metrics and to gatekeep on most of them, ensuring that the project moves in a positive direction to a higher quality project. Software quality means ensuring that software projects and their processes are efficient. Team process to administrate and maintain the software and the software itself do not have to be blindingly fast, but they need to be as efficient as they need to be for that project and for that team. They do not need to be fast as lightning, only fast enough for the software project itself. Software quality means ensuring that software projects are secure. If third party components are used for the project, those components need to be monitored for vulnerabilities, and any issues that arise must be addressed quickly. Steps are taken, at a level that is appropriate for the type of software project, to reduce the possible ways that an user can use the software project do something malicious. To me, software quality is about the journey, continuously improving quality and showing that progress, while adding new features and fixing bugs at the same time. Wrapping It Up To put it succinctly, software quality for a project is about having a common nomenclature describing the various pillars of quality, having a common way of measuring against each of those pillars, and the publishing of those measures. Therefore, from my point of view, software quality is not a single metric but a collection of metrics and a philosophy. That philosophy is that your team can only really answer that question by having clearly defined goals for your project and it's quality metrics, and steering the project towards those goals. Does every project need to be super high quality? No, not even close. But I firmly believe that each project needs to have a solid understanding of what level of software quality they have in order to negotiate the definition of \"good enough\" for each project. In the United States, where I currently live, I am a Software Development Engineer in Test or SDET. I do not have an engineering degree. In any other country, including my native Canada, I am a Software Developer in Test or SDT. ↩ In the United States, where I currently live, a Software Development Engineer or SDE is the same as a Software Developer in any other country. ↩ Based on my experience, where the article breaks out Size as it's own pillar, I would place it in the Maintainability section. Similarly, while I can understand why they place Indentifying Critical Programming Errors in its own section, I would most likely fold half of the items into the Maintainability section and half of them into the Reliability section. To be clear, I agree with the content they present, it is just the organization that I disagree with on two small points. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2019/09/15/what-is-software-quality/","loc":"https://jackdewinter.github.io/2019/09/15/what-is-software-quality/"},{"title":"Static Websites: Getting Ready For Publishing - Themes and Minutiae","text":"This is the fourth article in a series about setting up my own website using a Static Site Generator. For other articles in the series, click on the title of the article under the heading \"Static Site Generators\" on the right side of the webpage. Introduction In the previous articles, I was able to verify my setup of Pelican, posting a test article that I was able to view in a browser. I was then able to improve the fidelity of that article by using Lorem Ipsum to make sure it looked more like a real article in terms of content and length. Almost as important, I was able to come up with a more efficient workflow for publishing changes as I work on them. To make the jump from authoring to publishing, there are a number of things that I needed to finish: Fixing The Build Warning File Types and File Paths Better Values For Defaults A Default About Page Selecting a Theme Once all of that was completed, I should be ready to publish… so let's proceed! Before We Start Please read Operating System Paths from the second article in this series for my view on operating system pathing. (i.e. /mount/my/path vs. c:\\my\\path). To understand the term base project directory and the script text %%%MY_DIRECTORY%%% , please read the explanation under Step 2: Create a Project Directory For The Site , also from the second article in this series. Task 1: Fixing The Build Warning When the site was regenerated using the pelican-build.bat file, I noticed a warning at the top of the output. WARNING: Docutils has no localization for 'english'. Using 'en' instead. This is an easy one to handle. I went to the pelicanconf.py and changed the value for DEFAULT_LANG from english to en . Running pelican-build.bat again, the warning went away. This was simply changing the value from a human readable format to the ISO 2 letter code for the language, so it was an easy fix all around. Task 2: File Types and File Paths Even though I am creating a static website, there are various categories of content that I feel should be kept separate, for various reasons. Luckily, the contributors to Pelican though of this too, and the following change to pelicanconf.py separated the different forms of content: ARTICLE_PATHS = [ 'articles' ] PAGE_PATHS = [ 'pages' ] STATIC_PATHS = [ 'images' ] This configuration informs Pelican that articles will be contained within the content\\articles directory, pages within the content\\pages directory, and static content, such as images, in the content\\images directory. While this isn't 100% necessary at the moment, I feel that the organization will pay off later in the website's history. To complete this change, I moved the content\\test.md file into the content\\articles\\test.md directory to follow this paradigm. To make the paradigm more complete, I wanted to make sure that the articles that I write have the date they were created as part of their published path. Searching around the Pelican site itself, this was easily accomplished with the following change: ARTICLE_URL = '{date:%Y}/{date:%m}/{date: %d }/{slug}/' ARTICLE_SAVE_AS = '{date:%Y}/{date:%m}/{date: %d }/{slug}/index.html' Task 3: Better Values For Defaults The pelicanconf.py file contains two variables, LINKS and SOCIAL, which are still at their default values. Giving each a better value will give me a better idea of what things will look like with various themes, so it makes sense to change them now to: LINKS = ( ( 'Pelican' , 'http://getpelican.com/' ), ) SOCIAL = ( ( 'github' , 'https://github.com/jackdewinter' ), ) Task 4: A Default About Page In trying to determine what I needed before looking at themes, I noticed one small gap: I had no static pages. While I expect most of my content to by blog posts, there will be the occasional times where I want some kind of page on my site that isn't an article. The first type of page that came to mind was an About page, so I quickly created a new file content\\pages\\about.md with the contents: # About Me This is me . While it is just a placeholder, the intent was that it would give me a good idea of what to expect going forward. Sidebar 1: What Are Themes? In Pelican, extensions are performed using a combination of configuration, pip installs, and git repositories. The configuration changes and pip installs felt natural to me, as they are common paradigms in Python. However, I found the repositories as source took a bit of getting used to. Not too bad, but a bit of change that was good! Themes themselves are each an individual git repository, containing all of the asserts that the theme needs. Luckily there is a site that shows samples of approximately 80% of the themes. While it is a bit to process in one viewing, Pelican Themes currently contains 126 themes, of which 100 of them have images of themed pages. The better themes have 3-4 pages shown in the index, whereas some of the themes only have 1 image. Regardless, it is a lot better than downloading 100 themes and trying each one out! Task 5: Selecting a Theme At this point, I was pretty sure I had all of the assets I needed before I looked at themes. Sample articles. Check. Sample page. Check. Sample Links. Check. Sample social links. Check. It was time to start! Seeing as I hadn't actually read anything in people's blog posts about how hard it was to select a theme, I wasn't sure what to expect. As such, I budgeted a decent amount of time to the effort. In fact, I actually budgeted a whole week for this next section. For me, the choice of theme was pivotal in being able to communicate effectively with readers. To be clear, I didn't need it to be the right choice. I did need it to be the right choice for me and my voice. It was a clear proposition in my mind: come out of the gate with a theme that wasn't me, or take the time and try and nail it. Even if I missed by a bit, I wouldn't regret taking that time. If you are following these articles as a guide, remember that. Give yourself the time to make a solid choice that you believe in. Which Themes to Try? As I was trying to find a theme for myself, I went through the complete list of themes 3 or 4 times, just taking a look and seeing which ones appealed to me and which ones had ideas that I liked. I started a list on the first pass, and with each pass through the list, I whittled that list down based on what I saw. On the final pass, I focused on getting the number of themes down to a manageable 3 themes. For that pass, I found it important, yet difficult, to pare down the choices to 3 themes. It was important, because I didn't want to be stuck analyzing the themes for a long time. It was difficult because there are a lot of good options for themes, and to come up with only 3 options wasn't easy. However, I found that by focusing on my primary goal of ease of use from my first article , both ease of writing and ease of reading, it helped me narrow things down. The use of an actual list, in my case a simple list written in Markdown, was pivotal. At first it contained the URLs of each of the repositories I wanted to look at more. With each pass through the list, it contained more information about individual themes and a smaller number of themes, This approach helped to quickly whittle the selection down to the real contenders. This approach, while pedantic, saved me a number of times, as it became more of a struggle to remember which theme had which look and which features. With my final pass through the list, I arrived at my top pick of the nice-blog theme, with alternates of the blueprint theme and the elegant theme. Nice-blog is simple and uncomplicated, with a decent looking sidebar. blueprint is the theme for the site of Duncan Lock which I found during my research. blueprint had a bit more of a finished techy feel, with a nice layout on the sidebar. Each article had \"X min read\" text with the clock icon really grabbed me, which really appealed to me. Finally, the elegant theme seemed to keep things simple but elegant, with lots of room for creativity. Trying Out The Themes Each of the themes, Nice-Blog , Blueprint , and Elegant , exists in it's own GitHub repository. As such, one approach to downloading the themes was to create a blog-themes directory at the same level as the base project directory , creating a directory for each theme. As Nice-Blog and Elegant are in the main Pelican themes repository, the other approach for those two themes was to clone the Pelican Themes Repository into the blog-themes directory using: git clone --recursive https://github.com/getpelican/pelican-themes For the first approach, I would have to individually add each theme, whereas with the second approach, I can get most of the themes all at once. The was also the concern that regardless of which way I chose for nice-blog and elegant , I would have to use the first approach for blueprint . Was it worth it to have two paradigms? After thinking about it a bit, I decided to go with the first approach, as I only had 3 themes I was interested in. So, on the command line, I entered: mkdir ..\\blog-themes git clone https://github.com/guilherme-toti/nice-blog ..\\blog-themes\\nice-blog git clone https://github.com/dflock/blueprint ..\\blog-themes\\blueprint git clone https://github.com/Pelican-Elegant/elegant ..\\blog-themes\\elegant The plan was to try each of the candidates, writing down notes about what aspects I found good and bad about each. Following the instructions on the Pelican home page, I modified pelicanconf.py to refer to the first theme as follows: THEME = '%%%MY_DIRECTORY%%%\\\\blog-themes\\\\nice-blog' Save the file, switch to the browser and refresh. Check to make sure it changed properly. Look around the site a bit and write down some notes. Easy. While I would end up coming back to this theme later for more information, the first pass was a solid 5 minutes with no issues. Expecting similar behavior, I did the similar change to make the THEME variable point to the blueprint directory. I switched to the browser and refreshed and it was the same as before. A quick examination of the Builder command window, and I got the following notification: CRITICAL: TemplateSyntaxError: Encountered unknown tag 'assets'. Critical error… encountering that was foreboding. I stopped the windows started by pelican-devserver.bat , and buckled down to do more research. This was the start of a long diversion. Off The Beaten Path: Getting Blueprint to Work Note: I have not contacted the developer of the Blueprint theme, and his blog and his theme have not had any recent changes. When I decided to try it out, it was with the knowledge that it would probably require more effort to get it to work. I had hoped to wait for a bit before exploring plugins, as there are many plugins listed on the Pelican Plugins website. In addition, unless you know what you are looking for with a plugin, it's effects are either invisible or difficult to spot. For those reasons, I wanted to wait until the more major variables regarding the website were set before tackling these more minor ones. Adding Required Plugins After doing my research, it appeared that blueprint was dependent upon the assets plugin. While installing Plugins faces the same issue as how to install Themes, I chose to do the \"all at once\" approach for the plugins. The main reason for this was to allow me in the future to try each plugin, figuring out that plugins is worth the impact. As such, having all of the common plugins together made a lot more sense. Similar to the way described above to install all the themes, I created a blog-plugins directory at the same level as the base project directory . Changing into that directory, I issued the following command to pull the contents down to my system. git clone --recursive https://github.com/getpelican/pelican-plugins ..\\blog-plugins Once I had the contents of the plugin repository on my machine, I added configuration to Pelican to point to the new plugin directory. Then I needed to add the assets plugin to satisfy the blueprint theme. This was done by adding the following to pelcianconf.py : PLUGIN_PATHS = [ '../../blog-plugins/' ] PLUGINS = [ 'assets' ] Running the pelican-build.bat script this time, I received the following error, buried deep within the output: WARNING: `assets` failed to load dependency `webassets`.`assets` plugin not loaded. Plugins With Python Package Requirements Luckily, as part of the previous research, this warning was mentioned, and it was because the assets plugin requires the webassets Python package. A quick pip install webassets later, it's time to build the website again. This time, after running the build script, the output ended with the following lines: ... CRITICAL: ValueError: not enough values to unpack (expected 3, got 2) ... File \"XXX\\blog-themes\\blueprint\\templates\\base.html\", line 37, in top-level template code {% for anchor_text, name, link in SOCIAL %} ValueError: not enough values to unpack (expected 3, got 2) Configuration Changes For Plugins Once again, research to the rescue, this being a relatively easy issue. Different plugins and themes have different configuration requirements, and this one is no different. By looking at the error message, mixed with my knowledge of Python, I saw that the plugin is expecting 3 values for the SOCIAL configuration variable: anchor_text, name, and link. A quick look at my current configuration, and I see the default settings of: SOCIAL = ( ( 'github' , 'https://github.com/jackdewinter' ), ) represent the 2 values that the theme is expecting. Needing a third, I simply cloned the first value into the second position: SOCIAL = ( ( 'github' , 'github' , 'https://github.com/jackdewinter' ), ) Wash. Rinse. Repeat. Running the script again, I received the following critical error: ... CRITICAL: TemplateAssertionError: no filter named 'sidebar_date_format' ... File \"XXX\\blog-content\\virtualenv\\lib\\site-packages\\jinja2\\compiler.py\", line 315, in fail raise TemplateAssertionError(msg, lineno, self.name, self.filename) jinja2.exceptions.TemplateAssertionError: no filter named 'sidebar_date_format' More Configuration Changes This time, it took a lot of looking. I did scans over the entire blog-themes directory as well as the blog-plugins and blog-content directories. Just in case. In fact, I had almost given up hope when I started to look at the pelicanconf.py file that Duncan himself used. Down near the bottom were a number of configuration entries, including one for the missing configuration item. from datetime import date ... def month_name ( month_number ): import calendar return calendar . month_name [ month_number ] def custom_strftime ( format , t ): return t . strftime ( format ) . replace ( '{S}' , str ( t . day ) + suffix ( t . day )) def archive_date_format ( date ): return custom_strftime ( '{S} %B, %Y' , date ) def sidebar_date_format ( date ): return custom_strftime ( '%a {S} %B, %Y' , date ) def suffix ( d , wrap = True ): tmp = 'th' if 11 <= d <= 13 else { 1 : 'st' , 2 : 'nd' , 3 : 'rd' } . get ( d % 10 , 'th' ) if wrap : return '<span class=\"day_suffix\">' + tmp + '</span>' else : return tmp # Which custom Jinja filters to enable JINJA_FILTERS = { \"month_name\" : month_name , \"archive_date_format\" : archive_date_format , \"sidebar_date_format\" : sidebar_date_format , } Copying this into my own pelicanconf.py file, it was time to run the build script again. To be honest, for this section of configuration, I started only copying the sidebar_date_format function. Then I realized it needed custom_strftime . Then I realized it needed… It was at that time that I figured it was easier to just copy all of this code over, and if I stayed with the theme, I would see about cleaning it up. With the complete section copied over, running the build script produced the following error: CRITICAL: UndefinedError: 'pelican.contents.Article object' has no attribute 'stats' More Build Iterations Realizing that more of the configuration may be in the pelicanconf.py file, with the above error in mind, I scanned the configuration and noticed a plugin called post_stats . Looking at the documentation for post_stats , it seemed like it would expose that attribute. So, adding ‘post_stats' to the plugins, I re-ran the build, with the attribute error disappearing, only to be replaced with: ModuleNotFoundError: No module named 'bs4' Having looked at the documentation for post_stats , I was able to solve this one right away. To get the proper word count for the stats, the plugin uses the Python Beautiful Soup package (version 4) to scrape the HTML output. Executing pip install beautifulsoup4 , and then rebuilding again, we get the following output: File \"%%%MY_DIRECTORY%%%\\blog-themes\\blueprint\\templates\\footer.html\", line 41, in top-level template code {% for anchor_text, name, link in LINKS %} ValueError: not enough values to unpack (expected 3, got 2) This is the same error as with the SOCIAL variable, and the same solution will work. Changing the LINKS variable to: LINKS = ( ( 'Pelican' , 'Pelican' , 'http://getpelican.com/' ), ) And recompile and… all errors gone. Switch back to the browser and refresh the page. Repeat the entire process as with the nice-blog theme, taking notes. Back to the Final Theme Bracing for the worst, I changed the theme to Elegant and it… just worked. After blueprint , I expected a lot more effort, but it just worked. Counting my blessings, I did the usual playing around and taking notes, then I sat back. From then on, I went back to the three themes frequently and took more detailed notes and observations until I got to a point where I wasn't adding any meaningful notes. It was then that I was sure that I had the information I needed to make my decision. Lessons Learned Before I go on to my decision process, I wanted to go over some things I learned or remembered along the way of finding a theme. Some are obvious, some are not. The more standard things are, the less you are going to have to do to make them work. Blueprint was not in the main collection of themes, and it took a lot of effort to make it work in it's out-of-the-box mode. Elegant and Nice-Blog both worked out-of-the-box with 0-2 modifications to configuration and no extra packages or plugins. The auto-build feature of the generator can hang. There were a number of times I had to Ctrl-C and restart the generator. This seemed to happen more when I was changing the themes or plugins. It also happened when I introduced something that required a new Python package. I didn't regret putting in the work. I wanted to put the work in to each theme on the short list to have a balanced comparison. I wanted to have a good understanding of what it would take to maintain each of the themes. The work I put in gave me the understanding that blueprint would take a lot more work than the others to maintain. As old school as it is, a simple pros and cons list help me figure out the best theme. The first two themes were easy to keep in my head at first. The more themes that I saw, the harder it was to remember any points about each of the themes. By the time I got down to the last 2 themes, I had so many parts of so many themes going through my head. Without the list, I would have been lost. The Final Selection Due to the amount of work required for blueprint , including a number of limitations I encountered clicking around, it was out of consideration right away. One of my core issues from the first article was ease of use, and I didn't get the feeling that blueprint would fall into that category. Looking at the notes a number of times, it was hard to distinguish the two remaining themes from each other. They both had a simple and elegant look-and-feel, and that was making the decision even more difficult. There were a couple of small features that were different, but those missing from one were balanced by ones missing from the other one. It was pretty much a stalemate in my head. After going back and forth a number of times to try and resolve the stalemate, I decided I needed more criteria to help. The first two that came to mind were the documentation and active maintenance of the theme. This was returning to the ease-of-use consideration that was a driving force in my first article, so I knew it would be relevant. It was then that the elegant theme became the winner by a fair amount. So, after about a weeks worth of shifting between themes in \"spare time\", I landed on the elegant theme for my website. It took a couple of days for it to sink in while I was writing this article, but with each passing day, my confidence that I made the right choice increased. I took the time, examined the themes, wrote the notes, and in the end, it resulted in a single, clear choice. With finality I went to the configuration file, and near the top added: # Current Theme THEME = ' %% %MY_DIRECTORY %% % \\\\ blog-other-themes \\\\ elegant' What Was Accomplished At the beginning of this article, I had most of what I needed to start selecting a theme. It took some small updates to the configuration to make sure I had a good test site available. This was critical to allowing me to go through each theme I was interested in and see if it was for what I was looking for. While one of the themes proved to be a handful, the experience was good in advising me of possible issues I might have in customizing my own site. In the end, I made a strong, confident choice of the elegant theme, which as benefits, is actively being developed and has great documentation. What's Next? Now that I have completed all of the major tasks, the next step is to publish the website to an external host. For choices that will become evident, I will be publishing the website to GitHub pages. Completing this task will be covered in the next post Publishing To GitHub Pages . Epilogue: Jack's Notes on Themes In the previous sections, I mentioned taking notes on each themes. To do this, I used the main page, the about page, and the two sample articles as a litmus test. I clicked around the website, looking for things that worked and didn't work, as well as the feel of the website. I also went to the home repository of each theme and checked for how often it was updated, and what kind of support the repository offered for customization. In the end, I came up with three lists of items, with pros and cons, as follows: nice-blog pros starting point minimal but elegant look and feel categories support metadata for articles subtitle, cover image, gallery can change color easily cons doesn't seem to handle TOC properly no tags support simple tracking blueprint pros tags by name and frequency like \"4 min read\" nice blue date format Fri 3rd December, 2010 category and tag support origin of better figures plugin cons too limited otherwise no archives by default documentation for theme very limited elegant pros nice layout very elegant and simple feel category and tag support extensive documentation lots of metadata actively being maintained cons no \"4 min read\" support no links support","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/09/08/static-websites-getting-ready-for-publishing-themes-and-minutiae/","loc":"https://jackdewinter.github.io/2019/09/08/static-websites-getting-ready-for-publishing-themes-and-minutiae/"},{"title":"Static Websites: Posting My First Article","text":"This is the third article in a series about setting up my own website using a Static Site Generator. For other articles in the series, click on the title of the article under the heading \"Static Site Generators\" on the right side of the webpage. Introduction With Pelican installed, it was time to start creating stuff. The first I thing I wanted to do was to figure out if the base site was good by creating a default site and viewing it in a browser. Then I wanted to come up with a very simple test article, publishing it and viewing it to verify my ability to publish and exercise that workflow. Finally, I wanted to take some time to come up with a more representative test article and improve the publishing workflows. Similar to the last article, this is not about leaping forward, but to take a decent step forward in understanding the tools and processes. Before We Start Please read Operating System Paths from the second article in this series for my view on operating system pathing. (i.e. /mount/my/path vs. c:\\my\\path). To understand the term base project directory and the script text %%%MY_DIRECTORY%%% , please read the explanation under Step 2: Create a Project Directory For The Site , also from the second article in this series. Step 1: Verify Our Basic Web Site The first thing I wanted to do is to make sure I didn't mess anything up in the last article, Setting Up the Pelican Static Site Generator . Therefore, it made the most sense that I verify that my website was up and running before I added my first article. After doing some research, it was evident that there are two phases that I need to take care of: building the content and viewing the content. Step 1a: Building The Web Site Content The first article's section on Static Site Generators (SSGs) hopefully made it clear that the focus of SSGs is to render the site content when updated. In the second article, Setting Up the Pelican Static Site Generator , I created the base configuration for a generic website, but did nothing to build that content. To build that content, I needed to find out how take the empty content and generate the output files from it. Looking at some articles, I was drawn to the output of the pelican-quickstart command that generated the base configuration. In particular, I noticed the prompt: > Do you want to generate a tasks.py/Makefile to automate generation and publishing? (Y/n) n I specifically answered No because of my research. If you answer Yes , the template will ask a number of additional questions related to publishing, and it will place the Makefile and tasks.py in the website directory. As my primary machine is a Windows machine, these two files are mostly useless. Their primary value would be to instruct me on what I needed to do to replicate their behavior on the command line or in Python. Thankfully, between the command pelican –help , using the Makefile for reference, and experimentation, I was able to arrive that this command for building the website: pelican --debug --output website\\output --settings website\\pelicanconf.py website\\content Nothing too fancy. I started Pelican in debug mode to have more information about what was going on. Using the above command, the output was placed in the website\\output directory, the settings were retrieved from the website\\pelcianconf.py file, and the content for the website is located in the website\\content directory. I double checked to make sure these were all correct, and it ran once without issue. To make sure that I didn't lose this command, I created a simple script pelican-build.bat in my base project directory, with the above command being the only line in that file. Pelican generated a lot of output, but in the end, it looked like everything was okay. I was excited to check out the new website, so I used my web browser to open the file website\\output\\index.html and got this result: I must admit… it was kind of underwhelming. And then I thought about it a bit. A lot of the stuff that makes web pages powerful are other parts that included by the web page, and may not be loaded if the page is loaded directly from the file system. To properly view the content, I was going to have to host it somewhere. Step 1b: Viewing The Web Site Content If you are like me, hearing or thinking the phrase \"I need to install my own web server\" fills me with read almost right away. Even for a pared down web server, there are usually 3-6 directories to set up, ports to clear with firewalls, and other little things. I must admit, when I started looking around, I was not exactly in a good mood. However, the first 3 websites that talked about Pelican were from the Pelican project's various versions. Looking deeper into the documentation, I found the part of the documentation titled Preview Your Site . Without taking too much away from the documentation, it specified that Python 3 includes a pared down web server that is available for simple uses, like I needed. After a bit of experimentation and fiddling, and I came up with the following lines and placed them in pelican-server.bat : pushd website\\output python -m pelican.server popd Executing that script, I then saw the following output: XXX\\python\\python37-32\\Lib\\runpy.py:125: RuntimeWarning: 'pelican.server' found in sys.modules after import of package 'pelican', but prior to execution of 'pelican.server'; this may result in unpredictable behaviour warn(RuntimeWarning(msg)) WARNING: 'python -m pelican.server' is deprecated. | The Pelican development server should be run via 'pelican --listen' or 'pelican -l'. | This can be combined with regeneration as 'pelican -lr'. | Rerun 'pelican-quickstart' to get new Makefile and tasks.py files. -> Serving at port 8000, server . ... While this is what the official documentation suggests, it does look like it is out of date. Using the output from above, the installed Makefile for guidance, and more experimentation, I replaced the contents of the pelican-server.bat file with the following: pelican -l -p 8000 --debug --output website\\output --settings website\\pelicanconf.py website\\content This time, the output of the script was: DEBUG: Pelican version: 4.0.1 DEBUG: Python version: 3.7.3 DEBUG: Adding current directory to system path DEBUG: Temporarily adding PLUGIN_PATHS to system path DEBUG: Restoring system path WARNING: Docutils has no localization for 'english'. Using 'en' instead. -> Serving at port 8000, server . To me, it looked like the web pages were served up properly, or at least Pelican didn't report any errors to the screen. The only thing left was to actually see if it would allow me to load the generated website to my browser. Instead of loading the file directly into the web browser as before, I entered \"localhost:8000\" in the address bar. To be honest, I took a guess that server . meant the localhost. This time, I got the following result: Unstyled, no unique content, and only locally visible. It's not a great result… but it's a start! Step 1c: Commit Changes Before I went forward, I wanted to save the current state of the website, so back to Git and the git status -s command, whose output was now: ?? pelican-build.bat ?? pelican-server.bat ?? website/__pycache__/ ?? website/output/ In my head, I compared these results to the changes I made: I added 2 script files to help me build and serve the content both file are detected, and I want them as source in my project I built the website, placing the output into website/output the directory was detected, but I want to generate this output, and not persist it from previous experience, the website/ pycache / contains pyc files that are built when the python scripts are interpreted the directory containing these files was detected, but these should never be persisted Using thought processes derived from what I documented in the section Step 5: Committing the Changes from the previous article, it was clear that I needed to git add pelican-build.bat and git add pelican-server.bat to persist those changes. However, the two items that I did not want to persist would require different tacks for each one. The first case, website/output/ , is a directory like the virtualenv directory in the section # Step 2: Create a Project Directory For The Site from the previous article. Therefore, I edited the .gitignore file to include the directory by name. That was the simple one. The second case was more complex, the website/ pycache / directory. This directory only exists to contain compiled Python byte code designed to speed up execution on subsequent passes. If I add the directory, as with the first case, it only takes care of that one directory. If I run any Python scripts from other locations, I will have to add those directories too. This was not an efficient option, hence I edited the .gitignore file to ignore the files themselves, by specifying &ast.pyc as the pattern to ignore. Therefore, following those changes, the .gitignore file looked like: virtualenv/ website/output/ *.pyc Using git status -s , I verified that only the 2 scripts were being added, as well as the .gitignore file itself being changed. Quickly adding git add .gitignore , I then used git commit -m \"my message\" to commit these changes to the repository. So let's take inventory. We have a basic website up and running, we have tested it in our browser, and we have committed any changes to our local repository. It's definitely time to write our first article. Step 2: The First Article Looking through the Pelican Documentation, I found another good section on Writing Content . From here, I learned that certain Markdown processors, such as the Python implementation, support metadata on files. As a base example, they specify this as a sample post: --- Title : My super title Date : 2010 - 12 - 03 10 : 20 Modified : 2010 - 12 - 05 19 : 30 Category : Python Tags : pelican , publishing Slug : my - super - post Authors : Alexis Metaireau , Conan Doyle Summary : Short version for index and feeds --- This is the content of my super blog post . Reading further, a lot of the metadata fields have defaults, but the Title metadata is required for the page to be picked up and processed by Pelican. That got some wheels turning in my head, but I put it aside for a later article. First I wanted to have a solid way of writing the articles before adding to that. Taking the markdown content from above, I created a file in the website/content directory called test.md and placed the content in there and saved it. Following the same pattern I used for the base content, I ran pelican-build.bat and then pelican-server.bat to view the content, providing the following: Granted, its a stock article, but I now had a way to publish my articles! Along the way, I noticed a couple of things I noticed needing improvement. Step 3a: A More Streamlined Build/Preview Workflow In the previous section, the workflow I used took 4 actions to go from something I wrote to something I could see and validate: save the file, run pelican-build.bat , run pelican-server.bat , and refresh the web page. Wondering if there was a more efficient way to do that, I checked back on the Makefile that I used as the inspiration for the two scripts, and noticed a devserver entry. As the devserver entry only differed from the server entry by the introduction of the -r flag, I quickly created a pelican-devserver.bat with that simple change, and executed it. Within seconds, I got the feedback: CRITICAL: TypeError: can't pickle generator objects Weird error, but understandable. In Python, the default serialization of objects to be passed outside of executables is referred to as pickling . If Pelican was already hosting the server and wanted to spawn a process to rebuild the website, it would make sense that it pass the current configuration to that new process. Doing a bit more searching, I came across this issue logged against the Pelican project in GitHub. There are a fair number of entries for this issue, offering various pieces of advice. The low down for me is that due to the way pickling works, I cannot use it on my Windows machine with that given program. Reading further in the comments for the issue, a number of people did mention that the -r flag works fine when applied to the command I am using for building the pages, just not for building the pages from the server. Trying it out, I renamed pelican-devserver.bat to pelican-autobuild.bat , and switched the file contents to: pelican -r --output website\\output --settings website\\pelicanconf.py website\\content Based on what I read, this should have my desired effect, and a quick modification of the test.md file confirmed it. When I saved the file, the generator detected it and rebuilt the contents. However, thinking through things a bit more, I wondered if the Windows start command would help build a better solution. When I write a Windows batch file and I need to execute another batch file, I use the call primitive. This allows me to run the new batch file within the current batch file's shell. When that new batch file completes, I can check the return code from that script, and determine what action, if any, to do next. By structuring batch files and scripts this way, I find that I assemble things together more quickly, keeping the batch files and scripts more readable and more maintainable. The start primitive is almost identical to the call primitive, with one small exception: the new program is started in a newly created shell spawned from the current shell. As the current shell and the new shell are separate processes, by default, both shells operate independently of each other, unless the /wait flag is specified. While this is not ideal for composing batch files, this behavior seemed to fit what I was trying to achieve. Creating a new version of the pelican-devserver.bat file, I placed within it: @echo off start pelican-autobuild.bat start pelican-server.bat And executed the batch file. Two full size shell windows popped up, each one running one of the scripts, with the original shell window left alone. A new modification to the test.md file resulted in the rebuild happening in the window running pelican-autobuild.bat . A quick refresh of the browser page, and the newly changed article appeared in the browser. I was mostly where I needed the script to be. Further tweaking the contents of pelican-devserver.bat to: @echo off start \"Builder-Pelican\" /min pelican-autobuild.bat start \"Server-Pelican\" /min pelican-server.bat got me to where I wanted to be. To keep things clean, I wanted to specify a title for each shell window and I wanted to start them minimized. Thus, by default both windows are out of the way, but if I need them, I can find them quickly. Side Note: If you are using this as a guide, note that this only seems to be an issue on Windows machines. In quick tests I did using an Ubuntu image running under Docker, I did not see this issue, and the served pages updated properly. Step 3b: A More Representative Test Article The first thing I noticed about my test article is that it was short. Really short. When I posted it, it was hard to notice where the test article was being summarized, and where it was just the article. I needed to come up with a test article that would be a better representation of an actual article. For that, I dug back into my testing experience. When coming up with test information for test projects, I have used the site Lorem Ipsum and sites like it for years. The site goes into more details, but by using this text it allows someone viewing the text to focus on what is around the text instead of the text itself. This is also better than using random words, as someone viewing that random text may try and find patterns in the paragraphs, once again distracting from what is around the text. After a couple of quick clicks on the site, and it generated 5 decent sized paragraphs of random text that started with: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc eget velit porta, efficitur justo at, sagittis nulla. Donec neque arcu, condimentum sed massa a, elementum rhoncus justo. ... I didn't include all 5 paragraphs here, as it was randomly generated. If you are using this as a guide, you can easily generate your own test paragraphs and insert them into your test message. However, when the page was regenerated, this is what it looked like: Also, to give a bit of contrast, I created a copy of the page, called it test-2.md , changed the title a bit, and removed all except the first two paragraphs. My reasoning behind this was to make sure I had a sample that was long and a sample that was short. Step 3c: Commit The Changes As always, before moving on, I needed to make sure I committed the changes. This one was easy, simply performing the git add action on the files test.md , pelican-autobuild.bat , and pelican-devserver.bat , followed by a git commit . What Was Accomplished At the beginning of this article, I only had the foundation for a basic website. The first thing I did was to make sure I had a good workflow for generating the website and viewing it in a browser as a website. After that, I added a sample article, and also improved the original build/preview workflow. Finally, I created more realistic data for the test article, so I could see what a normal article would look like. As for the goal of creating a test article that I could use to start fine-tuning the site, I believe I am in a good place. So next, fine-tuning and selecting a theme! What's Next? Next, I need to clean up a couple of small things before selecting a theme for the website. Getting a solid choice for a theme is the last major task I have to complete before publishing the website itself. Completing this last major task will be covered in the next post Getting Ready For Publishing - Themes and Minutiae .","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/09/01/static-websites-posting-my-first-article/","loc":"https://jackdewinter.github.io/2019/09/01/static-websites-posting-my-first-article/"},{"title":"Static Websites: Setting Up the Pelican Static Site Generator","text":"This is the second article in a series about setting up my own website using a Static Site Generator. For other articles in the series, click on the title of the article under the heading \"Static Site Generators\" on the right side of the webpage. Introduction In the previous article, I decided that the Pelican Static Site Generator (SSG) was the right choice for me. This article goes through the steps that I took in setting up Pelican on my system, providing a play-by-play for anyone thinking of doing the same thing. Feel free to use this as a guide for setting up your own website to experiment with. If you are not using this as a guide, I hope it provides you with any details that you require for regarding Pelican and it's setup. From my experience, it is more important for me to make a small step forward and lay a good foundation for what comes next than it is to take leaps and bounds and miss things. Hence, I feel that focusing on the setup of Pelican is a good and properly scoped step. Why Another Pelican How-To-Guide? In looking around, there was a real mish-mash of articles out there: Making a Static Blog with Pelican Using pelican to generate static sites on windows Creating a Blog on GitHub.io with Python Creating your blog with Pelican How to Create Your First Static Site with Pelican and Jinja2 The first thing that was obvious to me was that all of the Pelican posts I found were written in 2017 or earlier. This means that these articles refer to versions of Pelican before the current 4.0.1 release that I am running, so they are either out of date or inaccurate. The second thing that was obvious was there were very few posts written about using Pelican on a Windows machine. According to the site NetMarketShare.com , Windows machines account for over 87% of the desktop machines surveyed. While it is true (from experience) that developers prefer Macs over Windows, projects like WSL are starting to chip away at those reasons. And it still remains that for a non-developer, Windows is by far the most common machine type. As it is my primary environment, I want to make sure it is represented. Component Versions If you are using this as a guide, you may try versions of any listed component other than those specified, but note that Your Mileage May Vary . This article was written from my detailed notes of how I set up my website, using the component versions listed. As such, if you experience any issues, I would fall back to those versions as an established baseline. If you would like to try other versions of components, I strongly encourage you to go to the baseline established in this article and commit it to your local repository (see Step 5: Committing the Changes below). Once you have that point set and committed, you can then try and use other versions of the components, having a known state to return to. Regardless, keep detailed notes about what you try, and if you find yourself in a bad state, fall back to one of your previous known states and try again. Operating System Paths From long experience as a developer, there is virtually no sematic difference between pathing that is meaningful. Windows uses a format of C:\\this\\is\\my\\directory and Linux systems use /my-mount/this/is/my/directory . I personally work on both types of systems equally and do not have any issues switching back and forth between them. One reason that I enjoy using Python over PowerShell, Bat/Cmd, and *Sh shells, is that I can properly obfuscate any such issues in my code. Python scripts can easily be written that are platform agnostic, eliminating duplicated scripts to handle platform issues. Add in to that additional support from editors such as VSCode and PyCharm, and it becomes a powerful scripting language with some real muscle behind it. While I realize others may feel differently, I expect the reader to be able to perform the same translation task while reading, with practice if required. Step 1: Install Pre-requisites There is one pre-requisite for Pelican itself, and that is having Python 3.7 installed with a current version of Pip. The information in this post was generated with Python 3.7.3 and Pip 19.1.1 . As I have a great habit of fat fingering commands, I prefer to keep most of my files in a version control system, specifically Git . While I use Source Tree as a graphical interface to Git, I realize most people use the command line. Therefore, for the purpose of any examples, I will use the Git command line interface, assuming that anyone reading this who uses a Git GUI can find the appropriate command in their GUI if needed. The commands I will be using in this article are as follows: git init - Create an empty git repository. git status - Working tree status of the current repository. git add - Add a new file to the working tree. git commit - Commit any changes to the working tree to the repository. If you are new to Git or need a refresher, please go to the links about and do some searching online for examples. These are some of the base concepts of Git, and should be understood before proceeding. With respect to how to install Python and Git/SourceTree, there is plenty of information on how to install those programs for each platform. Please google Python install , Git install , and SourceTree install for the best installation instructions for a given platform. For the Windows system I am using, I simply downloaded the installations from the websites linked to in the previous 2 paragraphs. After installing Python on my system, the installation of required packages was very simple. At the command prompt, I entered the following line: pip install pip==19.1.1 virtualenv==16.6.0 The two packages installed were the Pip Package Manager itself and the virtualenv package. The first installed package, pip, makes sure that Python's own package manager is at the specified version. While pip does not often change meaningfully and I am not planning on using any new features, it usually pays to keep things current. The second one is a virtual environment system for Python. Virtualenv is a Python tool that allows you to isolate a given Python project. While it is not portable between different systems, it does provide for a manner in which to isolate different versions of Python and different versions of Python packages from each other. Using virtualenv will allow me to install a specific versions of Python and each package with no fear of any global changes affecting this project. To verify that I have the correct version of pip and virtualenv installed, I executed each tool with the –version parameter, expecting to see output similar to the following: XXX> pip --version pip 19.1.1 from XXX\\lib\\site-packages\\pip (python 3.7) XXX> virtualenv --version 16.6.0 After these pre-requisites were installed, I was ready to create a directory to host the content for the website. Step 2: Create a Project Directory For The Site Before generating content, I needed to create a place to install Pelican that was isolated and self-contained. As mentioned in the previous section, that is exactly the use case that the Virtualenv program was created for. That is the first of the two tools that I needed to set up for the new directory. The other tool is version control, to ensure I can replicate and version the website. For this purpose, I use Git. The first thing this accomplishes is to ensure that if the computer hosting the website content gets destroyed, I still have the latest information about the website. The other thing that this accomplishes is to ensure that if I make a change (or two, or three) to the website that I don't like, I can always return back to previous versions of any of the documents. That out of the way, I selected a directory as a location of the website. In my case, I keep all of my local Git repositories in a directory c:\\enlistments , so I created the directory I wanted to keep the website in was c:\\enlistments\\blog-content . The location is totally arbitrary, so for the sake of clarity, if I refer to this directory indirectly, I will use the term base project directory . If I refer to this directory directly in a script, I will use the pattern %%%MY_DIRECTORY%%%. To create the base project directory, I executed the following commands in order: mkdir %%%MY_DIRECTORY%%% cd %%%MY_DIRECTORY%%% git init virtualenv virtualenv virtualenv\\scripts\\activate.bat In order of execution, the commands first create a directory and then changed the current directory to that directory. Once the base project directory was created, git init was used to create an empty Git repository with nothing in it, ready for the project to populate. Next, virtualenv virtualenv was used to create a virtual environment for the website, housing that environment in the virtualenv directory of the project. Finally, the activate script of virtualenv was executed to enable the virtual environment. The script activate.bat on my Windows platform ( activate.sh on Linux platforms) performs two simple tasks: change the shell's path to use the virtual environment AND change the path to make sure that change is evident. To be sure, I checked the PATH environment variable to make sure it starts with the Python path of the project's virtual environment and that the prompt started with (virtualenv) . Note that while I used git init to create a local repository, I was still getting started with the project. As such, I didn't need to worry about ensuring that the local repository is reflected in a remote repository. At that point, the purpose of having the local repository was to ensure that I could see what changed and revert back to previous versions if needed. If you are using this as a guide, please note that from this point forward, any commands that I entered were entered in the virtual environment shell. If for some reason you close your shell and need to restore the shell to where you were, you will need to open a new shell and submit the following commands: cd %%%MY_DIRECTORY%%% virtualenv\\scripts\\activate.bat I had a good directory ready to go, but I had one small issue to fix. When I submitted the git status -s command, I encounter the output: ?? virtualenv/ As mentioned above, the virtual environment is specific to a given operating system and version of Python. Because of this, committing the virtualenv directory to Git didn't make sense, as it contains system specific information. Luckily, this issue was easily addressed by creating a file in the base directory called .gitignore with the contents: virtualenv/ The format of .gitignore is pretty simple to understand. In my case, I only wanted to ignore the virtualenv directory off the base project directory, so I just needed to add that directory to the .gitignore file. Submitting the git status -s command again, I then saw the output of: ?? .gitignore This showed me that Git is ignoring the entire virtualenv directory, instead showing the .gitignore file that I just created. Since I have only done limited setup in the base project directory, having only the .gitignore file showing up as untracked is what I expected. To be safe, I used the git add and git commit commands to save these changes as follows: git add .gitignore git commit -m \"initial commit\" The directory was ready, time to focus on Pelican. Step 3: Install Pelican Pelican itself is installed as a package using Python's Pip program. Based on information from Pelican's Installing page , both the markdown and typogrify package are useful, so I installed them as well. The markdown package allows for content authoring with Pelican using using Markdown. Using a simple text file, special annotations can be placed in the text that alter how it will look when rendered with a Markdown processor. The full range of Markdown annotations and their effects are shown here . As this is one of the requirements I established in the previous page , this support was critical. The Typogrify package \"cleans\" up the text to make it look more professional. It accomplishes this by wrapping certain blocks of text in HTML span tags to allow for CSS styling. In addition, it replaces certain sequences of characters with other sequences that add polish to the finished document. While not required to get the site up and running, I figured it would be of use later. To install these three packages, I submitted the command: pip install pelican==4.0.1 markdown==3.1.1 typogrify==2.0.7 This resulted in the installation of any dependent packages, is essence, going from the stock packages (displayed using pip list ) of: Package Version ---------- ------- pip 19.1.1 setuptools 41.0.1 wheel 0.33.4 to: Package Version --------------- ------- blinker 1.4 docutils 0.14 feedgenerator 1.9 Jinja2 2.10.1 Markdown 3.1.1 MarkupSafe 1.1.1 pelican 4.0.1 pip 19.1.1 Pygments 2.4.2 python-dateutil 2.8.0 pytz 2019.1 setuptools 41.0.1 six 1.12.0 smartypants 2.0.1 typogrify 2.0.7 Unidecode 1.0.23 wheel 0.33.4 To make sure that I would be able to keep these libraries at their current versions in the future, I needed to take a snapshot and save it with the repository. Thankfully, this is a scenario that the Pip contributors though of. On the command line, I typed in: pip freeze > requirements.txt This command produces a terse list of each package and it's version, which I redirected into the file requirements.txt . The benefit to doing this is that, at any time, I can execute the following command to restore the packages and versions: pip install -r requirements.txt Step 4: Create a Generic Web Site Now that Pelican is installed and ready to go, I needed to enact the templating system of Pelican to form the basis of my website. The authors of Pelican have kept this simple. All I needed to do is run the command: pelican-quickstart During the installation, I was asked a number of questions: > Where do you want to create your new website? [.] website > What will be the title of this website? Jack's Web Site > Who will be the author of this website? Jack De Winter > What will be the default language of this website? [English] > Do you want to specify a URL prefix? e.g., https://example.com (Y/n) n > Do you want to enable article pagination? (Y/n) > How many articles per page do you want? [10] > What is your time zone? [Europe/Paris] America/Los Angeles > Do you want to generate a tasks.py/Makefile to automate generation and publishing? (Y/n) n Done. Your new project is available at %%%MY_DIRECTORY%%%\\website For a decent number of the questions, the defaults were sufficient. The questions that I answered specifically were: website directory keep the website isolated in a subdirectory, for future use title something simple for now author my name here url prefix No, we can change this later time zone I didn't know this, so I entered in something silly, and it gave me an URL to a web page where I looked it up generate no, I will provide simple scripts for that When this was completed, my base directory had a new directory website/ which contained 2 files and 2 directories. The first file pelicanconf.py had the settings that I entered using pelican-quickstart . The second file publishconf.py has any remaining settings that Pelican will use when publishing. If things change with the website, I just need to change the settings in these files, and the next time I publish, they will be in effect. The 2 directories are the key here. The content directory was created to contain any files that are the source parts of the website. During the publishing action (covered in a subsequent post), Pelican will translate that content into output and place it in the other directory that was created, output directory. Step 5: Committing the Changes At this point, it was useful to use git add and git commit to commit what I did to the local Git repository, as there was useful progress. Entering the git status -s command, it reported that the only meaningful changes were that the website directory and the requirements.txt file were added. As both of these objects are considered source and configuration, I added them as follows: git add requirements.txt git add website git commit -m \"Base website\" If there was something that was added that was not part of source, configuration, or documentation for the website, I would have edited the .gitignore file to include a pattern that cleanly removed those changes from Git's preview. When this comes up in future articles in this series, I will point out what I added and why. What Was Accomplished Having decided that SSGs were the correct paradigm for my website, and Pelican the correct SSG for me to use, it was time to set it up. I documented how I installed various components, as well as how I set up the base project directory for the website itself. Finally, I created a default website as a solid foundation for my purposes, and made sure that I committed the base project directory to Git for version control. In my professional career, most of the time it is advantageous to build a foundation for your application, committing it to version control often. Having provided that foundation in this article, I can now proceed with the actual building of the website. What's Next? Next, I will start with publishing a simple file to the website and make sure I check it online for any errors. By providing realistic samples and getting experience with the publishing workflows, I will get invaluable information on using Pelican. This task will be covered in the next post Posting My First Article .","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/08/25/static-websites-setting-up-the-pelican-static-site-generator/","loc":"https://jackdewinter.github.io/2019/08/25/static-websites-setting-up-the-pelican-static-site-generator/"},{"title":"Embracing Something Hard","text":"If you look at my LinkedIn profile , you'll see that I have been around for a while. I was a Software Developer for many years beginning in 1991 at a small 2 person company in Waterloo, until a stint in Denmark gave me a bit of a wakeup call. When I came back to the United States in January 2011, I changed from a Software Developer to a Software Developer in Test. 1 While I started my career \"making things that work\", after 20 years I was more focused on \"making things work better.\" That career change was a wonderful and happy change for myself that I have never looked back. To be specific, I am not a tester. I do know a lot about testing, but that is not my primary focus. My focus is looking at interconnected systems and figuring out how to improve them. It is not about breaking things, like a lot of people assume, but making the team around me better. It is about standing up and mentioning that having 5 developers struggle through the same setup process for 4 days each is just plain inefficient. It is about standing up and mentioning that having 5 developers struggle through the same setup process for 4 days each is just plain inefficient. Solution: Add a responsibility for documenting the setup process for the first person, with each subsequent person responsible for the process of updating the document to current standards. Benefits: The team owns the process and its updating, with a clear path of responsibility if it doesn't work the next time it is used. It is about looking holistically at a group of systems and, while developers enjoy being creative, focusing that creativity on the parts of the systems that will benefit them and the team the most. Solution: Use templates and common libraries where possible to reduce the amount of \"creativity\" needed for anything remotely common to as close to zero as possible. Benefits: The team spends time on stuff that needs to be solved and not stuff that has already been solved. It is about asking each member of the team what they expect of documentation from the projects that their projects rely on. Solution: Setup a set of \"how-to\" guides that documents common practices for the documentation of projects for the team, hopefully with a \"dummy\" project that gives a live interpretation of those guides. Benefits: The team writes a set of documentation for their systems that looks like they were written by the same team, instead of a mish-mash of people thrown together. My job is as much about asking the simple but tough questions, as it is about having an idea on how to respond to both the questions and answers that follow those tough questions. The actual job is almost always about automation or process, it almost always involves changing the way people look at things, and unfortunately it almost always includes having someone's ego bruised along the way. Partially due to me having Autism, I can see certain patterns very easily, almost as if I was reading a book. Changing the way developers look at things almost always brings around the bruised egos. A lot of developers associate code they write with themselves. As such, saying that the code or process can be improved becomes confused with saying that the developers themselves can be improved. And yes, when that happens, it is often the people asking the questions and making suggestions on how to make things better that take the brunt of the anger that results. I still remember my daughter asking me one time why I liked being a software developer in test, as I am often frustrated with people over a perceived lack of momentum. Thinking about it quickly, the immediate answer was easy: I am good at a decent portion of it. If you are in a box and looking around you, all you see is the box. I am able to elevate my perspective to a higher level and not only see the one box, but the boxes around it. I can see how they are stacked, and if they are looking like they will tip over. That came second nature to me. But it wasn't the complete answer. Even as I responded with that answer to my daughter, there was something missing in that answer, and it bothered me when I thought about that conversation over the next couple of years. It was years later during one of those teaching moments we as parents have with our children that it occurred to me. I was reminding one of my children that we have a saying in our family: being knocked flat on your ass is not the last step, it's just the step before we pick ourselves up, brush ourselves off, and try again. Yeah, having issues and making mistakes sucks, but they helped make us who we are, and it's how we stand up again that defines us. It was then that I realized: I became a software developer in test because it was hard. I wanted the challenge to make things better, and to help others get better at what they were doing. I knew I was going to encounter stubborn people along the way, but I was determined that I would try and figure out a way to get through to them. Sure, I get knocked flat on my rear end a fair number of times, but I always get back up and try again. And it wasn't just the other people, it was myself. I had to learn when to strive for perfection and when to strive for \"just good enough\". I had to learn to find the balance between what I felt was the right decision and what the right decision was for the business right now. I had to learn that while my own passion and vision were wonderful, unless I was able to share those things in a way that others were receptive to, they meant nothing. I had to learn to get in a state of continuous learning. After all that time, I finally had my answer: I liked being a Software Developer in Test because I was good at it and because it was a hard challenge that forced me to learn and grow. That takes me to the last couple of months. For a long time I have wanted to start my own blog and help out an open source project. I was under no illusion that either objective was going to be easy, I just didn't have a clue about how different it would evolve into from what I thought it originally was. As I was going through and picking out a platform for my blog, I kept notes and started to turn them into articles. That was relatively easy. Or at least the first pass was. I found out that when it comes to articles, I want to make sure I am saying the right thing in the right way, and can literally spend 45 minutes working on one sentence. Shortly after that, I also learned that I can spend 5 minutes getting said sentence in a way that makes sense, add text a marker like **TBD** before it, and then come back to it at the end of the article. And yes, following that, I realized that about half the time, going downstairs and doing something totally unrelated caused me to think of THE EXACT phrase that I needed within seconds of coming back after the break. Yup, learning is fun, and hindsight is perfect! This blog isn't hard in terms of writing for me, but the production of it sometimes gets me. If you want to stay on track, you have to give yourself some kind of schedule. For me it was to publish once a week on something technical. It is a challenge to always make sure you have a couple of articles on the go at any time, and that you can polish one up and publish it on a weekly basis. I also have to balance the writing with exploring stuff so that I can write about it in my blog. And I realized I have to extend that out 4-6 weeks to give me time to go through a good ideation process. In picking a theme for my website, my attention was drawn to the Elegant theme for its simplicity and crispness. Looking into the documentation a bit, I noticed that some things were close, but not spot on. I wanted to get one of those features working for my website, so asking for some help getting it working, I changed the document to better document what needed to be done. The change was welcomed, and I volunteered to help out with any other articles. That is how I started contributing to the Elegant theme. What does it entail? Take the work I am doing for my blog articles, subtract the subject matter research, in certain cases add some localized research, and supplement that with making sure I write the articles in a more professional and international. 2 On top of that, apply a bit of my developer in test training and try and make sure I have a solid theme, and that I am making the process easier for me and other users of Elegant in the process. For sure, doing these things at the same time can be nuts, but I am thoroughly enjoying the challenge. I am growing both personally and professionally as I work though things on each project, some inside of my expertise and some outside of it. Sometimes I wish there were more hours in the day, but I wouldn't trade the learning I am doing for the world. Yeah, it's quite often hard, but it wouldn't be worth it if it wasn't hard, would it? In the United States, where I currently live, I am a Software Development Engineer in Test or SDET. I do not have an engineering degree. In any other country, including my native Canada, I am a Software Developer in Test or SDT. ↩ I will probably write an upcoming article about this to explain it fully. In essence, if you are writing in English for an international audience, you have to remember that a fair percentage of your readers are not native English speakers. Until they reach a point in their English proficiency, they will typically think in their native language and translate what they are reading from English to that native language. As such, you want to keep your language as free from idioms and imprecision as possible. ↩","tags":"Personal","url":"https://jackdewinter.github.io/2019/08/18/embracing-something-hard/","loc":"https://jackdewinter.github.io/2019/08/18/embracing-something-hard/"},{"title":"Static Websites: Choosing a Static (Web) Site Generator","text":"This is the first article in a series about setting up my own website using a Static Site Generator. For other articles in the series, click on the title of the article under the heading \"Static Site Generators\" on the right side of the webpage. Introduction Why do I want a static website? It has been on my mind for a couple of years that I would like to talk about some of the things that I find cool and other things that I feel strongly about. As a big proponent of servant leadership , I want to help and inspire people to do more, be more, and help more. Basically, I want a low-friction, easy to use platform that allows me to communicate with and help others in my own way. I also want tooling and workflows to be efficient so that I can spend more time communicating and less time fidgeting on the website. To this end, I want to focus on something that is simple, but extensible. I want to be able to have a good mix of my own pages and a blog, allowing for flexibility on what I am doing. I want to be able to focus on the message and my voice, rather than the medium. From my research, Static Site Generators fulfils those requirements in spades. But as with a number of things I do, I want to take the time to determine if they are the right choice for my website, and if so, select the right one for me. I would rather take a small amount of time now to ensure it is a good fit, than to take time reworking things because it wasn't. What are Static Site Generators? Static Site Generators (SSGs) are programs that take a certain type of website and shift the heavy loading of the website from the content-request point to the content-update point. Put another way, the decision of what content to show the viewer of the web page is changed from when the web page is requested by the browser to when the web page content is updated by the author. Therefore, to be eligible to use a SSG for a given website, that website must have all its content available when the content is updated, with no real time content present. There are small ways to get around that restriction, but those present their own challenges. As such, I am going to assume that 95% of the website is going to be static, and that all the important information is within that 95%. SSGs themselves usually combine different technologies to achieve their results: writing, templating/styling, metadata, and publishing. The pages for the site need to be written in either a supplied WYSIWYG editor or in some format like Markdown. Once the pages are written, generic styling and templating are applied to the collection of pages to make sure that the branding on the pages is consistent. To help organize the pages, most SSGs include some form of metadata that is attached to the pages, used by the SSG to group pages and provide other guidance to the SSG itself. Finally, when the pages are checked over, there needs to be a simple way to publish them to the intended website. Reducing the barrier to entry and keeping it low is essential to a SSGs success. Therefore providing multiple technology choices for parts of the SSG is common. All the SSGs that I looked at had a variety of options for most of these technologies, with templating and styling being the exception. In the case of templating, most of the SSGs support a single default templating engine that is used on all the pages. With respect to styling, common HTML technologies such as Cascading Style Sheets (CSSs) are most commonly used. Besides keeping the cost of the templating and styling low, using a common technology allows for lots of examples to be provided by the Internet at a low cost. Is a Static Site Generator The Right Choice For Me? In determining whether SSGs are the right technology for my site, I started making a list of the things I was primarily worried about with a website: Security As I deal with security concerns at work, this one was top of mind. How often have I heard about various platforms and websites being hacked in the last month? To avoid this, I want to keep my website as simple as possible. Static pages created by a SSG leave a single attack vector: the repository where my website's HTML pages are being stored. Mitigating attacks by only having a single attack vector is very attractive solution for me. Ease Of Use I don't want to be messing with the website more than I have to. I either want to be updating something on the site in response to something I am working on, or working on that thing. I am already used to writing documentation at work in Markdown, so writing web pages in Markdown is already very efficient for me. In a similar manner, at work I keep documentation in a version control system, so keeping the pages in their various stages of completion in a readable form in version control is also efficient. Using my already existing workflows, with minor modifications, is an acceptable solution that keeps ease of use high. Real-Time Contents and User Input A lot of my focus at work is making sure that we can anticipate and plan for events that happen to change our system's state. If a user or another system sends a request to our system, can we validate the request, verify the system is in a good state for the request, and enforce the correct behavior for the response? Any good SSG takes care of this issue by eliminating any change of state. Resolving the concerns of state management by removing all state information seems like an ideal solution for the limited scope of my personal website. Once those were out of the way, only the big question remained: Do I have any plans for a web site that would require it to support dynamic content? My primary purpose is to allow me to communicate to interested readers. As such, the sites's content is largely going to change only when I write something new and publish it. Based on the 95% barrier I set for myself above, such content appears to be will within that barrier. To handle the remaining 5%, I am fine with any dynamic content for my site being generated using JavaScript. A good example of that is using something like Disqus for dynamic comments. By handling such things with a Javascript approach, I can keep the simple things simple, only making things more complex when they need to be. To me, that seems to be a solid way to handle the few exceptions that may arise. For those reasons, I believe an SSG is an ideal choice for a personal website with a blog. Which One to Choose? In case I need or want to do any extension work, I want the SSG to be in a language I am comfortable with, maintaining the ease of use concern from the previous section. At the moment, that limits the SSG language to C#, Java, and Python. While I can work effectively in other languages, the comfort and enjoyment levels are not there. As I am doing this for myself, I want the writing of any extensions to be easy and fun to increase the chances that I will stick with the writing and the SSG choice. Looking at a number of sites, these are the SSGs that appear the most. Generator Language Website Last Updated Hugo Go https://gohugo.io/ May 30, 2019 Pelican Python http://blog.getpelican.com/ May 13, 2019 Hexo Node.js https://hexo.io/ Jun 6, 2019 Jekyll Ruby http://jekyllrb.com Jun 9, 2019 Gatsby NodeJs https://github.com/gatsbyjs Jun 9, 2019 (Note that the last updated column is current as of 2019-Jun-09.) Given these goals in mind, I looked through the choices for static site generators and decided to go with Pelican. It's written in Python, has been maintained recently, any extensions are written in Python, and seems to be easy to use. This choice supports my desire to write pages and articles in Markdown, and any needed extensions can be tweaked if needed. What Was Accomplished At the beginning of this article, I was intrigued by Static Site Generators and whether or not SSGs would be an effective tool for me to communicate through. During the article, I not only determined that it was the right fit, but selected the Pelican as the SSG to use, based on its Python coding and recent updating. I feel that this determination to use SSGs, and Pelican specifically, puts me in a good position to start building my website with confidence. If You Are Trying To Decide… If you are trying to decide if SSGs, or a specific SSG, will work for you and your site, I would encourage you to walk through a similar process to what I did. Figure out your own concerns for your own website, and determine whether or not an SSG will address those concerns. If you think you may write extension for it, take a look at the language of the SSG and make sure you are familiar with the extension language. Most importantly, figure out whether your choice of SSGs in general or a specific SSG will serve as a tool for you to publish your information, or whether it will be an impediment to you publishing. From the point where I was at when I made the decision, Pelican appeared to be a good choice for me. I did some research to make sure this was the right choice for me. Make sure you ask yourself those questions, get those answers through research, and make sure your choice is going to work for you! What's Next? Next, I need to start exploring Pelican and how to set it up with default content. This will be covered in the article Setting Up the Pelican Static Site Generator .","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/08/18/static-websites-choosing-a-static-web-site-generator/","loc":"https://jackdewinter.github.io/2019/08/18/static-websites-choosing-a-static-web-site-generator/"},{"title":"Glanceable Displays: What Do I Want To Display?","text":"Preface This is the final article in a series about Glanceable Displays and my efforts to create one for my household. For other articles in the series, click on the title of the article under the heading \"Glanceable Displays\" on the right side of the webpage. Introduction In the previous article Glanceable Displays: Setting Up Our Display , I added the finishing touches to allow a Raspberry Pi to display a single web page on my Glanceable Display. It took a bit of time to get there, but the work that I put in over the previous articles made sure that the foundation for the Glanceable Display is solid, a necessity for a device that I want to be as \"hands-off\" as it can be. This article takes a look at the options and work I did to put a series of web pages on that display. I have tried to document the various processes that I went through with my family's display to help any readers come up with a good way to help them come up with their own requirements for their Glanceable Display. What Are My Options? As of the writing of this article, there are 3 major options available: roll your own use Dakboard use Magic Mirror To walk through each of these issues, I have shared my \"Pros and Cons\" lists, as well as some general discussion on those lists. Option 1: Roll Your Own Pros Cons write it yourself write it yourself Writing down a list of pros and cons for this option, I came up with the same thing on both sides: write it yourself. On the pros side, you have the freedom to do what you want, however you want to do it. On the cons side, you have the responsibility to learn and create what you want, as well as maintaining it, debugging it, and hosting it. While this is the option with the greatest ability to be creative, it is also the option with the greatest costs. Taking a hard look at this option, while I am capable of doing that work, it would be slightly outside of my normal comfort zone. Any changes would likely be code changes, and not configuration changes. The web pages would have to be hosted somewhere, either on a local machine or in the cloud, and the monetary and time costs associated with that. Taking all of this into account, the maintenance cost seemed to be the primary cost, and it just seemed to continue to get higher with this option. For the most part, this was immediately discarded by me, and that decision was backed up by my family. It seems that I get grumpy if I have to do too much maintenance on our systems at home, and they would prefer to avoid that, is possible. Go figure! Option 2: Use Dakboard Pros Cons professional solution locked in to their pricing scheme they do the dev and testing locked in to their feature set multiple screens available not in control of new features loops of screens available simple WYSIWYG editors This option uses the frequently sited DAKboard site to host and maintain the webpages to display. As their pricing indicates, if you want to do something custom, you are at least looking at their Essential package. However, for that monthly fee, you do get a number of neat features that just work . The first feature is the WYSIWYG editor, with which you create custom screens. You get seamless integration with different types of calendars and both provided and custom data sources. Comparing this option to the previous option was like comparing night to dark. Yes, there is a monetary cost to consider. But for that cost, any screens I created would be hosted on their server. The maintenance cost for me is the configuration of the screens, with no development or debugging cost. And that maintenance cost would be limited to look-and-feel issues with the displays, not debugging why something is not working. Taking an honest look at the cost-to-benefit ratio for this option, the primary cost was the monthly fee and there were multiple features that just worked on the benefit side, making this a promising option. Option 3: Use Magic Mirror Pros Cons lots of third party plugins oriented to sharing a single screen can customized output you handle the installation of plugins you handle the configuration This option uses an open source solution that is often cited, Magic Mirror . As this solution is open source, you will be putting your own time into this project to download and configure things. That configuration is mostly done with a file editor and the testing of changes, re-iterating with reboots of the system. If there is something that you need this to do, if worse comes to worse, you can write the plugin and get it into Magic Mirror's ecosystem. From my viewpoint, this option landed somewhere between the Dakboard option and the Roll Your Own option. While there is a lot of existing choices and a framework to host it, there is still a bunch of experimentation and maintenance that is needed to get this working. The big issue here is that MagicMirror is intended to be a single screen display that shows a collection of information, not a series of screens with different information. What Did I Decide? To satisfy my family's requirements, Dakboard's solution was the best fit for what I wanted to do. As this is a family project, an implicit requirement is that if we decided that we want to change something, I need to be able to affect that change pretty quickly. Between Dakboard's features and their custom screen editor, I had confidence that I would be able to make these changes quickly. The decision came down to an easy calculation of cost versus benefit. For Dakboard's monthly pricing, the ease of which our glanceable display would display information and require almost no maintenance is still worth it. While spending time on side projects is fun for me, this project's main goal is to use a glanceable display to make an appliance that we all use. In my mind, spending hours laboring over the maintenance of the appliance defeats the purpose of calling it an appliance. Starting to Use Dakboard Using my family's requirements as a guideline, starting to use Dakboard was a pretty easy journey. As I knew we needed around 2-3 pages in rotation, the default templates were not satisfactory and I registered for a premium account. At the time of this article, that premium account was 5.95 USD per month and included 3 screens that can be added to a single loop. Depending on what my family's future plans are, we might revisit the pricing, but for now that works for us. Logging in to my Dakboard account, I had a screen called My Predefined Screen ready to go in my account. Taking a quick look at it, from my point of view, it was too cluttered and busy for our family. Based on prior discussions, we distinctly wanted 2 screens: one for a calendar and one for the weather. Everything after that was extra. Selecting the Add a Custom Screen or Template button under Custom Screens , I was left at a screen asking me to put things together for the screen. Where to start? Thinking through a theme Before I came back to write about this point in the article, I tried a number of times to create screens that took care of our requirements for calendars and weather… but they didn't seem right. It just didn't seem to me like the pages where cohesive and their purpose clear. It wasn't until I took a couple of days away from the project that I realized my problem: the pages didn't have a common theme. Now being aware that I needed to find a theme, I went to Dakboard's Templates display for inspiration. After a bit of thinking, I figured that the folks at Dakboard wanted to put their best foot forward, so they probably had someone with artistic experience help them with their templates. As such, using them as meta-templates or inspiration for our screens seemed like a straight forward thing to do. Similar to my approach for deciding on the software to use to generate the pages, I went through each of the template screens, writing down good and bad things about each one. Right off the mark, I was able to eliminate some screens as too busy for my taste, which helped out. In the end, the list of things I wanted for the display boiled down to the following items: a horizontal layout instead of a vertical layout the left quarter of each screen will be very simple text on background of a rotating set of images good examples of this are the Simple Agenda and Big Calendar templates the current date and time will appear at the top of that quarter only current weather warnings will appears the bottom of that quarter the remaining three quarters of each screen will have a singular focus the calendar screen will have a calendar that takes up the entire screen, due to it's importance to our family the Big Calendar screen is a good example, but must support a different color for each family member the weather screen will have specific items related to the weather, including a storm map if possible the Beach screen's left side is a good example the final screen will have other things we want to display and test out Keeping with my desire to keep my family on board with the project, I talked to them about this after dinner one night, and with the exception of some small squabbles (about which family member got represented with which color on the calendar) everything was good to proceed. Doing the work - The Left Quarter Panels To make the transitions between screens seamless, it was important to me to ensure that the size of the left image panel and the date and time panels we consistent through all three screens. While it might have appeared to be a very pedantic behavior on my part, I wanted to reduce any barriers to adoption by my family. From doing some reading on user interfaces for my job, choppy or inconsistent interfaces can often turn off users. Based on that experience, the extra time used to get it \"just right\" was a worthwhile investment for me. To carry the theming further, I wanted the images displayed in this left image panel to have a common theme. Looking through stock images from Unsplash , I went through and started downloading 15-20 images that were 634 pixels by 951 pixels, a common size available on the site. Looking at the download directory, I noticed that those pictures appeared to have 3 common groups: pictures of night skies, pictures of nature, and pictures of various paths. I then created three new folders to hold each group of pictures, moved the relevant pictures into their own directory, and pointed the image panels of each screen to a specific directory. Before moving on to the next part of the screens, I added these screens to a loop and looked over and over again at various parts of the display, making sure that the display was reflecting what I wanted the look and feel to be. After a number of nitpicky hours of fiddling with panel size and location, I brought my family in and took some creative criticism on the screens. I took their feedback and applied it to the screens, repeating the vetting process with myself and then the other members of my family until it was just right. Once I had everyone onboard, I moved to the content of the other screens. Doing the work - The Calendar Screen Dragging the Calendar panel to the screen was the easiest part of this screen. While the biggest decision was to decide whether or not to use the Monthly calendar type or the Big Monthly calendar type, it was the small details that really needed tweaking. The first part was to add each of our personal calendars to the panel. The people at Dakboard facilitated this by having a button called Show iCal help that displays easy to follow instructions. I had no problems adding each of our personal calendars to the screen, and assigning unique colors to each of those calendars. The fine tuning and nitpicking came over the next 2 weeks as me and my family took a look at the display and tried various options to get it dialed in to what we needed. The reason that this screen is so important to my family is that we all use our phones to record when we have things to do and where we need to be to do them. Having a separate whiteboard that we had to update with those schedules wasn't working for us, so having something that was able to use the same calendar source as our phones was pivotal to it's acceptance. Doing the work - The Weather Screen In approaching this screen's design, it was important for me to think of how my family intended to use this screen. In asking my family, there were three main use cases: What will the weather be like today? (for dressing) What will the weather be like in a few days? (for planning) Are there any weather alerts for the area? (for implementing plan-B to avoid the bad weather) For the first two, it has been hard to figure out which two weather sources are more accurate, Darksky Weather or OpenWeatherMap, so instead I just show both. In the future, I am going to have to change that, but for now my family is still deciding. To increase our information though, we have a precipitation radar map courtesy of The Weather Channel . To embed it in the panel, I added an image panel, went to the weather map I wanted to add, right clicked on the image and then selected Copy Image Address and placed that in the configuration for the image panel. It took a couple of tries to get it looking the way I wanted, but was pretty easy. Adding the weather alerts was even easier for me as Dakboard already had a separate panel for that. It was as simple as adding the panel, setting the location, and saving the panel. One note for all of these weather panels was that they worked better when I used the closest major city instead of the small town that I live in. Results were weird when I didn't do this, and if I figured it out correctly, were showing the weather for New York City! Doing the work - The Left-over Screen Yes, the left-overs. Where to put the fun stuff that really didn't fit in anywhere else? For fun, I used the News/Rss Panel to add feeds for Brainy Quotes and Word of the Day . The panels were easy to configure and get them to display in a way that was easy to read. My wife's family loves Christmas, so I added a X Days Until Christmas display using the Countdown panel, setting the date to December 25, 2019 and to show only the days. The really fun panel was to show the number of astronauts currently in space as counted by NASA. Searching online, I found NASA's open api which includes the number of astronauts in space complete with instructions on how to interpret the data . Adding it into a new Fetch panel, I set the panel to extract the number of people in space from the number field and use the user-astronaut icon from font-awesome to display beside the number. I am sure there will be more fun things to add, but confident that this was a good start, it was time to wrap it up. Wrapping The Screens in a Loop To facilitate the showing of multiple screens in succession, Dakboard provides a wrapper called a loop. A loop can change the current screen at a various intervals from every 15 seconds to every hour. To add a loop, I went to the main screen for my account, clicked on the loops button and followed the simple instructions provided on the popup that appeared. For my family, I chose a change frequency of 15 seconds and added all 3 of the screens I created into the loop. To view the loop, I clicked on the View Loop button that was visible once I clicked on the ellipses (…) at the bottom of the panel in the loops display. Once things looked right on my computer, it was time to add them to the glanceable display. When I pressed the View Loop button, the browser changed to an URL that begins with https://dakboard.com/loop/uuid/ . This URL is the actual URL you need to use to view the loop from any browser without having to go through the configuration screen. SSHing into our glanceable display, I went back to my notes in the article on Setting Up Our Display - Starting Chromium In Kiosk Mode and replaced the placeholder URL of https://bingwallpaper.anerg.com/ with the URL that I found from the View Loop window. Double checking my work, I used my SSH session to enter sudo reboot and reboot the glanceable display. After a couple of minutes of boot up time, the glanceable display finished it's startup and began showing the web page from Dakboard, rotating through the screens! How Did It Work? I started the first draft of this article in late July, just after I got the glanceable display working for my family. As of October, when I am adding this section to the article, we are using the display on a daily basis, mostly in the way it was intended. We continue to tweak little bits on the \"Left-over\" screen, but the other screens are dialed in the way we want them to be. One of the things that I believe is crucial to it's success is the placement in our kitchen. We have a nice corner where we have it plugged in, out of the way of various devices, tucked under the cupboards where we store our dinnerware and our glasses. This ensures that when we go to get some food or drink, we almost instinctivly glance down at the display as we are getting something out of one of those cupboards. For me, this eliminates the \"but I didn't look at the display\" factor for each of us, as we each know that getting something from those cupboards means we saw the display, if even for a second. It also may have seemed like a small thing to include my family in the process, but I firmly believe that engaging with them at various points and incorporating their feedback into the project had a large bearing in their acceptance of the display. It was important to all of us to be able to talk to the others and say \"hey, can we add X to the display\" and talk through whether there was a benefit to all of us from including that idea on the display. By including each family member in the creation process and design process, they continue to have an investment in what is being displayed, and ensuring it is current and relevant. And yes, we have all had to ask or remind each other to add various meetings on to our own calendars so they show up on the main calendar. By their very act of doing that, I know they are using the calendar. By the number of times my wife has mentioned that there is snow at the nearby Snoqualmie Pass, I know the weather screen is getting used, especially the weather alerts section. The harder one to figure out is the \"Left-Overs\" screen, but I am sure that I have had my son try hard to seamlessly work the word of the day into casual conversations, sometimes with hilarious results. But it's the thought that counts! What Was Accomplished This article detailed the work that was needed to take my family's glanceable display from an appliance that was just sitting around to one that we use every day. I shared with readers a number of different notes I made for this project at various points, along with my rationale for those notes. Finally, I walked through the various setup that I performed on the various screens to get them on the display. If you have followed this process through all 5 articles, thanks for your support, and I hope you take the time to look at some of the other articles on this site!","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/08/11/glanceable-displays-what-do-i-want-to-display/","loc":"https://jackdewinter.github.io/2019/08/11/glanceable-displays-what-do-i-want-to-display/"},{"title":"Glanceable Displays: Setting Up Our Display","text":"Preface This is the fourth article in a series about Glanceable Displays and my efforts to create one for my household. For other articles in the series, click on the title of the article under the heading \"Glanceable Displays\" on the right side of the webpage. Introduction In the first article, Glanceable Displays: What Are They? , I introduced the concept of a Glanceable Display, describing a number of things to consider if you decided to embark on a project like this. Assuming you decided to proceed, the article Glanceable Displays: Installing Raspbian on a Raspberry Pi detailed how I went from an unformatted Raspberry Pi to one with the Raspbian operating system installed. As I prefer working from my own workstation, I also detailed the setup of the SSH service on the Raspberry Pi to allow for remote connections. Finally, the article Glanceable Displays: Fine Tuning A Raspberry Pi Installation showed how I filled in a number of gaps that I encountered with the normal installation, namely setting up a wireless connection with my network and ensuring the Raspberry Pi has a solid understanding of the current time. Having taken all of those steps to be confident that setting up the actual display will work, it is time to jump right in and set the display up. But what is it actually that I was setting up? What Are Our Display's Implementation Requirements? When it comes down to it, the articles in the series have been building up to this point. I now have a glanceable display is a Raspberry Pi. Upon boot, it will start a web browser pointing at a specific webpage. It's that simple. However, the devil is in the requirements. The high level requirements for the display were covered in the first article in this series, Glanceable Displays: What Are They? . What is left are the specific requirements that will realize those high level requirements into an actual display: the implementation requirements. The first set of implementation requirements is that any administration of the machine can be performed from my desktop. With the exception of the Raspberry Pi seizing up, which I have noticed from time to time, I should not have to touch the Raspberry Pi itself. For the most part, I will be running sudo reboot to restart the machine, but all of that should be done without the need to plug a keyboard into the Raspberry Pi. Thanks to the documentation in the previous articles, these requirements have been completed. The second set of implementation requirements is that after a reboot of the machine, I shouldn't need to plug in a keyboard and type some commands to get it started. After the machine starts, it should open a browser and display the page or pages that are required. To be clear, on reboot with no keyboard and no mouse, the display should start by itself. The physical cost of plugging a keyboard into the machine kind of defeats the \"appliance\" feel that I want it to have. The final set of implementation requirements is that, to the best extent possible, any processing of what to show on the display should be performed on a machine other than the Raspberry Pi. While some of the more recent machines have more power on them, I want to be able to use lower cost Raspberry Pi machines for the display. If I must also run scripts to pull information to generate information for the display, it means I need a heftier machine. While this may change later, I believe that starting with the lower machine requirements is the right thing to do. Installing the Right Tools To make sure that I had the right browser and other utilities I needed, I ran the following command: sudo apt-get install chromium-browser x11-xserver-utils unclutter A lot of Raspbian installations will come with chromium-browser package already installed, but I included it just to make sure it is there. The x11-xserver-utils package has one or two small utilities that will make the display look cleaner. The unclutter package allows me to hide the mouse cursor after inactivity, perfect for a display where I know there will be no mouse attached. Note When I was testing out the installation instructions, one of the things that made me include instructions on setting up a time server is the apt-get command. In certain cases, if your Raspberry Pi's clock is too far in the past, you will not be able to access the right packages with apt-get . Please make sure your Raspberry Pi's clock is current before using the apt-get command. Creating a Local Startup file By default, Raspbian comes with a heavily modified version of the LXDE or \"Lightweight X-11 Desktop Environment\". According to the documentation , the startup configuration file for the a given user needs to be located at the path /home/$user/.config/lxsession/LXDE-pi/autostart . If it is not there, it will default to the generic file located at the path /etc/xdg/lxsession/LXDE-pi/autostart . As this setup uses the default pi user, the startup configuration file needs to reside at /home/pi/.config/lxsession/LXDE-pi/autostart Following the advice of various articles, I elected to create a copy of the autostart file under the local /home/pi/ directory. That way, if something bad happens, I can always restart the configuration by copying the default file into the /home/pi/ directory again. To accomplish this, I executed the following commands: mkdir -p /home/pi/.config/lxsession/LXDE-pi/ cp /etc/xdg/lxsession/LXDE-pi/autostart /home/pi/.config/lxsession/LXDE-pi/autostart To place the file in the proper directory, I performed a mkdir command as the LXDE-pi directory did not exist with the clean Raspbian installation I was using. Once I had the directory created, I used the cp command to copy the default version of the autostart file into that directory. At that time, the file looked like this: @lxpanel --profile LXDE-pi @pcmanfm --desktop --profile LXDE-pi @xscreensaver -no-splash @point-rpi Starting Chromium In Kiosk Mode Now that the local startup file was present, I edited the file with the command nano /home/pi/.config/lxsession/LXDE-pi/autostart in order to add my own startup commands. To be honest, I tried a number of different startup commands recommended by different articles, and each one had good points and bad points. After much experimentation, I ended up with a simple set of startup commands which was as follows: @unclutter @xset s off @xset s noblank @xset -dpms @chromium-browser --incognito --start-maximized --disable-notifications --disable-extensions --disable-hang-monitor --disable-infobars --kiosk https://bingwallpaper.anerg.com/ As mentioned above, the unclutter tool makes the mouse disappear when not used, which is perfect for a display that is never going to have a mouse. The xset tools allows for the setting of various XWindows related settings. Specifically, the s setting is for the screen saver and the -dpms setting is for the monitor's Energy Start features. Finally, the Chromium browser is the browser I chose to start with for displaying the webpages as it has the most documentation on command line switches . Note When I say the most documentation on command line switches, look at the link. The list is way too large to confidently comprehend. As such, I had to take guesses as to which of the –disable switches I needed. In order, the changes I made to the configuration file: hide the mouse turn the screen saver off don't blank the screen turn off any Energy Star power save monitor features start the browser (window maximized, incognito, in kiosk mode) pointing at the Bing wallpaper page After I finished editing the file, I made sure to save the file (ctrl-X, yes , enter), and then double checked all of my changes. When I confident I had all of them entered correctly, I proceeded to the next step. Verifying things are Working Properly I issued a sudo reboot to reboot the Raspberry Pi. And waited. And waited. And waited. If the repetition doesn't make it clear, it felt like forever. I was sure I had followed my own instructions properly. Even going off of my own notes, there was the anticipation of seeing whether or not it would work. The Raspberry Pi I was using seemed slower than usual, but after a while, everything started up and it was displaying the website https://bingwallpaper.anerg.com/ in the browser. As I checked the display, I saw the mouse pointer disappear after a few minutes. Check. After a couple of hours, the screen saver had not kicked in. Check. After a couple of hours, the monitor was still displaying the website. Check. For the most part, due to some good notes that I kept, everything was up and running the way it was supposed to. But What if it Is Not? Let's be honest. The first 5-10 times that I performed this setup, I didn't get it right and it was only my notetaking that helped me figure out which things worked and which didn't. It took a lot of notetaking and a lot of debugging and looking at log files to figure out what was working. The first log file that I used to debug things was the /home/pi/.xsession-errors file. After executing the cat /home/pi/.xsession-errors command, I noticed that while it didn't have a lot of useful information, it had two important pieces of information: ** Message: 17:00:13.893: main.vala:101: Session is LXDE-pi ** Message: 17:00:13.894: main.vala:102: DE is LXDE ** Message: 17:00:16.375: main.vala:133: log directory: /home/pi/.cache/lxsession/LXDE-pi ** Message: 17:00:16.376: main.vala:134: log path: /home/pi/.cache/lxsession/LXDE-pi/run.log This information let me know I was putting the changes in the right place, and where to look for the log for the current user's session: /home/pi/.cache/lxsession/LXDE-pi/run.log . Also, the fact that it was putting the logs in the /home/pi/.cache/lxsession/LXDE-pi directory meant that it noticed the autostart file that I added, as was using it. That was a useful piece of verification. When I started looking at that file, I was at first overwhelmed before I found a couple of tricks to help me out. The first trick was to look for a file that looks like this: ** Message: 17:00:16.917: autostart.vala:42: Autostart path : /home/pi/.config/lxsession/LXDE-pi/autostart Everything before that point are just the other parts of LXDE getting set up, and it really didn't have any effect on what I was trying to do. After that line were a series of lines that began with Launching , corresponding with each line of the autostart file. The next section of lines complemented those lines, providing for the exit codes of each of the lines in that file. Finally, there is a section starting with Connecting ... that signifies the section of the log where the wired and wireless link status is logged. While the link status is important, the fact that it gets to this point successfully generally means that the display is ready to go! What Was Accomplished This article detailed the work that was needed to take my Raspberry Pi from a sufficiently set up machine to fulfilling 2 out of the 3 requirements of my a glanceable display. At this point, I have setup a Raspberry Pi that is remotely administered from a machine other than the Raspberry Pi itself, one of our requirements. Another of the requirements is to have the machine start displaying a web browser automatically, without any requirement for a keyboard or mouse to be attached to the machine, which has also been accomplished. Along the way of documenting that setup, I also provided some useful tools to clean up the display and some places to look for debug logs for the setup process. Specifically, this article took the big step forward from a Raspberry Pi that was remotely administered, to a Raspberry Pi that automatically launches a web browser pointing to a specific webpage on reboot. The next step is to provide that web browser with some content that is not generated on the Raspberry Pi itself. What's Next? In the final article in this series, Glanceable Displays: What Do I Want To Display? I walk through the steps I took to determine where to get finalized webpages to display in my glanceable display, while adhering to the final implementation requirement of my glanceable display.","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/08/04/glanceable-displays-setting-up-our-display/","loc":"https://jackdewinter.github.io/2019/08/04/glanceable-displays-setting-up-our-display/"},{"title":"Glanceable Displays: Fine Tuning A Raspberry Pi Installation","text":"Preface This is the third article in a series about Glanceable Displays and my efforts to create one for my household. For other articles in the series, click on the title of the article under the heading \"Glanceable Displays\" on the right side of the webpage. Introduction In the previous article , I detailed the steps required to get the Raspbian (Linux) operating system installed on a Raspberry Pi with remote access through SSH enabled. This article attempts to move the needle forward by tackling a number of the issues that I had in getting necessary foundation services up and running on my Raspberry Pi. The two services that I had issues with were: ensuring that Wi-Fi was working properly and that the Raspberry Pi clock was being set properly. Until I was confident that these two issues were resolved, I was not confident that I would be able to use my glanceable display in it's intended location, as that location does not have any wired network access. Note As the steps in the previous article concluded with providing SSH access to the Raspberry Pi, I performed all of the following configuration using an SSH client. While you are welcome to enter them through a keyboard directly connected to the Raspberry Pi, I did not test any of the steps in that manner. Warning From the experience of reviewing and retrying these steps, sudo reboot is a good friend. If it looks like something didn't work and you think it should, consider using sudo reboot to reboot the machine and carry on from there. This is very handy when changing configuration. Step 1: Wi-Fi Access Unless you have the fortune of having a wired network outlet available near your display AND enjoy the aesthetic of having a network cable going into your display, you most likely want to enable Wi-Fi access to your display. Here are the steps I went though to get wireless access enabled. Step 1a: Searching for Existing Wireless Access The first command that I used to check for wireless network access was the ifconfig command. This is a general command used to determine what network connections are available. I had a network cable running into the machine, but nothing else, so the following response 1 was expected: eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 ... lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536 ... While it looks like gibberish to some, it was what I expected. Linux systems often use the tag eth0 for the first wired connection and the tag lo for the loopback connection. The important thing I noticed was that I didn't see a wlan0 tag, typically used for the first wireless connection. The clear observation I had was that the system did not currently have a capability to have a wireless connection. This observation on a normal computer running Linux might be farfetched, but from information gained researching the Raspberry Pi on various forums, it is relatively normal for a Raspberry Pi to not have any wireless capability built-in. As wireless components have recently become cheaper, it seems like it is only the latest versions of the Raspberry Pi 3 have wireless access built in. Note I have not repeated these steps on a newer Raspberry Pi, but I would expect that doing so would allow me to skip the next section on installing the adapter. I will update the article once I have tested that scenario. Step 1b: Installing a Wireless Adapter In my instance, the machine I was configuring did not have any wireless capabilities built in. That was resolved in short order by noting down the connection requirements for the household router (WPA2-PSK (AES)) and purchasing an inexpensive USB Wi-Fi adapter from a nearby store. Note If possible, use that store's online presence to inspect the possible adapters, verifying 1-3 choices for adapters that meet the router's specifications. While not necessary, it can avoid round trips to the store to try and find the right adapter. Doing this, I found that my local store had a TP-Link TL WN823N USB Adapter that was right for the job for $15. Returning home from the store with an inexpensive TP-Link TL WN823N adapter, I powered off the Raspberry Pi, installed the Wi-Fi adapter in one of the open USB slots, and powered up the Raspberry Pi. Once Raspbian had booted up, I reconnected to the machine using SSH and entered the lsusb command. This command is similar to the ls command to list files, but is used to list various USB components that the machine has recognized. The response I received from that command was: Bus 001 Device 006: ID 2357:0109 TP-Link TL WN823N RTL8192EU Bus 001 Device 005: ID 0461:4e26 Primax Electronics, Ltd Bus 001 Device 004: ID 0461:4d64 Primax Electronics, Ltd Bus 001 Device 003: ID 0424:ec00 Standard Microsystems Corp. SMSC9512/9514 Fast Ethernet Adapter Bus 001 Device 002: ID 0424:9514 Standard Microsystems Corp. SMC9514 Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Seeing as the adapter I bought was a TP-Link TL WN823N, I was relieved to see it listed as device 06. At this point it was clear to me that the Raspberry Pi recognized the adapter, and it was time to install the required drivers. Step 1c: Installing the Wi-Fi Drivers From a research point of view, this was the most frustrating part of getting wireless access working. In most of the articles I researched, the authors seemed to assume that the drivers for a given adapter would already be installed in the Raspbian distribution. The few articles I found that handled missing drivers were very specific, and not very helpful. They often used the Raspbian apt-get family of commands to look for an adapter, and each adapter I found seemed to have a slightly different way of making sure it worked properly. As I was writing down notes for this article to help other people, that experience was frustrating and was far from helpful. Everything changed when my research led me to a reference to the Fars Robotics driver install script. This breaks down the process into the following three commands: sudo wget http://www.fars-robotics.net/install-wifi -O /usr/bin/install-wifi sudo chmod +x /usr/bin/install-wifi sudo /usr/bin/install-wifi Instead of playing around with apt-get commands and looking for a specific driver, this script automated the process, finding the correct driver and installing it. The commands first downloaded the script into the /usr/bin directory, set it to be executable, and then executed the downloaded script. Within 60 seconds, it had completed, and it's output suggested that the driver for my TP-Link TL WN823N had installed successfully. To double check that it worked properly, I resubmitted the ifconfig command and got the following: eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 192.168.2.3 netmask 255.255.255.0 broadcast 192.168.2.255 ... lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 ... wlan0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 ... Great! Progress! The new Wi-Fi adapter was installed, the driver was connected, and Raspbian was reporting that it could talk with it! From experience, I now had to configure the machine to connect to the wireless network. From the output, this was somewhat obvious. For the eth0 and lo adapters, there was a line beginning with inet that described the current IPv4 setup for that adapter. As the wlan0 adapter was missing the inet line (and also the inet6 line), Raspbian could talk to the adapter, but the adapter was not connected to the local Wi-Fi network. Next stop, configuring Wi-Fi access. Step 1d: Configuring Wi-Fi Access Similar to the previous section on Installing the Wi-Fi Drivers, I found many articles on how to do this by hand, each with its own little twist on how to set things up better. In my case, I wanted to go for repeatable and easy, and the sudo raspi-config command used to set up SSH access in a prvious article proved to be the best solution. From the main menu, I selected 2 network options and then N2 WiFi . As it was my first time setting up the network on this machine, I was prompted for my country, which I entered. Next I was prompted with Please enter SSID , and I responded with my router's SSID. This was followed with the inevitable Please enter passphrase. Leave it empty if none. , to which I responded with my router's password. Hoping everything would work out, I pressed the Next button. After waiting for approximately 30 seconds, the focus was returned to the main configuration screen. Exiting out of that screen and back to the main screen, executing the ifconfig now had the following response: eth0: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500 inet 192.168.2.3 netmask 255.255.255.0 broadcast 192.168.2.255 ... lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 ... wlan0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 192.168.2.67 netmask 255.255.255.0 broadcast 192.168.2.255 ... This was what I was waiting for! At the moment, the eth0 wired connection had an address of 192.168.2.3 and the wlan0 wireless connection had an address of 192.168.2.67. The address 192.168.2.3 is the one I had been SSHing into while fixing up the Wi-Fi, so that lined up. Based on my router's configuration, 192.168.2.67 was a likely address for a new machine, so that lined up. Warning If you have your network secured, make sure you follow the normal steps for adding a new machine to your network. This may include asking your local network person to add your machine to the network. In my case, forgetting that step cost me over an hour trying to figure out why the Raspberry Pi would not connect to the network! To verify things were working, I repeated the relevant portions of Step 5: Setting Up SSH Access from the previous article, using the new address 192.168.2.67, instead of the old one 192.168.2.3. Once this succeeded, the final test was to disconnect the physical connection and see if the wireless connection worked. It did take a lengthy bit of time, and one trip to a local store for hardware, but it was very gratifying being able to talk to the glanceable display over a wireless connection! Step 2: System Clock Synchronization While the Raspberry Pi is a useful little machine, it's small form causes it to not have a component that many machine take for granted: a system clock. Not having this set up for your glanceable display can cause any display of time to be extremely out of sync. In addition, if the Raspberry Pi's time is not decently close to the actual time, the downloading of extra components through mechanisms such as the apt-get commands may fail. To avoid these issues, setting up proper system clock synchronization is a must. Pre-Requisite The list of servers and download locations for the apt family of commands has most likely changed since the Raspbian image was constructed. If this is the case, any apt commands that require any kind of package updates will most likely fail. To solve this before it becomes an issue, I issued the following command to update those tables. sudo apt-get update Step 2a: Installing the NTP Service The quickest and most efficient solution to solve the synchronization issue was to install the NTP time service. In the distribution of Raspbian that I was using, the NTP service was not installed. When I entered the command sudo systemctl status ntp , I saw the following output: Unit ntp.service could not be found. Based on various articles, the clear solution was to install the NTP service using apt-get with the following command: sudo apt-get install ntp . Once that finished, when I repeated the sudo systemctl status ntp command, I then received the following output: ● ntp.service - Network Time Service Loaded: loaded (/lib/systemd/system/ntp.service; enabled; vendor preset: enabled) Active: active (running) since ... At this point, I was satisfied that the NTP service was up and running, but not that it was working correctly. Step 2b: Diagnosing the NTP Service Configuration Once the service was loaded, I needed to confirm that the NTP service was working properly. The easy way to do this was to reboot the machine using sudo reboot and then use the date command to check the date. When I followed that pattern, the date was over 2 weeks off from the actual time. Time to go into debug mode. The documentation on the NTP service mentions a useful command: ntpq -p . This command lists information about what the current NTP service is doing. Presented as a series of columns, the important column was the number of seconds since the service successfully contacted a specific NTP server. When I checked the output of the command, I was greeted with the following table: remote refid st t when poll reach delay offset jitter ============================================================================== 0.debian.pool.n .POOL. 16 p - 64 0 0.000 0.000 0.004 1.debian.pool.n .POOL. 16 p - 64 0 0.000 0.000 0.004 2.debian.pool.n .POOL. 16 p - 64 0 0.000 0.000 0.004 3.debian.pool.n .POOL. 16 p - 64 0 0.000 0.000 0.004 Looking at the when column, the issue seemed to be that it was not able to connect to those servers and get information about them. Doing a bit of research, it turned out that other people have had problems with the default servers for the NTP server, and there were ways to address that. Step 2c: Changing the NTP Service Configuration Now that I had the NTP service installed and had issues with the default configuration, it was time to look at it. The configuration for the service is located in the file /etc/ntp.conf . As I had found issues with the default configuration, I needed to learn more about this configuration file to fix the problem. Looking at the file using a command like more /etc/ntp.conf was daunting. There were a number of 1-2 line sections with multiple lines of comments before them. When I saw that, I was concerned. In my experience, a large comment to useful line ratio means they are commenting the heck out of it, because people have done stupid things in the past. Learning more about the configuration, it turned out that I only needed to look at one particular section. The most important part of the configuration is a section that started with the follow text: # pool.ntp.org maps to about 1000 low-stratum NTP servers. Your server will # pick a different set every time it starts up. Please consider joining the # pool: <http://www.pool.ntp.org/join.html> Right after this text, there was a group of servers specified where the first part of the server name was a number between 0 and 3, and the rest of the server name was the same. Many of the articles I found didn't touch the rest of the file, but focused on that small set of lines. A very useful page I found during my research was the NTP Pool Time Servers page. Near the bottom of that page is a table that lists the various geographic areas, from which I selected the North America selection. At the top of the next page was a clearly marked section of text, with clear instruction to add that text to the ntp.conf file. Given that information, I went back to the Raspbian machine, entered sudo nano /etc/ntp.conf to edit the /etc/ntp.conf file, and replaced the default debian servers in that section with the information from the clearly marked section of the NTP Pool Time Servers page. Followed up with Ctrl-X to save, y to save the file, enter to use the provided file name, and it was changed. Just to make sure, I did another sudo reboot after verifying my changes, and the date command now returned the right date. Step 2d: Verify that the NTP Service is Working Warning As a number of Denial-Of-Service attacks have used the NTP port to return bad information, a number of routers come pre-configured with their firewalls set to block port 119 and 123, the NTP ports. If you follow these instructions and are still having issues, check the firewall settings for your computer and your router. Previously, when I had checked the status of the NTP service using the ntpq -p command, I did not seeing anything other than - in the when column of the output. If things were working properly, I would expect that column to change. Submitting the ntpq -p command once the system rebooted, I got the following output: remote refid st t when poll reach delay offset jitter ============================================================================== -grom.polpo.org 127.67.113.92 2 u 21 64 1 59.302 9.614 6.053 +199.180.133.100 140.142.234.133 3 u 17 64 1 26.382 5.225 3.611 +time1-inapsales 216.218.254.202 2 u 17 64 1 41.043 -2.463 4.417 *vps5.ctyme.com 216.218.254.202 2 u 18 64 1 36.759 -5.673 2.870 Submitting the command multiple times, I observed that the when column values increased to a certain point before they started over with a low single digit number. From the documentation, this was a good indication that each of those servers was being queried successfully, then registering itself for the next time to check against that server. Seeing the successful connections with the NTP servers, the time synchronization issues were cleared up! Just to be sure, I did a couple of cycles of sudo reboot and checking the time, with no errors. What Was Accomplished This article detailed the steps taken to fix two of the major issues I had after installing Raspbian on my Raspberry Pi: the Wi-Fi access and the system clock being out of sync. I worked through the various steps taken to ensure that the wireless access was up and running, including the various checks I did throughout the process. I then walked through the steps I took to ensure that the time on the Raspberry Pi was reflecting actual time, despite not having an onboard system clock. With this accomplished, I had confidence in having firm foundations on which to start building the display part of my glanceable display. What's Next? In the next article in this series, Glanceable Displays: Setting Up Our Display I walk through the steps I took to setup the Raspberry Pi to start the display upon boot for the glanceable display. In any of these examples, a set of ellipses (\"…\") are used to denote that content was removed that was either sensitive or not relevant to the example being presented. ↩","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/07/28/glanceable-displays-fine-tuning-a-raspberry-pi-installation/","loc":"https://jackdewinter.github.io/2019/07/28/glanceable-displays-fine-tuning-a-raspberry-pi-installation/"},{"title":"Glanceable Displays: Installing Raspbian on a Raspberry Pi","text":"Preface This is the second article in a series about Glanceable Displays and my efforts to create one for my household. For other articles in the series, click on the title of the article under the heading \"Glanceable Displays\" on the right side of the webpage. Introduction The methods detailed in this article provide for simple installation of the Raspbian operating system using the New Out Of Box Software (NOOBS) installation method, as suggested for beginners by the Raspberry Pi site . While there are more direct methods for experienced users, the NOOBs installation was selected for it's simplicity and ease of installation. Using the NOOBs installation, this article details the first steps I took in setting up one of my Raspberry Pi systems to be a glanceable display for my family. Those steps start with the formatting of a MicroSD card and installation of the NOOBs installer on to that MicroSD card. After installing that card into the Raspberry Pi, the steps continue with the installation of a stock Raspbian distribution, detailing the a couple of questions that need to be answered to complete the installation. Finally, to enable remote access, the last step is to ensure that I can access the Raspberry Pi using SSH for later configuration and control. Requirements Raspberry Pi, version 3.0 or later Power Supply for Raspberry Pi Keyboard and Mouse HDMI cable connected to monitor Cat5 Ethernet cable Notepad (electronic or paper) to write down notes on specifics of the installation Note Please keep your own notes as you go, and refer back to them. While I have tested the steps on my own Raspberry Pi machines, they were by no means exhaustive tests. Step 1: Interfacing With a MicroSD Card The configuration and main drive for a Raspberry Pi is a MicroSD card. To get the card ready for use, your computer must be able to interface with the card. Most computers do not come with MicroSD slots, but there are a fair number with SD slots. To make things easier for MicroSD buyers, some of the more high end MicroSD makers include a MicroSD-to-SD adapter in their packaging, such as this 32GB MicroSD card from Amazon . I started out using this, but found that the adapter was only good for 3-4 uses, not for continual use. An alternative is a more multi-purpose adaptor, such as this multi-adapter from Amazon . As it is made from a more durable material, it will survive more uses. The one that I bought from Amazon at 7.00 USD is still working after about 70+ uses, so at 0.10 USD per use, it has already paid for itself. Also, as it has a USB adapter, I can plug it into a USB extension cable that I already have on my desk. Whichever way you decide to go, make sure to add the MicroSD card to the adapter before plugging the adaptor into you computer. Once it is securely in the adapter, make sure to apply it to the relevant slot on your computer firmly, and make sure the connection is there. On my Windows 10 machine, I can tell this happens as it will acknowledge the connection by opening up an Explorer window, with either a \"please format\" instruction or a list of any files in the directory. Step 2: Getting the MicroSD Card Ready Note Note that the steps that follow are for my Windows 10 machine. The NOOBs site , has sections for installing on Mac and Linux, but I did not test them. If they do not work,please Google/Bing linux microsd card format and linux microsd card mount . Feel free to replace the generic linux in the searches with the name of the Linux distribution that you are using. Step 2a: Reformatting a Used MicroSD Card Reusing old hardware is important, for important reasons such as the environment and cost to build. To make sure that is possible, it took me a number of tries to create a solid recipe for reformatting the MicroSD cards. As I mention in the Requirements section, keep good notes of what you do, or if following a recipe like this one, what changes you made to the recipe. While there is a command line only tool that will also do the job, I found it clunky and hard to use. Instead, the Disk Management applet for the control panel was the tool I settled on. This can be invoked by typing partition in the search window on the tool bar and selecting the create and format hard disk partitions item. Selecting that item brought up the Disk Management window, showing a break down of every drive connected to my computer. When the MicroSD card was properly connected to the computer, it showed up as Removable media after all of my permanent drives. Using the right mouse button, I clicked on each of the blocks on my MicroSD card and selected the Delete Volume menu item until all of the volumes were gone. When that was accomplished, I was left with two blocks, and right clicking on the rightmost block presented me with a Delete Partition menu item, which consolidated all of the partitions into a single unallocated block. From there, I was able to right click on the Unallocated partition to select the Create Volume menu item. This started a simple wizard that quickly walked me through the options for creating a new volume. I used all defaults that were presented with the exception of the file system and quick format settings. I changed the file system setting to FAT32 and unchecked the Use Quick Format checkbox, before clicking on finish and waiting for about 30 minutes before the format was complete. Step 2b: Formatting a New MicroSD Card From a Windows 10 point of view, this was easy. When the MicroSD card was properly connected to my computer, it prompted me to format the card, presenting me with the format dialog itself. When formatting the MicroSD card, it was important to select FAT32 as the type of format and to unselect Quick Format on the dialog. Once I clicked the format button, it took a good while before it was completed. As a rough estimate, I guessed that it was roughly 1 minute per gigabyte on the MicroSD card, regardless of computer speed. Step 3: Install Raspbian Lite Using NOOBS Note When the format is finished from the previous step, it is important to go to your taskbar and eject the media from your computer. I accomplished this by right clicking on the USB stick icon and selecting \"Eject Mass Storage Device\" from the menu. At that point, I cleanly removed the adaptor and the MicroSD card from the computer to ensure the ejection was complete. When I tested various scenarios, any time that I forgot to eject the media at this point, it did not take later on. The people behind the Raspberry PI made sure there is a simple to use installation system that simplifies the task of installing operating systems on to the Raspberry Pi. The New Out Of Box Software (NOOBS) site aims to allow a fairly basic installation of Raspberry Pi operating systems with little effort. Unless you are familiar with Linux systems, their installation can be very daunting, so it is best to keep the installation as simple as possible. To start that process, I downloaded the NOOBS zip file from their website to my computer. After reinserting the MicroSD card and adapter to my computer, I then unzipped the contents of the NOOBS_V3_2_0.zip file to the root of the drive for the MicroSD card. I had to take care to ensure that the contents were in the root of the drive, not in a subdirectory of the drive. This happened enough times that I actually unzipped the files to a local directory and just used XCOPY to copy the files over, solving the placement problem for myself. As with the note at the start of this section, once this action was done, I once again ejected the USB device before disconnecting it from the computer, for the same reasons. Taking the MicroSD card, I found the MicroSD port on the Raspberry Pi. The port is flat with the motherboard of the Raspberry Pi, and the cases I have all have a hole in the case to make it easy to find. Inserting the card into the port, I then attached the other cables for monitor (HDMI), ethernet (Cat5), keyboard (USB), and mouse (USB), with the 5V adapter cable being last. Two minutes later, I was presented with a screen which prompted me to select the operating system to install. I tried a number of times to get the Raspbian Lite install to work, but encountered a number of issues, so I defaulted to the stock Raspbian [RECOMMENDED] install. Once I made this choice, I selected Raspbian [RECOMMENDED] from the top of the list in the NOOBs installation dialog, followed by pressing the Install button at the top. From there, it took about 30 minutes or so before I was prompted with a dialog box that said: OS(es) installed successfully When I pressed the OK button on that dialog, the system rebooted with first a rainbow screen, then a screen with a big raspberry on it, then lots of text scrolling by quickly. After a relatively small amount of action and a lot of waiting, it was now time to set up the operating system for simple uses! Step 4: Initial System Setup There was a lot of text that scrolled by so quickly, I was unable to read it. From what I could see, there were a lot of green OK texts on the left side, so I guessed that the installation had succeeded. After a nice round of blinking lights from the Raspberry Pi, the desktop showed up and proceeded to walk me through the setup configuration. The first dialog was pretty simple, with the title Welcome to Raspberry Pi . The important thing to note off of this dialog is at the bottom right of the dialog is the IP address that the system currently has assigned to it. As this was important, I wrote it down, and proceeded with the rest of the configuration. The configuration is a series of simple dialogs, each giving a clear indication of what is required. Whenever I pressed the Next button, it wrote the information to the system configuration files. As such, I expect delays between when I pressed the Next button and when the next dialog showed up. Turns out that was a rather healthy expectation. Some of the things that were setup were: country language time zone the default user pi setting a new password for the user pi take care of black border update software Having tested this workflow, I knew that the next workflow for my glanceable display would include updating existing packages and installing new packages. As such, I skipped the update software, knowing I would do it later. Both paths produce the same results, so feel free to skip it like I did, or update at a later point. Warning If you forget the password for the user pi , there is no easy way to recover what you changed the password to. Consider creating a fake entry in a password manager, like LastPass, and storing the password there for later use. Step 5: Setting Up SSH Access That being accomplished, the last thing to complete before stopping the installation of the bare bones system was to enable SSH access. By enabling SSH access, I could sit at my comfortable workspace, using my normal computer, chair, and desk instead of at the workbench where I had the Raspberry Pi connected. Frankly, the computer was connected in an almost Frankenstein like mess of wires on an old desk with a chair that was normally reserved for people visiting, not typing. My own workspace looked very inviting compared to that mess. To enable access, I entered the command sudo raspi-config , selecting 5. Interfacing Options , then selecting P2 SSH , and finally answering Yes to the question Would you like the SSH server to be enabled? . After this, the computer took about 30 seconds before being responsive again, with the text The SSH server is enabled. appearing on the screen. Pressing the enter key, and then selecting Finish , I was then back at the command prompt. This was the moment I was working towards: being able to have a bare bones system to use that I could access from my own computer. Entering sudo reboot , I waited about 45 seconds for the system to reboot and to be greeted with the raspberrypi login: prompt. Looking just above that text, I saw the text: [ OK ] Started OpenBSD Secure Shell server. This gave me a bit of confidence to move forward. At the very least, the operating system was indicating that it should allow SSH access. At the command line, I entered: ssh pi@192.168.2.3 and with the exception of the input yes to answer the question, the output was as follows: The authenticity of host '192.168.2.3 (192.168.2.3)' can't be established. ECDSA key fingerprint is SHA256:`some text here`. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '192.168.2.3' (ECDSA) to the list of known hosts. ... pi@raspberrypi:~ $ Note After each repeated installation on the same Raspberry Pi, when I went back to open a new SSH connection, it would report the error WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! . To allow access, I needed to edit the ~/.ssh/known_hosts file and remove the line for the IP address of the machine, as indicated at the end of the provided error message. What Was Accomplished This article detailed the steps taken to install the Raspbian operating system on a MicroSD card. It started by my formatting of the MicroSD card and copying the NOOBs installer onto the card, followed by inserting it into the Raspberry Pi's MicroSD slot. The steps continued with the largely automated installation of the operating system, only requiring the answers to six questions on my part. Finally, it concluded with the setup for SSH to allow me to configure the Raspberry Pi remotely. What's Next? In the next article in this series, Glanceable Displays: Fine Tuning A Raspberry Pi Installation , I walk through the steps I took to move the installation from a bare bones system, to one that had Wi-Fi and time support set up properly.","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/07/21/glanceable-displays-installing-raspbian-on-a-raspberry-pi/","loc":"https://jackdewinter.github.io/2019/07/21/glanceable-displays-installing-raspbian-on-a-raspberry-pi/"},{"title":"Glanceable Displays: What Are They?","text":"Preface This is the first article in a series about Glanceable Displays and my efforts to create one for my household. For other articles in the series, click on the title of the article under the heading \"Glanceable Displays\" on the right side of the webpage. Introduction In this article, I will introduce the concept of glanceable displays and describe how they can be used in an everyday office or home. I will then discuss how the audience for the display is important, including a side bar about my opinion on Spousal Acceptance Factor. Understanding the limitations of your audience is also covered, leading up to developing a set of definitions on what should be displayed. Finally, I will talk about how my family followed those steps to arrive at the requirements for our glanceable display, which has buy in from every member of our household. What Are Glanceable Displays? According to Dictionary.com : 1 noting or relating to information on an electronic screen that can be understood quickly or at a glance: glanceable data; a glanceable scoreboard. 2 enabling information on a screen to be quickly understood: a glanceable design; glanceable interfaces. In essence, a glanceable display is a display that provides information that can be quickly understood by people reading the display. An important qualification of these displays is that the information they display needs to have a broad degree of applicability without any ability for a specific reader to customize the data to their needs. As the display must provide information with no personal customization, it is important to think about various things up front. Who Is the Display For? The audience of the display will drive many of the other choices for the glanceable display. Be honest about who the real audience is for your glanceable display, and what benefits that audience will gain from the display. Negotiate with the audience members and make sure that there is something on the display for each member. If possible, engage with those members to help with the display so their work becomes part of the display, fostering interest from the beginning. The key is for your project to make the jump from \"my toy project\" to \"our useful project\", solving real world issues with real world requirements. If you are not honest about your audience and do not communicate with them sincerely, you will invariably end up missing your goals for the display. Without these, your base requirements will not be correct, and your final product will not be correct. If you only want the display for your personal office, it is perfectly acceptable for you to say that you are the sole audience and geek out on the display. That is, it is acceptable if you are honest about that audience. In that case, just be realistic and don't expect people you didn't include in your original audience to appreciate your display. After all, you made it just for you! Spousal Acceptance Factor As an aside, I have often heard of concerns from people about something they made having a low SAF or Spousal Acceptance Factor. I honestly think that is silly. If there is a low SAF, it probably means that someone did something where their spouse was either not consulted on or not considered with in terms of requirements. It is human to want to share your excitement with family around you but remember that your excitement is not their excitement. Unless you sincerely include them in the audience, the chance of acceptance will remain low. How Are You Going To Display It? In terms of deciding how to display the glanceable information, you really have only one decision to make: are you going to ask the viewer to visit an online site with a device or are you going to provide access to a device at a given location? If you decide on the visit paradigm, you don't have any hardware concerns as the reader is supplying the hardware. With the viewer using their own device to view the display, another decision must be made regarding whether to standardize on a single device type or to support multiple device types. If you decide to support multiple device types, you will probably need to use an intermediate step where you start with support for only the most used device type. Once that device type has been completed, you can then slowly adapt your display to the next device type your viewers are using. You will also need to ensure that you have a clear definition on where your display can be accessed from. If you have a website for your office that you display on phones or tablets, can it be viewed from anywhere, or just within the office's WiFi area? If you decide on the specific location paradigm, you limit your device type to one, but you take on additional hardware concerns. You get to focus on a single device type, but in exchange, you need to provide for the setup and control of that device. Consider the case where the power goes off and then comes back. How will your hardware solution handle that or will someone need to reboot it? Another important consideration is the cost of the hardware and any needed peripherals. Will you reuse existing hardware that you already have, or will you require expenditures? The output from your evaluation of this section should be a choice of a single approach and a list of reasons why the selected approach was chosen. If possible, provide as many details as possible as it will help you in other sections going forward. Also, from your audience's point of view, it will help them understand the decisions that you have asked them to buy in to. What Do You Need To Consider Up Front? As the display is a glanceable display, this means that everyone should be able to view the display with few issues, if any. Common issues to think about are: Near and Far Sightedness A large segment of every population either wears glasses or contacts at some point in their life. Especially as people get older, their eyesight gets weaker and they rely on glasses more frequently. Depending on the differing quality of eyesight of your audience, you may want to consider using larger fonts to enable them to see the screen more clearly. In addition, you may want to consider a high contract mode that includes fewer, but bolder colors to improve visibility. Color Blindness Color blindness, as shown in this article on Wikipedia , is an inability to see differences in either all colors or certain types of colors. Keep in mind that if your audience is not a family environment, the person with color blindness may not disclose that they are color blind up front. If one of your audience is color blind, using colors on your display to indicate certain things is a bad idea. Use shapes or text instead of colors to denote differences in the data being displayed. Dyslexia and Other Reading Disorders Dyslexia, as shown in this article on Wikipedia , is actually a family of similar disorders involving the brain and how numbers and letters are processed when reading. Other reading disorders, such as those mentioned in this Wikipedia article , are often grouped by people as dyslexia, when they are only related. As with dyslexia, keep in mind that if you audience is not a family environment, the person with dyslexia may not disclose that they are color blind up front. Advances in research on reading issues have produced advances such as the Dyslexie font which is specially made for people with dyslexia. Engage with your audience to determine if any such issues apply, what their effects are, and talk with them and research with them on ways to adapt the display to make it easier for them to comprehend. Young Readers Young readers, due to their age, are either still learning how to read or are building their vocabulary as the grow. To assist any young readers that are going to use your display, consider replacing some of the objects that you want to display with pictures that indicate the object's meaning. For ‘older' young readers, keep in mind that their vocabulary is different than yours, and change you designs for the display accordingly. What Are You Going To Display? Once you have all the other sections taken care of, the decision of what to put on the display is almost anti-climactic. After going through the process of identifying your audience, the type of display to use, and any considerations for your audience, you have probably defined at least one or two things to display along the way. At the very least, armed with the information above, you can engage with your audience in a brainstorming session that will allow you to get buy-in from them. Two important things to remember at this stage: soliciting help and iterative development. Don't be afraid to ask your audience to help you in the design for the display. That can take any form you want, from design the information for the display with them there to asking them to create the displays and presenting them to the entire audience. Remember, for the glanceable display to be successful, you will need buy-in from your audience. Having them help with the work will do that. Iterative development means that you and your audience are going to try and make something that works, but you do not expect it to be perfect on the first try. You and your audience may be confident think something works when you look at it initially, but over time that confidence may change. Don't be afraid to iterate on the design, keeping the things that work and changing the things that don't work. Our Discussion About Our Glanceable Display Understanding that any decision would affect my entire family, I asked if we could talk about it after dinner one night. In discussing the possibility of a display, we all agreed that there were common things that it would be nice to see. These things came down to 3 categories: calendar related, weather related, and other. As we all have busy lives, the calendar was a slam dunk. Our whiteboard calendar didn't get updated when it should, which left us looking at our online calendars on our phones. But even with our online calendars, it was a pain to remember to invite other family members to any events we had, regardless of whether or not they were part of the event. Having a single place to display that information keyed to a given individual would clear things up and simplify things a lot. Information on the weather was another category, mostly due to the Seattle weather. While my son wears the same type of clothes every day, my wife and I vary our clothing by the type of weather and activities we have planned. Having that advance knowledge of weather would cut down on having to actively seek that information from a number of online sources. After those two big categories, there were also some other little things brought up. Not having a good place to put them, the \"others\" category was formed. The discussion then moved to decide which class of glanceable display to use, and our family made a simple decision to go with a monitor driven by a web page hosted on a Raspberry Pi. We all agreed that we wanted something that would replace a seldom updated whiteboard calendar in our kitchen. It needed to be big enough to show several weeks' worth of calendar at a time, to allow us to plan that far out. We also wanted to make sure we kept each other honest, so we explicitly wanted it not tied to any of our personal computers and tied to a location that we know we all frequent: the kitchen. The choice of the Raspberry Pi satisfied these concerns pretty easily. From a hardware point of view, I had a spare one at home from a project I had wanted to do, but never started. From an operating system point of view, I have enough knowledge of Linux systems that I was confident that I would be able to handle the configuration. Finally, I was prepared to take the challenge on of setting up the system and working with my family to define the elements of the display with their input at ever step. The Decisions for Our Glanceable Display So, from those discussions, I arrived at the following. Audience The audience was our family. Display The display itself would be a simple Raspberry Pi with a monitor that was left from a project that I had (almost) worked on. The display would be located in the kitchen in a location that would be visible to family members, but not visible outside of the house. Considerations In our family, we don't have any members that have vision issues other than needing glasses. As such, the primary concern is that we can all ready the text on the display from 2 meters or 6 feet away. What To Display? The primary two goals for the display were to display our calendars and the weather for the next 5 days. Any enhancements of those two goals were fine, as long as the primary goals were not ignored. Some of the other ideas that were floated may seem funny, but they nicely fit into our other category: chore lists from Trello quote of the day number of days until Christmas number of people on the International Space Station current exchange rates Wrapping It Up The rest of the articles in this series will detail my family and I worked on our glanceable display. Based on the information from above, we have had good success with our display. Most of the fixes to the display were tweaks to the information being displayed, rather than the Raspberry Pi itself. I emphatically stand by the previous sections about and making sure you understand and engage your audience. I credit my family, the audience for our glanceable display, with having an honest conversation on what would help, and getting the buy in from them from the beginning. What Was Accomplished? This article presented a set of things to consider when creating a glanceable display, followed by notes on how my family followed that pattern to arrive at our requirements for our glanceable display. Those considerations started with defining your audience, proceeded to understanding your audience, and finally arriving at a set of things that you and your audience want to display. I cannot promise that if you follow these considerations that your journey will be as successful as ours. However, I believe I can say with some certainty that it will help you along the way with your own journey. To succeed, you need information to help guide you, and each of the considerations above will help you inch closer to that success. What's Next? In the next article in this series, Glanceable Displays: Installing Raspbian on a Raspberry Pi , I walk through the steps I took to set up a Raspberry Pi as our glanceable display of choice. It documents the journey from installation on to a blank MicroSD card to a bare bones installation that enabled remote SSH access.","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/07/14/glanceable-displays-what-are-they/","loc":"https://jackdewinter.github.io/2019/07/14/glanceable-displays-what-are-they/"},{"title":"Starting With GitHub: Setting Up Credentials For My Personal Website","text":"Introduction Part of any project I do, private or open-source, is to set up a version control system and securing access to that version control system. In addition, it is always a high priority for me to make sure that the secure access follows best common practices on security, as it is protecting work that I am doing. Using Git as my version control system of choice, and GitHub as the open source repository host of choice, it made logical sense to make my website platform of choice GitHub Pages 3 . As such, in setting up this website I needed to make sure I had Git 1 and more specifically GitHub 2 setup, and setup securely. This article details the actions and choices I made in setting up my access to GitHub for my blog. It details how I followed the GitHub Pages instructions for creating a personal website and creating a personal repository on GitHub to achieve that. Then it describes the two modes of accessing GitHub, SSH and HTTPS, and why I chose SSH. Finally, it provides detailed notes on how I generated a SSH keypair specifically for GitHub, and configured both my local Git and the remote GitHub to use them. Getting Started With GitHub There are many good articles out there on how to install Git for different operating systems, so I feel it is safe to assume that anyone reading this can do the research needed to install Git for their operating system. Setting up access to GitHub is even easier, as the folks at GitHub have provided excellent instructions. Start the setup by simply going to the GitHub Home Page and following their instructions. The pivotal part is that to start using GitHub in a serious manner, you need to authenticate yourself to the GitHub servers. Thus, the workflow will either allow you to login, if you already have an account, or create a new account, if you don't have an account. Having already dealt with a couple of open source projects, I logged on to my account jackdewinter without any issues. After I logged in, the browser deposited me on my home page. From there I was able to see any projects that I had either contributed to or filed an issue against. Just starting in my Open Source journey, the contents were only a couple of projects that I had filed issues with. Prior to this point, I had no need to authenticate my Git client with GitHub as I was just downloading from public repositories. Having done some research on GitHub Pages, I knew that setting up my own website with GitHub would require me to create my own repositories. As such, my next task was to create that repository. Creating My First GitHub Repository The GitHub Pages home page has a really simple formula on their webpage for setting up personal webpages. The first step is pretty easy: make sure that that the name of the repository is my GitHub user id (in my case jackdewinter ) followed by .github.io . When the creation of my repository finished, GitHub deposited my browser at the base of my new repository: jackdewinter/jackdewinter.github.io . The remaining steps in the formula dealt with cloning the repository, defining a sample index.html file for the website, and pushing that code back to the repository. While I was familiar with those concepts, I wasn't shy about checking back with the Git Basics documentation on the Git website when I forgot something. From there I was able to find the correct helper article on what I need to accomplish within 2-3 clicks. In GitHub, unless you mark a repository as private, everyone can see that repository and read from that repository. As my website's repository is public, reading wasn't a problem. However, pushing the code back to my repository would be writing, and that was a problem. Each GitHub project has a list of who can write to it and the creator of the project is on that list by default. But to write to the project, I needed my local Git tool to login to GitHub when needed and authenticate itself. To do this securely, I was going to have to dive into credentials. GitHub Authentication: SSH vs HTTPS Any time you log in to a website or to a program to gain specific privileges, you establish your credentials by supplying your user id and password. You tell the website or program \"I can prove I am me, let me see my stuff!\". The GitHub website is no different that any of those sites. If you want to be able to see any of your private stuff or write to your stuff, it needs to verify who you are. Going to the Authenticating with GitHub from Git , there are two choices that allow us to connect to GitHub: HTTPS and SSH. Both of these are valid options, allowing for enhanced security when Git connects to GitHub. Each of these options has different things going for and against it. After doing some research, it seemed to me to break down to the following: SSH HTTPS set up keys set up credential manager setup is more involved easy setup more secure less likely blocked by firewall Looking at this information, I decided to go with SSH as I wanted to opt for more security. SSH Access to GitHub During my research at the GitHub site, I found this very good page on SSH over the HTTPS port . In it, they explain that there is a simple test to see if SSH will work from your system to GitHub. When you execute the following command: ssh -T git@github.com it will either return one of two responses. If it returns with: > Hi *username*! You've successfully authenticated, but GitHub does not provide shell access. then you can access GitHub via SSH without any issues. If you see the other response: > ssh: connect to host github.com port 22: Connection timed out then you have to setup SSH to connect to GitHub over the HTTPS port. This access can be verified with a small modification to the above command: ssh -T -p 443 git@ssh.github.com The command is now trying to establish a SSH session over port 443, and if you get the You've successfully... response, it's working fine. Running these tests myself, I found that I got a timeout on the first command and a success on the second command. Following the article, it recommends changes to ~/.ssh/config 4 to include the following: Host github.com Hostname ssh.github.com Port 443 The next time, when I executed the ssh -T git@github.com command, the response was the You've successfully response. Now I was ready to set up the SSH keys. Unique SSH Keys Going back to the Authenticating with GitHub from Git , the next step was to generate a new SSH key pair and add it to the local SSH keyring. The page that points to generating a new key is pretty detailed, so I won't try and improve over GitHub's work. On my reading of the page, it seems to assume that if you will only have 1 key pair 5 generated and that you will reuse that key pair for GitHub. I have issues with that practice, so I want to talk about it. Having a bit of a security background from my day job, I want to limit exposure if something gets broken. Just from a quick search, there are articles by Leo Notenboom , Malware Bytes Labs , and WikiHow that all describe how you should have different passwords for each account, and in many cases, use a password manager. And to be honest, that was just the first 3 that I clicked on. There were a lot more. I can sum up and paraphrase the justification raised in each of those articles by posing a single question: If someone breaks your password on one site, what is your exposure? If you have one password for all sites, then whoever breaks your password has access to that one site. If you have a different password for each site, the damage is limited to one site, instead of all sites using that password. In my mind, using a key pair for credentials is a similar concept to using a user-id and password for credentials. Therefore, it followed that if I follow good security practices for passwords, I should also follow the same practices for key pairs as credentials. Generating a New Key For GitHub To ensure I have secure access to GitHub, I followed the instructions for generating a new key . To generate a distinct key pair for GitHub, I made one small modification to the instructions: I saved the new key pair information with the filename github-key instead of the default id_ras . This resulted in the files ~/.ssh/github-key and ~/.ssh/github-key.pub being created as the key pair. With those files created, I followed the remaining instructions for setting up ssh-agent and uploading the key information to GitHub, replacing any occurrence of id_ras with github-key . With that accomplished, I had a specific key pair specifically for GitHub and it was registered locally. I also had setup GitHub with the public portion of the credentials using the contents of ~/.ssh/github-key.pub , as instructed. The only remaining step was ensure that any SSH connections to GitHub would use the GitHub credentials. Doing a bit more research on the SSH configuration files, I quickly found that there was built in support for this by adding the following to my ~/.ssh/config file: Host github.com User git PreferredAuthentications publickey IdentityFile /c/Users/jackd/.ssh/github-key Note The location of ~/ on my Windows machine is %HOMEDRIVE%%HOMEPATH%\\ or c:\\Users\\jackd\\ . The format for the IdentityFile property is a standard Unix path format. This requires a translation from the Windows path format C:\\Users\\jackd\\.ssh\\github-key to the Unix path format of /c/Users/jackd/.ssh/github-key . Combined with the change from earlier in this article, my ~/.ssh/config file now looked like: Host github.com Hostname ssh.github.com Port 443 User git PreferredAuthentications publickey IdentityFile /c/Users/jackd/.ssh/github-key Testing Against GitHub: GitHub Pages Having performed a number of thorough tests of the above steps, everything passed without any issues! Now it was time to try and push some commits for the blog to GitHub. To create a directory for the GitHub project, I largely followed these instructions detailed in the companion article on setting up your own static website. I then followed these instructions , adding the remote repository to my local configuration with the following line: git remote add origin jackdewinter/jackdewinter.github.io Having associated the directory with the remote repository, the final test was to make a change to the directory, commit it, and push it to the remote. For this test, I used a very simple index.html file: < html > < body > Hello world! </ body > </ html > Adding the file to the directory, I staged the file and committed it with: git add index.html git commit -m \"new files\" and then pushed it to the remote repository with: ssh-agent git push origin master --force Crossing my fingers, I waited until I got a response similar to: Counting objects: XXX, done. Delta compression using up to 8 threads. Compressing objects: 100% (XX/XX), done. Writing objects: 100% (XX/XX), XXX.XX KiB | 0 bytes/s, done. Total XX (delta XX), reused XX (delta XX) remote: Resolving deltas: 100% (XX/XX), completed with XX local objects. To blog:jackdewinter/jackdewinter.github.io.git XXXXXX..XXXXXX master -> master With no error messages, I double checked the repository at https://github.com/jackdewinter/jackdewinter.github.io and was able to see the index.html file present in the repository. Following through with the instructions for GitHub Pages, I then went to https://jackdewinter.github.io and was able to see the text \"Hello world!\" on in the browser. What Was Accomplished This article started with my creation of a GitHub repository to contain the files for my personal website using GitHub Pages. To securely access the repository, I chose the SSH protocol and discovered that I needed to employ SSH over HTTP. For enhanced security, I described a solid reason for wanting a unique SSH key for GitHub. Following that advice, I generated a new key and then changed the ~/.ssh/config file to use SSH over HTTPS and to point to that newly generated keypair. Finally, I committed a sample file to the project and was able to see it pushed successfully to the remote repository, and displayed as my personal home page. Git is an open-source source control tool. For more information, look here . ↩ GitHub is a common repository for open-source projects. For more information, look here . ↩ GitHub Pages are a feature of GitHub that allow people to host their personal websites on GitHub. For more information, look here . ↩ My primary system is a Windows 10 machine, so instead of modifying the ~/.ssh/config file, I modified the %HOMEDRIVE%%HOMEPATH%\\.ssh\\config file. On my system, that file is the c:\\Users\\jackd\\.ssh\\config file. ↩ When a SSH key is generated, it comes in two parts. The private part is kept on the user's system while the public part can be distributed to any interested parties. Together they are referred to as a key pair. ↩","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/07/07/starting-with-github-setting-up-credentials-for-my-personal-website/","loc":"https://jackdewinter.github.io/2019/07/07/starting-with-github-setting-up-credentials-for-my-personal-website/"},{"title":"Extended Markdown Examples","text":"This is a continuation of the previous cheat sheet for my website. This article specifically addresses any extensions that are not part of the base Markdown specification. Each section here represents an extension that I have enabled on my website. The formatting from the previous page is continued, with one small exception. The title of each section specifies the name of the extension instead of the name of the feature being documented (see Admonitions ). If an extension contains more than one feature, such as the Extra extension, the title specifies the name of the extension, a dash, and the name of the feature (see Footnotes ). Introduction The authors of the Python Markdown Package anticipated the addition of extra features. To ensure people would have choice, the base package can be extended using configuration . The Markdown extensions have been activated on my website by inserting the following text into my peliconconf.py: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 MARKDOWN = { 'extension_configs' : { 'markdown.extensions.extra' : {}, 'markdown.extensions.admonition' : {}, 'markdown.extensions.codehilite' : { 'css_class' : 'highlight' }, 'markdown.extensions.meta' : {}, 'smarty' : { 'smart_angled_quotes' : 'true' }, 'markdown.extensions.toc' : { 'permalink' : 'true' }, } } Table Of Contents [TOC] Introduction Table Of Contents CodeHilite - Code Blocks With Line Numbers Extra - Footnotes Extra - Abbreviations Extra - Definition Lists Smartypants Admonitions CodeHilite - Code Blocks With Line Numbers ``` #!python # Code goes here ... ``` 1 # Code goes here ... Extra - Footnotes Here's a simple footnote,[&#94;1] and here's a longer one.[&#94;bignote] [&#94;1]: This is the first footnote. [&#94;bignote]: Here's one with multiple paragraphs and code. Here's a simple footnote, 1 and here's a longer one. 2 Extra - Abbreviations The HTML specification is maintained by the W3C. *[HTML]: Hyper Text Markup Language *[W3C]: World Wide Web Consortium The HTML specification is maintained by the W3C . Extra - Definition Lists Apple : Pomaceous fruit of plants of the genus Malus in the family Rosaceae. Orange : The fruit of an evergreen tree of the genus Citrus. Apple Pomaceous fruit of plants of the genus Malus in the family Rosaceae. Orange The fruit of an evergreen tree of the genus Citrus. Smartypants advantage is that code blocks are unaffected - apostrophe ' by itself - apostrophe as in 'quote me' - quotations mark \" by itself - quotations mark as in \"quote me\" - replacement of multi-character sequences with Unicode: << ... -- >> --- apostrophe ‘ by itself apostrophe as in ‘quote me' quotations mark \" by itself quotations mark as in \"quote me\" replacement of multi-character sequences with Unicode: « … – » — Admonitions broken down into section by the way that the Elegant theme colors the admonitions !!! note You should note that the title will be automatically capitalized. Note You should note that the title will be automatically capitalized. !!! important \"Replacement Title\" You should note that the default title will be replaced. Replacement Title You should note that the default title will be replaced. !!! hint You should note that the title will be automatically capitalized. Hint You should note that the title will be automatically capitalized. !!! tip \"Replacement Title\" You should note that the default title will be replaced. Replacement Title You should note that the default title will be replaced. !!! warning You should note that the title will be automatically capitalized. Warning You should note that the title will be automatically capitalized. !!! caution \"Replacement Title\" You should note that the default title will be replaced. Replacement Title You should note that the default title will be replaced. !!! attention \"\" You should note that this will have no title due to the empty title. You should note that this will have no title due to the empty title. !!! danger You should note that the title will be automatically capitalized. Danger You should note that the title will be automatically capitalized. !!! error \"Replacement Title\" You should note that the default title will be replaced. Replacement Title You should note that the default title will be replaced. This is the first footnote. ↩ Here's one with multiple paragraphs and code. ↩","tags":"Website","url":"https://jackdewinter.github.io/2019/06/30/extended-markdown-examples/","loc":"https://jackdewinter.github.io/2019/06/30/extended-markdown-examples/"},{"title":"Standard Markdown Examples","text":"As I started writing my articles for my blog, I realized I needed something. To help me write articles using this flavor of Markdown 1 , I needed my own cheat sheet. My hope is that it provides clear guidance on which aspects of the various forms of Markdown worked for me, and which didn't. Introduction Horizontal Break Headings Text Emphasis Numbered lists Bulleted List Block quote Code Block Tables Links Local Links Remote Links Download Links ) Images Introduction I am writing articles and pages on Pelican 4.0.1 2 using the Elegant 3 theme, therefore I want to make sure I have a cheat sheet that is specific to this dialect of Markdown. The base Markdown used for Pelican uses the Python Markdown Package which (with 3 exceptions) follows John Gruber's Markdown definition very literally. Pelican configuration also supports providing Markdown with additional configuration that enables other features. Those features are documented separately in the next page . The format of this cheat sheet is simple. Each section is separated from the next with a horizontal break and the name of the section. Any notes regarding that section are placed at the top of the section in point form, to ensure they are brief. Then a Code Block section is used to show the literal code used to produce the effects that are presented right after the code block. Horizontal Break A horizontal break occurs after 3 or more hyphens. --- A horizontal break occurs after 3 or more hyphens. Headings # Heading Level 1 ## Heading Level 2 ### Heading Level 3 Heading Level 1 Heading Level 2 Heading Level 3 Text Emphasis two spaces at the end of a line will be equivalent to <br/> This text is **bold** and this text is also __bold__. This text is *italic* and this text is also _italic_. This text is **_italic and bold_**, but no two spaces at end. Single ```line``` block. Inline `code` has ```back-ticks like this ` around``` it. This text is bold and this text is also bold . This text is italic and this text is also italic . This text is italic and bold , but no two spaces at end. Single line block. Inline code has back-ticks like this ` around it. Numbered lists to maintain the indentation, place 4 spaces at the start of the line 1. One New para. Blah 2. Two - unordered - list 3. Three 1. ordered 2. list - unordered - list 3. items One New para. Blah Two unordered list Three ordered list unordered list items Bulleted List to maintain the indentation, place 4 spaces at the start of the line - This is a list item with two paragraphs. This is the second paragraph in the list item. You're only required to indent the first line. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. - Another item in the same list. - Bulleted item - Bulleted item This is a list item with two paragraphs. This is the second paragraph in the list item. You're only required to indent the first line. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Another item in the same list. Bulleted item Bulleted item Block quote > This is the first paragraph of a blockquote with two paragraphs. > Lorem ipsum dolor sit amet, > consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. > Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. > > This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. > > This is the first level of quoting. > > > This is nested blockquote. > > Back to the first level. This is the first paragraph of a blockquote with two paragraphs. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. This is the first level of quoting. This is nested blockquote. Back to the first level. Code Block line numbers can be added via extensions ```text Make things only as complex as they need to be. ``` ```Python # Blogroll LINKS = ( ('Pelican', 'Pelican', 'http://getpelican.com/'), ) ``` Make things only as complex as they need to be. # Blogroll LINKS = ( ( 'Pelican' , 'Pelican' , 'http://getpelican.com/' ), ) Tables colons can be used to align columns. | Column1 | Column 2 | Column 3 |---|---|---| | Value 1 | Value 2 | Value 3 | | Value 4 | Value 5 | Value 6 | | Value 7 | Value 8 | Value 9 | | Tables | Are | Cool | | ------------- |:-------------:| -----:| | col 3 is | right-aligned | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | Column1 Column 2 Column 3 Value 1 Value 2 Value 3 Value 4 Value 5 Value 6 Value 7 Value 8 Value 9 Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 Links Local Links {filename} tag indicates location in the content folder. [About Page]({filename}/pages/landing-page-about-hidden.md) About Page Remote Links proper URL indicates a remote website [Python Package Index](https://pypi.org) Python Package Index Download Links download links are not natively supported in Markdown must explicitly create HTML text inline to achieve that Creating a link to a file to download, not display, is not natively supported in markdown. [Pelican Brag Document (display)]({static}/images/markdown-1/pelican.txt) <a href=\"{static}/images/pelican.txt\" download>Pelican Brag Document (download)</a> Pelican Brag Document (display) Pelican Brag Document (download) Images {filename} tag indicates location in the content folder. ![python logo]({static}/images/markdown-1/python_icon.png) Markdown allows for HTML pages to be written using a simple text editor with no knowledge of HTML. ↩ Pelican is a Static Site Generator written in Python. ↩ The Elegant theme's repository is here . ↩","tags":"Website","url":"https://jackdewinter.github.io/2019/06/29/standard-markdown-examples/","loc":"https://jackdewinter.github.io/2019/06/29/standard-markdown-examples/"},{"title":"My Gallery Article","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer malesuada sed tortor et pulvinar. Donec a vehicula ligula. Quisque porta erat vitae lectus lacinia varius. Integer sed lacus in libero volutpat lobortis ac vitae velit. Praesent rutrum turpis sem, id mattis sem pulvinar id. Morbi leo felis, facilisis in ex a, viverra placerat justo. Donec ac risus non sapien feugiat malesuada.","tags":"Website","url":"https://jackdewinter.github.io/2010/12/06/my-gallery-test/","loc":"https://jackdewinter.github.io/2010/12/06/my-gallery-test/"},{"title":"My Long Article","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer malesuada sed tortor et pulvinar. Donec a vehicula ligula. Quisque porta erat vitae lectus lacinia varius. Integer sed lacus in libero volutpat lobortis ac vitae velit. Praesent rutrum turpis sem, id mattis sem pulvinar id. Morbi leo felis, facilisis in ex a, viverra placerat justo. Donec ac risus non sapien feugiat malesuada. Para 1 Donec quam neque, vulputate quis purus at, tempus tincidunt neque. Sed posuere eros eu massa lobortis varius. Ut condimentum elit eros. Sed vel nunc vitae nibh aliquet vestibulum vitae quis justo. Sed vel ligula turpis. Aliquam et mi mollis, suscipit sapien vel, molestie enim. Morbi sodales, dui nec congue tristique, risus mi luctus nulla, vel egestas sem nulla quis augue. Nulla vitae efficitur odio, quis egestas ex. Pellentesque a est viverra, fringilla dui ac, laoreet purus. Suspendisse porta aliquet nunc et pulvinar. Integer ante felis, tincidunt eu ipsum a, imperdiet convallis augue. Cras vulputate sapien sit amet metus placerat, sed congue turpis tempus. Nunc pretium ac dolor eget tincidunt. Para 2 Nunc id tortor lectus. Quisque fermentum sem ut elit ultricies sollicitudin. Curabitur blandit, elit at suscipit mattis, purus lectus eleifend felis, id rutrum neque sapien vitae arcu. Aenean elementum lacus tristique purus facilisis placerat. Nunc pharetra lorem ut finibus blandit. Aenean scelerisque elit nec malesuada accumsan. Proin eu orci eget odio scelerisque viverra a ac nulla. Vestibulum elementum lobortis quam. Morbi porta rutrum mi, quis laoreet nunc dictum at. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Ut in lobortis massa. Para 2a Phasellus et leo in nunc fermentum vulputate. Nullam sed interdum augue. Duis eu dignissim eros. Mauris pretium turpis non purus porta, non consequat enim rutrum. Fusce dui odio, consequat in rhoncus sed, interdum vulputate quam. Nullam nec dolor ex. Curabitur dapibus vestibulum odio at sodales. Para 2b Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum.","tags":"Website","url":"https://jackdewinter.github.io/2010/12/03/my-super-long-post/","loc":"https://jackdewinter.github.io/2010/12/03/my-super-long-post/"},{"title":"My Short Article","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer malesuada sed tortor et pulvinar. Donec a vehicula ligula. Quisque porta erat vitae lectus lacinia varius. Integer sed lacus in libero volutpat lobortis ac vitae velit. Praesent rutrum turpis sem, id mattis sem pulvinar id. Morbi leo felis, facilisis in ex a, viverra placerat justo. Donec ac risus non sapien feugiat malesuada.","tags":"Website","url":"https://jackdewinter.github.io/2010/12/03/my-super-short-post/","loc":"https://jackdewinter.github.io/2010/12/03/my-super-short-post/"}]};