<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Jack De Winter" />

        <meta name="description" content="Introduction¶ As part of the process of creating a Markdown Linter to use with my personal website, I firmly believe that it is imperative that I have solid testing on the linter and the tools necessary to test the linter. In previous articles, I talked about the framework I use …
" />
        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="pytest, scenario testing, Software Quality, " />

<meta property="og:title" content="Clarity Through The Summarizing of Test Measurements "/>
<meta property="og:url" content="https://jackdewinter.github.io/drafts/clarity-through-the-summarizing-of-test-measurements.html" />
<meta property="og:description" content="Introduction¶ As part of the process of creating a Markdown Linter to use with my personal website, I firmly believe that it is imperative that I have solid testing on the linter and the tools necessary to test the linter. In previous articles, I talked about the framework I use …" />
<meta property="og:site_name" content="Jack&#39;s Digital Workbench" />
<meta property="og:article:author" content="Jack De Winter" />
<meta property="og:article:published_time" content="2020-01-20T00:00:00-08:00" />
<meta name="twitter:title" content="Clarity Through The Summarizing of Test Measurements ">
<meta name="twitter:description" content="Introduction¶ As part of the process of creating a Markdown Linter to use with my personal website, I firmly believe that it is imperative that I have solid testing on the linter and the tools necessary to test the linter. In previous articles, I talked about the framework I use …">

        <title>Clarity Through The Summarizing of Test Measurements  · Jack&#39;s Digital Workbench
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://jackdewinter.github.io/theme/css/style.min.css?bec7d543">

        <link href="https://jackdewinter.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Jack&#39;s Digital Workbench - Full Atom Feed" />


    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://jackdewinter.github.io/"><span class=site-name>Jack's Digital Workbench</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://jackdewinter.github.io
                                    >Home</a>
                                </li>
                                <li ><a href="https://jackdewinter.github.io/categories">Categories</a></li>
                                <li ><a href="https://jackdewinter.github.io/tags">Tags</a></li>
                                <li ><a href="https://jackdewinter.github.io/archives">Archives</a></li>
                                <li><form class="navbar-search" action="https://jackdewinter.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="https://jackdewinter.github.io/drafts/clarity-through-the-summarizing-of-test-measurements.html">
                Clarity Through The Summarizing of Test Measurements
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
    <div class="span2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div class="toc">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#why-not-discuss-the-script-itself">Why Not Discuss The Script Itself?</a></li>
<li><a href="#setting-up-pyscan-for-its-own-project">Setting Up PyScan For It’s Own Project</a></li>
<li><a href="#before-we-continue">Before We Continue…</a></li>
<li><a href="#introducing-changes-and-observing-behavior">Introducing Changes and Observing Behavior</a><ul>
<li><a href="#adding-new-code">Adding New Code</a></li>
<li><a href="#adding-a-new-test">Adding a New Test</a></li>
<li><a href="#populating-the-test-function">Populating the Test Function</a></li>
<li><a href="#establishing-a-new-baseline">Establishing a New Baseline</a></li>
<li><a href="#refactoring-code-my-refactoring-process">Refactoring Code - My Refactoring Process</a></li>
<li><a href="#refactoring-code-leveraging-the-summaries">Refactoring Code - Leveraging The Summaries</a></li>
<li><a href="#determining-additive-test-function-coverage">Determining Additive Test Function Coverage</a></li>
</ul>
</li>
<li><a href="#wrapping-up">Wrapping Up</a></li>
</ul>
</div>
        </nav>
    </div>
    <div class="span8 article-content">
            
            
<h2 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">¶</a></h2>
<p>As part of the process of
<a href="https://jackdewinter.github.io/2019/12/08/markdown-linter-collecting-requirements/">creating a Markdown Linter</a>
to use with my personal website, I firmly believe that it is imperative that I have
solid testing on the linter and the tools necessary to test the linter.  In previous
articles, I talked about the framework I use to
<a href="https://jackdewinter.github.io/2020/01/06/scenario-testing-python-scripts/">scenario test Python scripts</a> and
how my current PyTest setup
<a href="https://jackdewinter.github.io/2020/01/13/measuring-testing-in-python-scripts/">produces useful test reports</a>,
both human-readable and machine-readable.  These two things allow me to properly
test my Python scripts, to collect information on the tests used to verify those
scripts, and to determine how well the collection of tests covers those scripts.</p>
<p>While the human-readable reports are very useful for digging into issues, I often find
that I need a simple and concise “this is where you are now” summary that gives me the
most pertinent information from those reports.  Enter the next tool in my toolbox, a
Python script that summarizes information from the machine-readable reports,
unimaginatively called <code>PyScan</code>.  While it is simple tool, I constantly use this tool
when writing new Python scripts and their tests to ensure the development is going in
the direction that I want to.  This article describes how I use the tool and how it
provides a benefit to my development process.</p>
<h2 id="why-not-discuss-the-script-itself">Why Not Discuss The Script Itself?<a class="headerlink" href="#why-not-discuss-the-script-itself" title="Permanent link">¶</a></h2>
<p>When coming up with the idea for this article, I had two beneficial paths
available: focus on the code behind the PyScan tool or focus on the usage of the PyScan
tool.  Both paths have merit and benefit,
and both paths easily provide enough substance for a full article.  After a lot of
thought, I decided to focus on the usage of this tool instead of the code itself.  I
made this decision primarily due to my heavy use of the PyScan tool and it’s
significant benefit to my development process.</p>
<p>I rely on the PyScan to give me an accurate summary of the tests used to verify any
changes along with the impact on code coverage for each of those changes.  While I
can develop without PyScan, I find that using PyScan immediately increases my
confidence in each change I make.  When I make a given type of change to either the
source code or the test code, I expect a related side-effect to appear in the test
results report and the test coverage report.  By having PyScan produce summaries of the
test results and test coverage, each side-effect is more visible, therefore
adding validation that the changes made are the right changes.</p>
<p>In the end, the choice became an easy one: focus on the choice with the most positive
impact.  I felt that documenting how I use this tool satisfied that requirement with
room to spare.  I also felt that if any readers are still interested in looking at the
code behind the script, it’s easy enough to point them to the project’s
<a href="https://github.com/jackdewinter/pyscan">GitHub repository</a> and make sure it is well
documented.</p>
<h2 id="setting-up-pyscan-for-its-own-project">Setting Up PyScan For It’s Own Project<a class="headerlink" href="#setting-up-pyscan-for-its-own-project" title="Permanent link">¶</a></h2>
<p>Based on the setup from
<a href="https://jackdewinter.github.io/2020/01/13/measuring-testing-in-python-scripts/">the last article</a>, the PyTest command
line options <code>--junitxml=report/tests.xml</code> and <code>--cov-report xml:report/coverage.xml</code>
place the <code>tests.xml</code> file and the <code>coverage.xml</code> file in the <code>report</code> directory.
Based on observation, the <code>tests.xml</code> file is in a JUnit XML format and the
<code>coverage.xml</code>
file is in a Cobertura XML format.  The format of the <code>tests.xml</code> is pretty obvious from
the command line flag required to generate it.  The format of the <code>coverage.xml</code> file
took a bit more effort, but the following line of the file keyed me to it’s format:</p>
<div class="highlight"><pre><span></span><span class="c">&lt;!-- Based on https://raw.githubusercontent.com/cobertura/web/master/htdocs/xml/coverage-04.dtd --&gt;</span>
</pre></div>
<p>From within the project’s root directory, the main script is located at <code>../main.py</code>.
Since the project uses <code>pipenv</code>, the command line to invoke the script is
<code>pipenv run python pyscan/main.py</code> and invoking the script with the <code>--help</code> option
gives us the options that we can use.  Following the information from the help text,
the command line that I use from the project’s root directory is:</p>
<div class="highlight"><pre><span></span>pipenv run python pyscan/main.py --junit report/tests.xml --cobertura report/coverage.xml
</pre></div>
<p>With everything set up properly, the output from that command looks like:</p>
<div class="highlight"><pre><span></span>Test Results Summary
--------------------

Class Name                     Total Tests   Failed Tests   Skipped Tests
----------------------------  ------------  -------------  --------------
test.test_coverage_profiles              2              0               0
test.test_coverage_scenarios            12              0               0
test.test_publish_scenarios              9              0               0
test.test_results_scenarios             19              0               0
test.test_scenarios                      1              0               0
---                                     --              -               -
TOTALS                                  43              0               0

Test Coverage Summary
---------------------

Type           Covered   Measured   Percentage
------------  --------  ---------  -----------
Instructions       ---        ---        -----
Lines              505        507        99.61
Branches           158        164        96.34
Complexity         ---        ---        -----
Methods            ---        ---        -----
Classes            ---        ---        -----
</pre></div>
<h2 id="before-we-continue">Before We Continue…<a class="headerlink" href="#before-we-continue" title="Permanent link">¶</a></h2>
<p>To complete my setup, there are two more things that are needed.  The first thing is
that I primarily execute the tests from a simple Windows script called <code>ptest.cmd</code>.
While there is a lot of code in the <code>ptest.cmd</code> script to handle errors and options,
when the script is boiled down to it’s bare essence, the script runs tests and reports
on those tests as follows:</p>
<div class="highlight"><pre><span></span>pipenv run pytest
pipenv run python pyscan/main.py --only-changes --junit report/tests.xml --cobertura=report/coverage.xml
</pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>I also have a Bash version called <code>ptest.sh</code> which I have experimented with locally, but is not checked in to the project.  If you are interested in this script, please let me know in the comments below.</p>
</div>
<p>Setting up a script like <code>ptest</code> keeps things simple and easy-to-use.  One
notable part of the script is that there is a little bit of logic in the script to not
summarize any coverage if there are any issues running the tests under PyTest.  Call me
a purist, but if the tests fail to execute or are not passing, any
measurements of how well the tests cover the code are moot.</p>
<p>The other thing that I have setup is a small change to the command line for PyScan.  In
the “bare essence” text above, after the text <code>pyscan/main.py</code>, there is a new option
used for PyScan: the <code>--only-changes</code> option.  By adding the <code>--only-changes</code> option,
PyScan restricts the output to only those items that show changes.  If no changes are
detected, it displays a simple line stating that no changes have been observed.  In the
case of the above output, the output with this new option is as follows:</p>
<div class="highlight"><pre><span></span>Test Results Summary
--------------------

Test results have not changed since last published test results.

Test Coverage Summary
---------------------

Test coverage has not changed since last published test coverage.
</pre></div>
<p>To me, this gives a very clear indication that things have not changed.  In the
following sections, I go through different cases and explain what changes I made and
what effects I expect to see summarized.</p>
<h2 id="introducing-changes-and-observing-behavior">Introducing Changes and Observing Behavior<a class="headerlink" href="#introducing-changes-and-observing-behavior" title="Permanent link">¶</a></h2>
<p>For this section of the article, I temporarily added a “phantom” feature called
“nothing” to PyScan.  This feature is facilitated by two code changes.
In the <code>__parse_arguments</code> function, I added the following code:</p>
<div class="highlight"><pre><span></span>        <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">"--nothing"</span><span class="p">,</span>
            <span class="n">dest</span><span class="o">=</span><span class="s2">"do_nothing"</span><span class="p">,</span>
            <span class="n">action</span><span class="o">=</span><span class="s2">"store_true"</span><span class="p">,</span>
            <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
            <span class="n">help</span><span class="o">=</span><span class="s2">"only_changes"</span><span class="p">,</span>
        <span class="p">)</span>
</pre></div>
<p>and in the <code>main</code> function, I changed the code as follows:</p>
<div class="highlight"><pre><span></span>        <span class="n">args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__parse_arguments</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">do_nothing</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">"noop"</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
<p>Note that this feature is only present for the sake of these examples, and is not in
the project’s code base.</p>
<h3 id="adding-new-code">Adding New Code<a class="headerlink" href="#adding-new-code" title="Permanent link">¶</a></h3>
<p>When I added the above code for the samples, the output that I got after running
the tests was:</p>
<div class="highlight"><pre><span></span>Test Results Summary
--------------------

Test results have not changed since last published test results.

Test Coverage Summary
---------------------

Type       Covered   Measured     Percentage
--------  --------  ---------  -------------
Lines     507 (+2)   511 (+4)  99.22 (-0.39)
Branches  159 (+1)   166 (+2)  95.78 (-0.56)
</pre></div>
<p>Based on the introduced changes, this output was expected.  In the <code>Measured</code> column,
4 new lines were added (1 in <code>__parse_arguments</code> and 3 in <code>main</code>) and the
<code>if args.do_nothing:</code> line added 2 branches (1 for True and one for False). In the
<code>Covered</code> column, without any tests to exercise the new code, 2 lines are
covered by default (1 in <code>__parse_arguments</code> and 1 in <code>main</code>) and 1 branch is covered
by default (the False case of <code>if args.do_nothing:</code>).</p>
<h3 id="adding-a-new-test">Adding a New Test<a class="headerlink" href="#adding-a-new-test" title="Permanent link">¶</a></h3>
<p>Having added source code to the project, I added a test to address the new code.  To
start, I added this simple test function to the <code>test_scenarios.py</code> file:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_nothing</span><span class="p">():</span>
    <span class="k">pass</span>
</pre></div>
<p>This change is just a stub for a test function, so the expected change is that the
number of tests for that module increase and there is no change in coverage.  This
effect is born out by the output:</p>
<div class="highlight"><pre><span></span>Test Results Summary
--------------------

Class Name            Total Tests   Failed Tests   Skipped Tests
-------------------  ------------  -------------  --------------
test.test_scenarios        2 (+1)              0               0
---                       --                   -               -
TOTALS                    44 (+1)              0               0

Test Coverage Summary
---------------------

Type       Covered   Measured     Percentage
--------  --------  ---------  -------------
Lines     507 (+2)   511 (+4)  99.22 (-0.39)
Branches  159 (+1)   166 (+2)  95.78 (-0.56)
</pre></div>
<h3 id="populating-the-test-function">Populating the Test Function<a class="headerlink" href="#populating-the-test-function" title="Permanent link">¶</a></h3>
<p>Now that a stub for the test is in place and registering, I added a real body to the
test function as follows:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_nothing</span><span class="p">():</span>

    <span class="c1"># Arrange</span>
    <span class="n">executor</span> <span class="o">=</span> <span class="n">MainlineExecutor</span><span class="p">()</span>
    <span class="n">suppplied_arguments</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"--nothing"</span><span class="p">]</span>

    <span class="n">expected_output</span> <span class="o">=</span> <span class="s2">"""noop</span>
<span class="s2">"""</span>
    <span class="n">expected_error</span> <span class="o">=</span> <span class="s2">""</span>
    <span class="n">expected_return_code</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1"># Act</span>
    <span class="n">execute_results</span> <span class="o">=</span> <span class="n">executor</span><span class="o">.</span><span class="n">invoke_main</span><span class="p">(</span><span class="n">arguments</span><span class="o">=</span><span class="n">suppplied_arguments</span><span class="p">,</span> <span class="n">cwd</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

    <span class="c1"># Assert</span>
    <span class="n">execute_results</span><span class="o">.</span><span class="n">assert_results</span><span class="p">(</span>
        <span class="n">expected_output</span><span class="p">,</span> <span class="n">expected_error</span><span class="p">,</span> <span class="n">expected_return_code</span>
    <span class="p">)</span>
</pre></div>
<p>The code that I added at the start of this section is triggered by the command line
argument <code>--nothing</code>, printing the simple response text <code>noop</code>, and returning a return
code of 1 .  This test code was crafted to trigger that code and to verify the expected
output.</p>
<div class="highlight"><pre><span></span>Test Results Summary
--------------------

Class Name            Total Tests   Failed Tests   Skipped Tests
-------------------  ------------  -------------  --------------
test.test_scenarios        2 (+1)              0               0
---                       --                   -               -
TOTALS                    44 (+1)              0               0

Test Coverage Summary
---------------------

Type       Covered   Measured     Percentage
--------  --------  ---------  -------------
Lines     509 (+4)   511 (+4)  99.61 ( 0.00)
Branches  160 (+2)   166 (+2)  96.39 (+0.04)
</pre></div>
<p>Based on the output from the test results summary, the test does verify that once
triggered, the code is working as expected.  If there was any issue with the test,
the summary would include the text <code>1 (+1)</code> in the <code>Failed Tests</code> column to denote
the failure.  As that text is not present, it is safe to assume that both tests in
the <code>test.test_scenarios</code> module succeeded.  In addition, based on the output from the
test coverage summary, the new code added 4 lines and 2 branches to the code base, and
the new test code covered all of those changes.</p>
<h3 id="establishing-a-new-baseline">Establishing a New Baseline<a class="headerlink" href="#establishing-a-new-baseline" title="Permanent link">¶</a></h3>
<p>With the new source code and test code in place, I needed to publish the results and
set a new baseline for the project.  To do this with the <code>ptest</code> script, I invoked the
following command line:</p>
<div class="highlight"><pre><span></span>ptest -p
</pre></div>
<p>Within this <code>ptest</code> script, the <code>-p</code> option was translated into the following command:</p>
<div class="highlight"><pre><span></span>pipenv run python pyscan/main.py --publish
</pre></div>
<p>When executed, the <code>publish/coverage.json</code> and <code>publish/test-results.json</code> files were
updated with the current summaries.  Following that point, when the script was run, it
reverts back to the original output of:</p>
<div class="highlight"><pre><span></span>Test Results Summary
--------------------

Test results have not changed since last published test results.

Test Coverage Summary
---------------------

Test coverage has not changed since last published test coverage.
</pre></div>
<p>This process can be repeated at any time to establish a solid baseline that any new
changes can be measured against.</p>
<h3 id="refactoring-code-my-refactoring-process">Refactoring Code - My Refactoring Process<a class="headerlink" href="#refactoring-code-my-refactoring-process" title="Permanent link">¶</a></h3>
<p>In practice, I frequently do “cut-and-paste” development during my normal development
process.  However, I do this with a strict rule that I follow: “2 times on the
fence, 3 times refactor, clean up later”.  That rule break down as follows:</p>
<ul>
<li>if I cut-and-paste code once, I then have 2 copies, and I should consider refactoring unless I have a good reason to delay</li>
<li>if I cut-and-paste that code again, I then have 3 copies, and that third copy must be into a function that the other 2 copies get merged into</li>
<li>when I have solid tests in place and I am done with primary development, go back
to all of the cases where I have 2 copies and condense them if beneficial</li>
</ul>
<p>My rationale for this rule is as follows.</p>
<p>When you are creating code, you want the
ideas to flow free and fast, completing a good attempt at meeting your current goal
in the most efficient way possible.  While cut-and-paste as a long term strategy is not
good, I find that in the short term, it helps me in creating a new function, even if
that function is a copy of something done before.  To balance that, from experience, if
I have pasted the same code twice (meeting the criteria for “3 times refactor”), there
is a very good chance that I will use that code at least one more time, if not more.  At
that point, it makes more sense to refactor the code to encapsulate the functionality
properly before the block of code becomes to unwieldly.</p>
<p>Finally, once I have completed the creation of the new source code, I go back and
actively look for cases where I cut-and-pasted code, and if it is worth it to refactor
that code, with a decision to refactor if I am on the fence.  At the very least,
refactoring code into a function almost always makes the code more readable and
maintainable.  Basically, by following the above rule for refactoring, I almost always
change the code in a positive manner.</p>
<p>The summaries provided to me from PyScan help me with this refactoring in a big way.
Most of the time, the main idea with refactoring is to change the code on the “inside”
of the program or script without changing the “outside” of the program or script.  If
any changes are made to the “outside”, they are usually small changes with very
predictable impacts.  The PyScan summaries assist me in ensuring that any changes to the
outside of the script are kept small and manageable while also measuring the
improvements made to the inside of the script.  Essentially, seeing both summaries
helps me keep the code refactor of the script very crisp and on course.</p>
<h3 id="refactoring-code-leveraging-the-summaries">Refactoring Code - Leveraging The Summaries<a class="headerlink" href="#refactoring-code-leveraging-the-summaries" title="Permanent link">¶</a></h3>
<p>A good function set of functions for me to look at for clean-up refactoring were the
<code>generate_test_report</code> and <code>generate_coverage_report</code> functions.  When I wrote those
two functions, I wasn’t sure
how much difference I was going to have between those two functions, so did an initial
cut-and-paste (see “2 times on the fence”) and started making changes.  As those parts
of PyScan are now solid and tested, I went back (see “clean up later”) and compared
the two functions to see what was safe to refactor.</p>
<p>The first refactor I performed was to extract the xml loading logic into a new
<code>__load_xml_docment</code> function.  While I admit I didn’t get it right the first time, the
tests kept me in
check and made sure that, after a couple of tries, I got it right.  And when I say
“tries”, I mean that I made a change, ran <code>ptest</code>, got some information, and diagnosed
it… all within about 30-60 seconds per iteration.  In the end, the summary looked like
this:</p>
<div class="highlight"><pre><span></span>Test Results Summary
--------------------

Test results have not changed since last published test results.

Test Coverage Summary
---------------------

Type        Covered   Measured     Percentage
--------  ---------  ---------  -------------
Lines     499 (-10)  501 (-10)  99.60 (-0.01)
Branches  154 ( -6)  160 ( -6)  96.25 (-0.14)
</pre></div>
<p>As expected, the refactor eliminated both lines of code and branches, with the measured
values noted in the summary.</p>
<p>The second refactor I made was to extract the summary file writing logic into a new
<code>__save_summary_file</code> function.  I followed a similar pattern to the refactor for
<code>__load_xml_docment</code>, but there was a small difference.  In this case, I observed that
for a specific error case, one function specified <code>test coverage</code> and the other function
specified <code>test summary</code>.  Seeing as consistent names in output is always beneficial,
I decided to change the error messages to be consistent with each other.  The
<code>test coverage</code> name for the first function remained the same, but the <code>test summary</code>
name was changed to <code>test report</code>, with the text <code>summary</code> added in the refactored
function.</p>
<p>At this point, I knew that one test for each of the test results scenarios and test
coverage scenarios was going to fail, but I knew that it would fail in a very specific
manner.  Based on the above changes, the text <code>Project test summary file</code> for the
results scenario test should change to <code>Project test report summary file</code> and the text
<code>Project test coverage file</code> for the coverage scenario test should change to
<code>Project test coverage summary file</code>.</p>
<p>When I ran the tests after these changes, there were indeed 2 errors, specifically
in the tests I thought they would show up in.  Once those 2 tests were changed to
reflect the new consistent text, the tests were ran again and produced the following
output:</p>
<div class="highlight"><pre><span></span>Test Results Summary
--------------------

Test results have not changed since last published test results.

Test Coverage Summary
---------------------

Type        Covered   Measured     Percentage
--------  ---------  ---------  -------------
Lines     491 (-18)  493 (-18)  99.59 (-0.01)
Branches  152 ( -8)  158 ( -8)  96.20 (-0.18)
</pre></div>
<p>Once again, the output matched my expectations.  While it wasn’t a large number of code
or branches, an additional 8 lines and 2 branches were refactored.</p>
<h3 id="determining-additive-test-function-coverage">Determining Additive Test Function Coverage<a class="headerlink" href="#determining-additive-test-function-coverage" title="Permanent link">¶</a></h3>
<p>There are times after I have written a series of tests where I wonder how much actual
coverage a given test contributes to the overall test coverage percentage. As test
coverage is a collaborative effort of all of the tests, a single number that identifies
the amount of code covered by a single test is not meaningful.  However, a meaningful
piece of information is what unique coverage a given test contributes to the collection
of tests as a whole.</p>
<p>To demonstrate how I do this, I picked one of the tests that addresses one of the error
conditions, the <code>test_summarize_cobertura_report_with_bad_source</code> function in the
<code>test_coverage_scenarios.py</code> file.  Before I
changed anything, I made sure to publish the current state to use it as a baseline. To
determine the additive coverage this test provides, I simply changed it’s name to
<code>xtest_summarize_cobertura_report_with_bad_source</code>.  As the <code>pytest</code> program only
matches on functions that start with <code>test_</code>, the function was then excluded from the
tests to be executed.</p>
<p>Upon running the <code>ptest</code> script, I got the following output:</p>
<div class="highlight"><pre><span></span>Test Results Summary
--------------------

Class Name                     Total Tests   Failed Tests   Skipped Tests
----------------------------  ------------  -------------  --------------
test.test_coverage_scenarios       11 (-1)              0               0
---                                --                   -               -
TOTALS                             43 (-1)              0               0

Test Coverage Summary
---------------------

Type       Covered   Measured     Percentage
--------  --------  ---------  -------------
Lines     507 (-2)        511  99.22 (-0.39)
Branches  159 (-1)        166  95.78 (-0.60)
</pre></div>
<p>Interpreting this output, given what I documented earlier in this article, was pretty
easy.  As I “disabled”
one of the coverage scenario tests in the <code>test_coverage_scenarios.py</code> file, the summary
reports one less test in <code>test.test_coverage_scenarios</code> as expected.  That disabled
test added 2 lines of coverage and 1 branch of coverage to overall effort, coverage
that was now being reported as missing.  As this test was added specifically to test a
single error case, this was expected.</p>
<p>If instead I disable the <code>xtest_junit_jacoco_profile</code> test in the
<code>test_coverage_profiles.py</code> file, I get a different result:</p>
<div class="highlight"><pre><span></span>Test Results Summary
--------------------

Class Name                    Total Tests   Failed Tests   Skipped Tests
---------------------------  ------------  -------------  --------------
test.test_coverage_profiles        1 (-1)              0               0
---                               --                   -               -
TOTALS                            43 (-1)              0               0

Test Coverage Summary
---------------------

Type       Covered   Measured     Percentage
--------  --------  ---------  -------------
Lines     501 (-8)        511  98.04 (-1.57)
Branches  152 (-8)        166  91.57 (-4.82)
</pre></div>
<p>Like the previous output, the disabled test is showing up as being removed, but there
is a lot more coverage that was removed.  Strangely enough, this was also expected.  As
I also use PyScan to summarize test results from Java projects I work on, I used all 6
coverage measurements available from Jacoco <sup id="fnref:jacoco"><a class="footnote-ref" href="#fn:jacoco">1</a></sup> as a baseline for the 2
measurements generated by PyTest for Python coverage.  With a quick look at the
<code>report/coverage/pyscan_model_py.html</code> file, this was indeed the reason for the
difference, with the test exercising 4 additional paths in each of the serialization
and deserialization functions. Basically, four paths of one line each, times two (one
for serialization and one for deserialization), and the 8 lines/branches covered is
explained.</p>
<h2 id="wrapping-up">Wrapping Up<a class="headerlink" href="#wrapping-up" title="Permanent link">¶</a></h2>
<p>I believe that making my decision to talk about how I use my PyScan tool to summarize
test results and test coverage was the right choice.  It is difficult for me to
quantize exactly how much benefit PyScan has provided to my development process, but it
is easily in the very positive to indispensable category.  By providing a quick summary
on the test results file and the test coverage file, I can ensure that any changes I
make are having the proper effects on those two files at each stage of the change that
I am making.  I hope that by walking through this process and how it helps me, it will
inspire others to adopt something similar in their development processes.</p>
<div class="footnote">
<hr/>
<ol>
<li id="fn:jacoco">
<p>For an example Jacoco HTML report that shows all 6 coverage measurements, check out <a href="https://www.jacoco.org/jacoco/trunk/coverage/">the report trunk coverage for Jacoco</a>. <a class="footnote-backref" href="#fnref:jacoco" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
</ol>
</div>


             
 
            
            
            






            <hr/>
        </div>
        <section id="article-sidebar" class="span2">
    <h4>Reading Time</h4>
    <p>~14 min read</p>
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2020-01-20T00:00:00-08:00">Jan 20, 2020</time>
            <h4>Category</h4>
            <a class="category-link" href="https://jackdewinter.github.io/categories#software-quality-ref">Software Quality</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://jackdewinter.github.io/tags#pytest-ref">pytest
                    <span>2</span>
</a></li>
                <li><a href="https://jackdewinter.github.io/tags#scenario-testing-ref">scenario testing
                    <span>2</span>
</a></li>
            </ul>
<h4>Stay in Touch</h4>
<div id="sidebar-social-link">
    <a href="https://github.com/jackdewinter" title="github-alt" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="GitHub" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
    </a>
    <a href="https://www.linkedin.com/in/jackdewinter/" title="linkedin" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="LinkedIn" role="img" viewBox="0 0 512 512" fill="#fff"><rect width="512" height="512" rx="15%" fill="#0077b5"/><circle cx="142" cy="138" r="37"/><path stroke="#fff" stroke-width="66" d="M244 194v198M142 194v198"/><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
    </a>
    <a href="https://jackdewinter.github.io/feeds/all.atom.xml" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="RSS" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#f80"/><circle cx="145" cy="367" r="35" fill="#fff"/><path fill="none" stroke="#fff" stroke-width="60" d="M109 241c89 0 162 73 162 162M109 127c152 0 276 124 276 276"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>
    <div>
        
&copy; Copyright 2019 by Jack De Winter and licensed under a <a rel="license"
  href="http://creativecommons.org/licenses/by/4.0/">
  <img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" />
  Creative Commons Attribution 4.0 International License</a>.

    </div>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>